{


"2024-02-23-czy-wiesz-czym-jest-i-jak-dziala-webworker-html": {
"title": "Czy wiesz, czym jest i jak działa WebWorker?",
"lang": "pl",
"tags": "javascript webworkers",
"content": "JavaScript, który wywołujemy na naszych stronach w postaci skryptów jest jednowątkowy. Jeżeli zdecydujemy się używać go do wykonywania zasobożernych obliczeń, może spowodować to, że interfejs użytkownika stanie się nieresponsywny.Z pomocą przychodzą nam webworkery. Jest to natywny mechanizm HTML5 API, który możemy wykorzystać bez żadnych dodatkowych bibliotek. Polega on na oddelegowywaniu operacji do specjalnie utworzonych wątków działających niezależnie od naszej aplikacji.Aby utworzyć nowy wątek należy stworzyć instancję klasy Worker:var worker = new Worker(&#39;worker.js&#39;);Dane można wysyłać do niego metodą postMessage():worker.postMessage(dataToSend);Z kolei sam worker musi nasłuchiwać na dane za pomocą listenera (znajduje się on w skrypcie worker.js przekazanym jako parametr do konstruktora)self.addEventListener(&#39;message&#39;, (event) =&amp;gt; {    // obsługa wejścia});Worker utworzony w ten sposób zostaje zakończony razem z zamknięciem strony, która powołała go do życia. Możemy również sami go zakończyć za pomocą polecenia:worker.terminate();Workery mają pewne ograniczenia:  nie mamy dostępu do naszej struktury DOM, ani obiektów window i document,  obiekt location jest tylko do odczytu, nie będziemy mogli z poziomu workera przekierowywać strony, ani tworzyć nowej zakładki,  worker nie może istnieć sam w oderwaniu od strony, która go utworzyła.Poniżej kilka typowych przykładów wykorzystania webworkerów:  obliczenia w tle: webworkery są wykorzystywane do wykonywania długotrwałych obliczeń w tle, takich jak przetwarzanie dużych zestawów danych czy złożone algorytmy, które mogłyby spowolnić działanie interfejsu użytkownika,  manipulacja na multimediach: webworkery są stosowane do manipulacji obrazami, dźwiękiem i wideo w tle. Mogą pomóc w przetwarzaniu, optymalizacji, tworzeniu miniatur czy konwersji formatów,  asynchroniczne żądania sieciowe: webworkery pozwalają na wykonywanie asynchronicznych żądań sieciowych, takich jak pobieranie danych z serwera czy przetwarzanie odpowiedzi HTTP,  WebAssembly: webworkery są również używane w połączeniu z WebAssembly, umożliwiając wykonywanie bardziej zaawansowanych obliczeń, które są zoptymalizowane pod kątem wydajności.Przydatne linki  Can I use dla Web Workers: https://caniuse.com/webworkers  Dokumentacja na MDN: https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API?retiredLocale=pl",
"url": "/2024/02/23/czy-wiesz-czym-jest-i-jak-dziala-webworker.html",
"author": "Piotr Grobelny",
"authorUrl": "/authors/pgrobelny.html",
"image": "pgrobelny.webp",
"highlight": "/assets/img/posts/2024-02-23-czy-wiesz-czym-jest-i-jak-dziala-webworker/code.webp",
"date": "23-02-2024",
"path": "pl/2024-02-23-czy-wiesz-czym-jest-i-jak-dziala-webworker"
}
,


"2024-01-26-czy-wiesz-jak-bezprzerwowo-zmienic-schemat-w-mongodb-html": {
"title": "Czy wiesz, jak bezprzerwowo zmienić schemat w MongoDB?",
"lang": "pl",
"tags": "mongodb Schema Versioning Pattern",
"content": "Wyobraźmy sobie konieczność wprowadzenia zmian w schemacie. Przy tradycyjnym podejściu musielibyśmy zatrzymać aplikację, dokonać migracji bazy i dopiero po jej zakończeniu uruchomić aplikację. Co jeśli nasza kolekcja jest już bardzo duża albo przerwy nie są akceptowalne? Jest też ryzyko, że nastąpi błąd przy migracji, a powrót do poprzedniej wersji w niektórych przypadkach może być dużym problemem. Co proponuje MongoDB? Schema Versioning Pattern.Kiedy użyć?  kiedy nie może być przerwy w dostępie do systemu  kiedy aktualizacja dokumentów może zająć godziny, dni lub tygodnie  kiedy nie trzeba aktualizować wszystkich dokumentów do nowej wersjiPrzykładPierwotnie zamodelowaliśmy dane kontaktowe w następujący sposób.{    &quot;_id&quot;: &quot;&amp;lt;ObjectId&amp;gt;&quot;,    &quot;name&quot;: &quot;Anakin Skywalker&quot;,    &quot;home&quot;: &quot;503-555-0000&quot;,    &quot;work&quot;: &quot;503-555-0010&quot;,    &quot;mobile&quot;: &quot;503-555-0120&quot;}Minęło trochę czasu, mamy już dużo dokumentów w naszej kolekcji, ale zmieniły się wymagania i musimy przechowywać więcej danych kontaktowych, związanych z nowymi formami komunikacji. Znamy na szczęście Attribute Pattern i decydujemy się wprowadzić nie tylko nowe pola, ale i zastosować ten wzorzec - dodajemy pole contact_method.&quot;contact_method&quot;: [    { &quot;work&quot;: &quot;503-555-0210&quot; },    { &quot;mobile&quot;: &quot;503-555-0220&quot; },    { &quot;twitter&quot;: &quot;@anakinskywalker&quot; },    { &quot;skype&quot;: &quot;AlwaysWithYou&quot; }]Nie możemy sobie jednak pozwolić na przerwę w działaniu aplikacji i migrację od razu wszystkich dokumentów. Nowe dokumenty będą miały już tylko pole contact_method, a stare jeszcze dawne pola home, work, mobile. Kod aplikacji będzie więc tworzył nowe dokumenty z polem contact_method przy dodawaniu. Może też ewentualnie aktualizować stare dokumenty przy okazji update’u.Wymaga to od nas dostosowania kodu aplikacji do dokumentów w dwóch różnych wersjach. Twórcy Mongo proponują bardzo proste rozwiązanie - dodanie dodatkowego pola schema_version w nowych dokumentach (lub zwiększenie wartości schema_version, jeśli już istnieje).{    &quot;_id&quot;: &quot;&amp;lt;ObjectId&amp;gt;&quot;,    &quot;schema_version&quot;: &quot;2&quot;,    &quot;name&quot;: &quot;Anakin Skywalker&quot;,    &quot;contact_method&quot;: [        { &quot;work&quot;: &quot;503-555-0210&quot; },        { &quot;mobile&quot;: &quot;503-555-0220&quot; },        { &quot;twitter&quot;: &quot;@anakinskywalker&quot; },        { &quot;skype&quot;: &quot;AlwaysWithYou&quot; }    ]}Mamy więc dokumenty w dwóch różnych wersjach, a nasz kod musi poradzić sobie z obsługą obu wersji. Dodatkowo, wskazane jest, żeby rozróżniać wersje na podstawie wartości pola schema_version (w przeciwieństwie do wykrywania np. czy dane pole istnieje). Możemy równocześnie wykonać w tle masową aktualizację i zadecydować sami, kiedy ma się wydarzyć. W kolejnej wersji aplikacji możemy też zdecydować o pozbyciu się kodu obsługującego obydwie wersje.Zauważmy jednak, że w skrajnym przypadku w okresie przejściowym będziemy potrzebować więcej indeksów - dotychczasowy, wspierający poprzednią wersję i nowy, wspierający nową wersję.Osobną kwestią, o której nie można zapomnieć, jest walidacja takiego schematu. Warto pamiętać, że chociaż Mongo, jako baza NoSQL, uznawana jest za schemaless, to pozwala na walidacje schematu, np. wymagalności pól.PodsumowaniePlusy:  bezprzerwowa migracja,  możliwość zarządzania migracją (kiedy i jak się odbędzie).Minusy:  może być konieczność utrzymywania 2 indeksów do czasu zakończenia migracji,  jeśli korzystamy z walidacji schematu, może wystąpić konieczność dostosowania walidacji do dwóch wersji.Przydatne linki  https://www.mongodb.com/blog/post/building-with-patterns-the-schema-versioning-pattern  https://docs.mongodb.com/manual/core/schema-validation/",
"url": "/2024/01/26/czy-wiesz-jak-bezprzerwowo-zmienic-schemat-w-mongodb.html",
"author": "Barbara Mitan",
"authorUrl": "/authors/bmitan.html",
"image": "bmitan.jpg",
"highlight": "/assets/img/posts/2024-02-09-czy-wiesz-jak-bezprzerwowo-zmienic-schemat-w-mongodb/code.webp",
"date": "26-01-2024",
"path": "pl/2024-02-09-czy-wiesz-jak-bezprzerwowo-zmienic-schemat-w-mongodb"
}
,


"2024-01-26-czy-wiesz-czym-jest-attribute-pattern-w-mongodb-html": {
"title": "Czy wiesz czym jest Attribute Pattern w MongoDB?",
"lang": "pl",
"tags": "mongodb attribute pattern",
"content": "Kiedy stosować Attribute Pattern?Wzorzec warto zastosować, kiedy:  mamy wiele dużych dokumentów z wieloma podobnymi polami, ale istnieje pewien podzbiór pól o tej samej charakterystyce, a chcemy pytać i sortować właśnie po tym podzbiorze pól, lub  tylko niewielka część dokumentów zawiera pola, po których chcemy sortować, lub  oba powyższe warunki występują jednocześnie.Przykłady:Kolekcja filmów - pola daty premier w poszczególnych krajach{    title: &quot;Star Wars&quot;,    director: &quot;George Lucas&quot;,    // ...    release_US: ISODate(&quot;1977-05-20T01:00:00+01:00&quot;),    release_France: ISODate(&quot;1977-10-19T01:00:00+01:00&quot;),    release_Italy: ISODate(&quot;1977-10-20T01:00:00+01:00&quot;),    release_UK: ISODate(&quot;1977-12-27T01:00:00+01:00&quot;)}Kolekcja utworów muzycznych{    title: &#39;What is love?&#39;,    // ...    streams_spotify: 23854235,    streams_tidal: 23432,    streams_youtube: 9993453,    streams_apple: 345665}Na czym polega problem?Chodzi o wydajność zapytań. W pierwszym przykładzie zapytanie po dacie premiery będzie wymagało indeksu na każdym z pól release_*.  Jeśli mamy wiele krajów, to zarządzanie tymi indeksami może być trudne, obsługa nowych wydań, np. w nowym kraju albo na festiwalu oznacza konieczność dodania jeszcze kolejnego indeksu.Indeksy - przed zastosowaniem AttributePattern[    {release_US: 1},    {release_France: 1},    {release_Italy: 1},    {release_UK: 1},    // ...]Na czym polega Attribute Pattern?Dane można zamodelować inaczej. Zamiast wielu osobnych pól wystarczy wprowadzić jedno pole typu Array, zawierające pary klucz-wartość. W przykładzie kolekcji filmów, kluczami będą kolejne lokalizacje, a wartościami daty.Po zastosowaniu wzorca otrzymamy dokumenty w następującej formie:{    title: &quot;Star Wars&quot;,    director: &quot;George Lucas&quot;,    // ...    releases: [        {        location: &quot;USA&quot;,        date: ISODate(&quot;1977-05-20T01:00:00+01:00&quot;)        },        {        location: &quot;France&quot;,        date: ISODate(&quot;1977-10-19T01:00:00+01:00&quot;)        },        {        location: &quot;Italy&quot;,        date: ISODate(&quot;1977-10-20T01:00:00+01:00&quot;)        },        {        location: &quot;UK&quot;,        date: ISODate(&quot;1977-12-27T01:00:00+01:00&quot;)        },        // ...    ],    // ...}Plusy takiego rozwiązania to:  łatwe dodanie nowych lokalizacji,  jeden indeks: { &quot;releases.location&quot;: 1, &quot;releases.date&quot;: 1}  łatwe dodawanie kolejnych pól charakteryzujących nasze dane,  szybsze zapytania.Praktyczny przypadek użyciaInny przypadek użycia to kolekcja produktów, posiadających różne charakterystyki w zależności od typu. Przykładowo ubrania mogą mieć rozmiar S, M, L.  Natomiast kubki mogą mieć pojemność podaną w różnych jednostkach. Jeszcze inne produkty będą miały wysokość, szerokość, długość, czy masę, również w różnych jednostkach. Niektóre z tych charakterystyk nie są znane na etapie projektowania aplikacji i mogą zostać wprowadzone później. Załóżmy, że chcemy zapewnić użytkownikowi możliwość wyszukiwania po wielu polach i móc łatwo dodać nowy rodzaj filtra. Tu również wzorzec Attribute Pattern okaże się idealnym rozwiązaniem. Kubek może mieć np. następujące dane:&quot;specs&quot;: [    { k: &quot;volume&quot;, v: &quot;500&quot;, u: &quot;ml&quot; },    { k: &quot;volume&quot;, v: &quot;12&quot;, u: &quot;ounces&quot; }]a szafka zupełnie inne dane, ale w tym samym polu specs:&quot;specs&quot;: [    { k: &quot;height&quot;, v: &quot;100&quot;, u: &quot;cm&quot; },    { k: &quot;height&quot;, v: &quot;39.37&quot;, u: &quot;inches&quot; },    // ...,    { k: &quot;weight&quot;, v: &quot;10&quot;, u: &quot;kg&quot;}]Aby efektywnie przeszukiwać taką strukturę potrzebujemy tylko jednego indeksu:{&quot;specks.k&quot;: 1, &quot;specs.v&quot;: 1, &quot;specs.u&quot;: 1}Otrzymujemy bardzo elastyczny model danych, pozwalający na skomplikowane i wydajne wyszukiwanie.Więcej informacjihttps://www.mongodb.com/developer/how-to/attribute-pattern/",
"url": "/2024/01/26/czy-wiesz-czym-jest-attribute-pattern-w-mongodb.html",
"author": "Barbara Mitan",
"authorUrl": "/authors/bmitan.html",
"image": "bmitan.jpg",
"highlight": "/assets/img/posts/2024-01-26-czy-wiesz-czym-jest-attribute-pattern-w-mongodb/code.webp",
"date": "26-01-2024",
"path": "pl/2024-01-26-czy-wiesz-czym-jest-attribute-pattern-w-mongodb"
}
,


"2024-01-12-czy-wiesz-co-to-style-processor-options-w-angular-html": {
"title": "Czy wiesz co to stylePreprocessorOptions w Angular?",
"lang": "pl",
"tags": "angular css",
"content": "StylePreprocessorOptions pozwala na dynamiczne dodawanie stylów do aplikacji. Zamiast używać relatywnych ścieżek do pliku ze stylami:// Relatywna ścieżka@import &#39;libs/feature-theme-A/src/style/palette&#39;;można zaimportować style w taki sposób:// Po dodaniu stylePreprocessorOptions, można tak:@import &#39;palette&#39;;Co to daje? Załóżmy, że mamy aplikację A oraz B, które korzystają z tych samych komponentów. Przedstawione rozwiązanie umożliwia zmianę stylów w zależności, w której aplikacji używamy określonego komponentu nie tracąc jednocześnie enkapsulacji stylów w Angularze. Możemy tak zaimportować zmienne, dodatkowe klasy czy mixiny.Dodanie StylePreprocessorOptions w aplikacji sprowadza się do wskazania odpowiednich ścieżek w projekcie:&quot;stylePreprocessorOptions&quot;: {    &quot;includePaths&quot;: [&quot;libs/feature-theme-A/src/style&quot;]}Miłego kolorowania!",
"url": "/2024/01/12/czy-wiesz-co-to-style-processor-options-w-angular.html",
"author": "Dorian Mejer",
"authorUrl": "/authors/dmejer.html",
"image": "dmejer.jpg",
"highlight": "/assets/img/posts/2024-01-12-czy-wiesz-co-to-style-processor-options-w-angular/css.webp",
"date": "12-01-2024",
"path": "pl/2024-01-12-czy-wiesz-co-to-style-processor-options-w-angular"
}
,


"2023-12-29-czy-wiesz-ze-w-typescript-mozesz-laczyc-typy-za-pomoca-interpolacji-ciagow-znakow-html": {
"title": "Czy wiesz, że w TypeScript możesz łączyć typy za pomocą interpolacji ciągów znaków?",
"lang": "pl",
"tags": "typescript literal types union types",
"content": "W TypeScript możemy tworzyć swoje własne typy (Literal Types) w następujący sposób:type lang = &#39;pl&#39; | &#39;en&#39;;gdzie używamy znaku | aby móc zdefiniować kilka różnych typów (Union Types).W TypeScript 4.1 pojawiła się możliwość łączenia ze sobą typów za pomocą interpolacji ciągów:type lang = &#39;pl&#39; | &#39;en&#39;;type textType = &#39;description&#39; | &#39;title&#39;;type textType = &#39;${textType}_${lang};Dzięki temu możemy otrzymać każdą możliwą kombinację typów w ramach nowego typu. Dla tego przykładu będą to: description_pl, description_en, title_pl, title_en.Więcej informacji znajdziecie pod adresem: https://www.typescriptlang.org/docs/handbook/2/template-literal-types.html",
"url": "/2023/12/29/czy-wiesz-ze-w-typescript-mozesz-laczyc-typy-za-pomoca-interpolacji-ciagow-znakow.html",
"author": "Piotr Grobelny",
"authorUrl": "/authors/pgrobelny.html",
"image": "pgrobelny.webp",
"highlight": "/assets/img/posts/2023-12-29-czy-wiesz-ze-w-typescript-mozesz-laczyc-typy-za-pomoca-interpolacji-ciagow-znakow/code.webp",
"date": "29-12-2023",
"path": "pl/2023-12-29-czy-wiesz-ze-w-typescript-mozesz-laczyc-typy-za-pomoca-interpolacji-ciagow-znakow"
}
,


"2023-12-15-czy-wiesz-czym-include-rozni-sie-od-some-html": {
"title": "Czy wiesz czym .includes() różni się od .some()?",
"lang": "pl",
"tags": "javascript",
"content": "includes() pozwala nam sprawdzić, czy element Y znajduje się w tablicy, np.:const zbior = [1,2,3,4,5,6,7];const wynik = zbior.includes(3);console.log(wynik); // true, 3 znajduje się w zbiorze liczbincludes() porównuje obiekty przez referencje, świetnie sprawdzi się w prostych typach, ale przy obiektach może nie zadziałać zgodnie z oczekiwaniem, np.:const bogurodzica = {name: &#39;Bogurodzica&#39;};const songs = [bogurodzica, {name: &#39;Cicha noc&#39;}, {name: &#39;Lulajże&#39;}]; const wynik = songs.includes({name: &#39;Bogurodzica&#39;});console.log(wynik); // false, jest to inny obiekt const wynikRef = songs.includes(bogurodzica);console.log(wynikRef); // trueRozwiązaniem tego problemu jest użycie funkcji some(fn). Iteruje ona po elementach i sprawdza warunek (fn) podany dla konkretnego elementu. Funkcja zwróci true, gdy dowolny element spełni warunek lub false, gdy żaden element nie pasuje do warunku, np.:const songs = [{name: &#39;Bogurodzica&#39;}, {name: &#39;Cicha noc&#39;}, {name: &#39;Lulajże&#39;}]; const wynik = songs.some((song) =&amp;gt; song.name === &#39;Bogurodzica&#39;);console.log(wynik); // true const brak = songs.some((song) =&amp;gt; song.name === &#39;Master of puppets&#39;);console.log(brak); // falseMiłego przeszukiwania tablic!",
"url": "/2023/12/15/czy-wiesz-czym-include-rozni-sie-od-some.html",
"author": "Dorian Mejer",
"authorUrl": "/authors/dmejer.html",
"image": "dmejer.jpg",
"highlight": "/assets/img/posts/2023-12-15-czy-wiesz-czym-include-rozni-sie-od-some/javascript.webp",
"date": "15-12-2023",
"path": "pl/2023-12-15-czy-wiesz-czym-include-rozni-sie-od-some"
}
,


"2023-12-01-czy-wiesz-jak-uzywac-discriminated-union-w-jezyku-typescript-html": {
"title": "Czy wiesz jak używać discriminated union w języku TypeScript?",
"lang": "pl",
"tags": "typescript discriminated union literal type",
"content": "Discriminated union (pol. unia dyskryminacyjna) to połączenie takich typów, z których każdy posiada jedno wspólne pole, na podstawie którego możemy określić, z jakim z nich mamy do czynienia. Zgodnie z ogólnie przyjętą konwencją jest to pole type, które jest typu Literal Type, czyli jego typ opisuje się za pomocą stringa.Zdefiniujmy unię dyskryminacyjną oraz funkcję anonimową przypisaną do getPrice, która rozpoznaje jej typ i wykonuje zależny od niego kod:type Invoice = {  type: &#39;INVOICE&#39;,  number: string,  date: Date  positions: {    name: string    price: number    quantity: number  }[],   } type Bill = {  type: &#39;BILL&#39;,  date: Date,  totalPrice: number} type CompanyPurchase = Invoice | Bill; const getPrice = (purchase: CompanyPurchase): number =&amp;gt; {  switch(purchase.type) {    case &#39;INVOICE&#39;:      return purchase.positions.reduce((acc, item) =&amp;gt; acc + item.price * item.quantity, 0);    case &#39;BILL&#39;:      return purchase.totalPrice;    default:      return 0;  }}Typy Invoice oraz Bill mają wspólne pole type. Dzięki zastosowaniu unii dyskryminacyjnej TypeScript sam rozpoznaje konkretny typ parametru purchase w funkcji anonimowej przypisanej do zmiennej getPrice, rzutowania są zbędne.Przykład działania: https://www.typescriptlang.org/play#code/C(…)A",
"url": "/2023/12/01/czy-wiesz-jak-uzywac-discriminated-union-w-jezyku-typescript.html",
"author": "Krzysztof Czechowski",
"authorUrl": "/authors/kczechowski.html",
"image": "kczechowski.jpg",
"highlight": "/assets/img/posts/2023-12-01-czy-wiesz-jak-uzywac-discriminated-union-w-jezyku-typescript/code.webp",
"date": "01-12-2023",
"path": "pl/2023-12-01-czy-wiesz-jak-uzywac-discriminated-union-w-jezyku-typescript"
}
,


"2023-11-17-czy-wiesz-jak-nalezy-trzymac-powiazane-dane-w-jednym-dokumencie-w-bazie-mongodb-html": {
"title": "Czy wiesz, jak należy trzymać powiązane dane w jednym dokumencie w bazie MongoDB?",
"lang": "pl",
"tags": "mongodb subset extended reference bazy danych",
"content": "Ważna zasada przy projektowaniu schematu w MongoDB mówi, żeby dane, do których dostęp jest zazwyczaj wspólny, przechowywać razem. Jest to całkowicie sprzeczne z koncepcją normalizacji – dzieleniem danych tak, żeby uniknąć duplikowania i ograniczyć potrzebne miejsce na dane – czyli ze wszystkim, co znamy z relacyjnych baz danych.Warto podkreślić, że odpowiednikiem JOINów w zapytaniu SQL jest operacja $lookup, która pozwala łączyć dane z kilku kolekcji. Operacja ta jest jednak wolna i kosztowna, dlatego jest wskazana tylko dla rzadko wykonywanych zapytań, na które można długo zaczekać lub zapytań analitycznych, które można wykonać bez limitu czasu (np. na osobnym przeznaczonym do takich zapytań węźle). Dla zapytań, które mają być wykonywane często i oczekujemy ich wyników szybko, istotne będzie odpowiednie zamodelowanie danych. Pomóc w tym mogą m.in. wzorce Subset i Extended Reference.Wyobraźmy sobie, że tworzymy serwis z książkami. Dla każdego autora chcemy szybko zaprezentować jego najnowsze książki (tytuł, rok wydania i zdjęcie okładki bez pozostałych szczegółowych informacji), cytaty (nie będzie ich dużo) oraz najnowsze recenzje na jego temat. Mamy więc kolekcję authors, kolekcję books, kolekcję comments i kolekcję quotes, ale spróbujmy zamodelować dane trochę inaczej.Dane w kolekcjach prezentują się następująco:  authors    {  &quot;_id&quot;: 234,  &quot;name&quot;: &quot;Robert C. Martin&quot;,  // ...}        books    {  &quot;_id&quot;: 123454,  &quot;name&quot;: &quot;Clean Code, A Handbook of Agile Software Craftsmanship&quot;,  &quot;year&quot;: 2009,  &quot;author_id&quot;: 234,  &quot;publisher&quot;: &quot;Prentice Hall&quot;,  &quot;img&quot;: &quot;https://bookpage/images?id=4567456&quot;,  &quot;isbn13&quot;: 9780132350884,  &quot;isbn10&quot;: 9780132350884,  // ...  &quot;summary&quot;: &quot;...&quot;}        comments    {  &quot;_id&quot;: 12454356,  &quot;author_id&quot;: 234,  &quot;comment_author&quot;: &quot;Marek&quot;,  &quot;title&quot;: &quot;Great author&quot;,  &quot;text&quot;: &quot;I read all his books!&quot;,  &quot;date&quot;: &quot;2021-05-26T07:23:12.000Z&quot;}        quotes    {  &quot;_id&quot;: 43,  &quot;author_id&quot;: 234,  &quot;text&quot;: &quot;Truth can only be found in one place: the code.&quot;,  &quot;tags&quot;: [&quot;programming&quot;, &quot;software&quot;]}      Najprostsza sytuacja dotyczy cytatów. Ponieważ cytaty przeglądamy zawsze razem z autorami i nie może być ich zbyt wiele, decydujemy się całkowicie usunąć kolekcję quotes i przenieść cytaty jako poddokument do kolekcji authors.Inaczej będzie z komentarzami. Zakładamy, że może być ich wiele, a sam komentarz może być dosyć długi. Nie chcemy przenieść wszystkich komentarzy do dokumentu autora, bo skończymy ze zbyt dużym dokumentem. Zamiast tego podzielimy komentarze w dwie kolekcje - najświeższe dane będziemy przechowywać w dokumencie autora, a starsze w kolekcji comments. Taki wzorzec to Subset Pattern.W przypadku najnowszych książek jest podobnie - potrzebujemy na wejście tylko kilku najnowszych książek. Tak naprawdę nie potrzebujemy też wszystkich informacji o tych książkach. Wystarczy tytuł, rok wydania i URL do zdjęcia książki. Będziemy więc nadal trzymać książki w kolekcji books i skrócone informacje o najnowszych książkach danego autora w kolekcji authors. Zastosowaliśmy właśnie wzorzec Extended Reference. Warto zwrócić uwagę, że powoduje to duplikację danych. Są to dane odczytywane często, ale idealna sytuacja to taka, w której nie są często aktualizowane. Musimy zawsze zastanowić się, co zrobić w przypadku aktualizacji dokumentu - w tym przypadku książki. Czy musimy zaktualizować dane również w dokumencie autora? W przypadku tego wzorca strategie są różne. Może to być np. asynchroniczna aktualizacja danych lub zachowanie historycznych danych (nie zawsze aktualizacja ma sens biznesowo). W naszym przypadku trzymamy w kolekcji authors dane o książkach, które nie powinny się prawie nigdy zmieniać.Ostatecznie nasz dokument w kolekcji authors będzie wyglądał tak:{    &quot;_id&quot;: 234,    &quot;name&quot;: &quot;Robert C. Martin&quot;,    // ...,    &quot;quotes&quot;: [        {            &quot;_id&quot;: 43,            &quot;author_id&quot;: 234,            &quot;text&quot;: &quot;Truth can only be found in one place: the code.&quot;,            &quot;tags&quot;: [&quot;programming&quot;, &quot;software&quot;]        }    ],    &quot;recent_comments&quot;: [        {            &quot;_id&quot;: 12454356,            &quot;author_id&quot;: 234,            &quot;comment_author&quot;: &quot;Marek&quot;,            &quot;title&quot;: &quot;Great author&quot;,            &quot;text&quot;: &quot;I read all his books!&quot;,            &quot;date&quot;: &quot;2021-05-26T07:23:12.000Z&quot;        },        // ...    ],    &quot;recent_books&quot;: [        {            &quot;_id&quot;: 123454,            &quot;name&quot;: &quot;Clean Code, A Handbook of Agile Software Craftsmanship&quot;,            &quot;year&quot;: &quot;2009&quot;,            &quot;author_id&quot;: 234,            &quot;img&quot;: &quot;https://bookpage/images?id=4567456&quot;        },        // ...    ]}Subset PatternKiedy stosować?  mamy w dokumencie dużo rzadko odczytywanych danych,  możemy podzielić te dane na dane historyczne i bieżące, występują relacje typu 1-N i N-N,  typowe przykłady to komentarze, recenzje produktu, aktorzy w filmach.Plusy:  szybsze odczyty.Minusy:  trochę bardziej skomplikowana logika.Extended Reference PatternKiedy stosować?  występują relacje N-1, konieczność zastosowania operacji $lookup, za dużo “joinów” i w efekcie problemy z wydajnością,  typowe przykłady to aplikacja do zarządzania zamówieniami, np. powiązania między zamówieniami, produktami a klientami, zamówieniami a klientami, fakturami a dostawcami i klientami,Plusy:  szybsze odczyty.Minusy:  duplikacja danych,  bardziej skomplikowana logika,  może wystąpić konieczność aktualizacji wielu dokumentów przy aktualizacji danych, które są zduplikowane.Przydatne linki:  Building with patterns: The Subset Pattern  Building with Patterns: The Extended Reference Pattern  Schema design anti pattern: Separating Data That is Accessed Together",
"url": "/2023/11/17/czy-wiesz-jak-nalezy-trzymac-powiazane-dane-w-jednym-dokumencie-w-bazie-mongodb.html",
"author": "Barbara Mitan",
"authorUrl": "/authors/bmitan.html",
"image": "bmitan.jpg",
"highlight": "/assets/img/posts/2023-11-17-czy-wiesz-jak-nalezy-trzymac-powiazane-dane-w-jednym-dokumencie-w-bazie-mongodb/data.webp",
"date": "17-11-2023",
"path": "pl/2023-11-17-czy-wiesz-jak-nalezy-trzymac-powiazane-dane-w-jednym-dokumencie-w-bazie-mongodb"
}
,


"2023-11-03-czy-wiesz-czym-jest-aspect-ratio-i-jak-go-uzywac-html": {
"title": "Czy wiesz, czym jest aspect-ratio i jak go używać?",
"lang": "pl",
"tags": "css aspect-ratio",
"content": "Niedawno w przeglądarkach pojawiło się wsparcie dla nowego atrybutu w CSS - aspect-ratio (caniuse). Jak nazwa sugeruje, służy on do określania proporcji elementu.Przyjmuje on dwa parametry oddzielone ukośnikiem:aspect-ratio: 1 / 2;Określa on stosunek wysokości do szerokości obiektu. Dla podanego wyżej przykładu, jeżeli wysokość ustawimy na 100px to szerokość automatycznie ustawi się na 50px. Jeżeli w stylu zdefiniujemy wysokość i szerokość, wtedy atrybut aspect-ratio zostanie zignorowany.Poniżej przykład, na którym można zaobserwować działanie omawianego parametru:https://codepen.io/Porkite/pen/xxPqyzR",
"url": "/2023/11/03/czy-wiesz-czym-jest-aspect-ratio-i-jak-go-uzywac.html",
"author": "Piotr Grobelny",
"authorUrl": "/authors/pgrobelny.html",
"image": "pgrobelny.webp",
"highlight": "/assets/img/posts/2023-11-03-czy-wiesz-czym-jest-aspect-ratio-i-jak-go-uzywac/measure.webp",
"date": "03-11-2023",
"path": "pl/2023-11-03-czy-wiesz-czym-jest-aspect-ratio-i-jak-go-uzywac"
}
,


"2023-10-24-jak-zmusic-mongodb-do-uzycia-indeksu-bez-zmiany-kodu-html": {
"title": "Jak zmusić MongoDB do użycia indeksu bez zmiany kodu - zastosowanie index filter",
"lang": "pl",
"tags": "mongodb index planCacheSetFilter index filter query shape",
"content": "Optymalizatory zapytańOptymalizator zapytań to element silnika bazy danych, który dba o to, aby zapytanie zostało wykonane w optymalny sposób, uwzględniając zbiór danych przechowywanych w danym momencie w bazie. Pod pojęciem optymalny mamy zazwyczaj na myśli taki sposób, który zwróci nam wynik zapytania w najkrótszym czasie. Optymalizator bierze pod uwagę statystyki gromadzone i aktualizowane na bieżąco podczas działania bazy danych. Optymalizatory są wbudowane zarówno w bazy SQL’owe jak i bazy NoSQL. Sposób działania optymalizatora dla MongoDB możemy znaleźć na stronie: https://www.mongodb.com/docs/manual/core/query-plans/. W znakomitej większości przypadków optymalizatory są dużym ułatwieniem dla programistów, którzy nie muszą poświęcać czasu na analizę rozkładu danych w poszczególnych tabelach/kolekcjach i samodzielną optymalizację wykonywanych zapytań. Ponieważ optymalizator działa bez kontroli programisty, zdarzają się sytuacje, w których jego zachowanie jest dla nas zaskakujące i może prowadzić do problemów wydajnościowych.Analiza problemów wydajnościowychJeżeli mamy podejrzenie, że problemy z wydajnością naszego systemu mogą być związane z bazą MongoDB istnieje dość prosty sposób, który pozwala na potwierdzenie lub wykluczenie takiej tezy. Należy mianowicie ustawić próg czasowy, przy którym MongoDB będzie logowało przekraczające go zapytania:https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-operationProfiling.slowOpThresholdMs.W analizowanym przez nas przypadku w logu diagnostycznym znajdowało się wiele wpisów, których czas wykonania przekraczał 10 sekund:2023-10-01T14:50:35.652+0100 I COMMAND  [conn121] command formstore_db.formModel command: find { find: &quot;formModel&quot;, readConcern: { level: &quot;majority&quot; }, filter: { formFields.formInstanceNumber.value: { $in: [ &quot;EXI123456789&quot;, (...) &quot;EXI987654321&quot; ] }, formType: &quot;some_form&quot; }, sort: { _id: -1 }, projection: { formType: 1, (...) lastUpdateTime: 1 }, $db: &quot;formstore_db&quot;, $clusterTime: { clusterTime: Timestamp(1671457810, 3), signature: { hash: BinData(0, 9A2225836AC174D40666995D6954154960FE67DB), keyId: 7137070794986749953 } }, lsid: { id: UUID(&quot;36e6cf62-4ad2-4b34-8a56-227d283da075&quot;) } } planSummary: IXSCAN { _id: 1 } keysExamined:254713 docsExamined:254713 fromMultiPlanner:1 replanned:1 cursorExhausted:1 numYields:2213 nreturned:75 reslen:289780 locks:{ Global: { acquireCount: { r: 2214 } }, Database: { acquireCount: { r: 2214 } }, Collection: { acquireCount: { r: 2214 } } } storage:{ data: { bytesRead: 15335932371, timeReadingMicros: 10566758 }, timeWaitingMicros: { cache: 56 } } protocol:op_msg 11597msLinijka logu jest dość długa. Na jej końcu mamy podany czas wykonania zapytania: protocol:op_msg 11597ms. Czas wykonania zdecydowanie przekraczał oczekiwaną wartość, co pozwoliło potwierdzić tezę, że przyczyna problemów wydajnościowych leży po stronie bazy danych. Warto zwrócić jeszcze uwagę na następującą informację w logu:planSummary: IXSCAN { _id: 1 } keysExamined:254713 docsExamined:254713.Wskazuje ona na to, że użyty został standardowy indeks MongoDB zakładany na identyfikatorze kolekcji _id. Oczekiwaliśmy tutaj raczej użycia indeksu założonego na polu, po którym odbywało się filtrowanie czyli na formFields.formInstanceNumber.value. W takim przypadku pierwsze podejrzenie padło na możliwy brak indeksu. W celu potwierdzenia lub zaprzeczenia tej możliwości pobraliśmy indeksy założone na problematycznej kolekcji: db.formModel.getIndexes();. Odpowiedź wskazywała jednak, że wymagany indeks został założony:[  {    &quot;v&quot;: 2,    &quot;key&quot;: {      &quot;_id&quot;: 1    },    &quot;name&quot;: &quot;_id_&quot;,    &quot;ns&quot;: &quot;formstore_db.formModel&quot;  },  {    &quot;v&quot;: 2,    &quot;key&quot;: {      &quot;formFields.formInstanceNumber.value&quot;: 1    },    &quot;name&quot;: &quot;formFields.formInstanceNumber.value_1&quot;,    &quot;ns&quot;: &quot;formstore_db.formModel&quot;,    &quot;background&quot;: true  }]Sprawdzamy plan zapytaniaW takim przypadku warto przeprowadzić analizę planu zapytania. Co ważne, analizę należy przeprowadzić na środowisku, na którym wystąpiły problemy, gdyż wyniki pracy optymalizatora są zależne od danych znajdujących się w bazie oraz statystyk zbieranych podczas jej działania. Wyświetlenie wybranego planu zapytania oraz planów alternatywnych realizujemy poprzez wywołanie zapytania wykonywanego przez aplikację, w którym na końcu dodajemy: .explain(&quot;allPlansExecution&quot;). W analizowanym przypadku otrzymaliśmy następujący wynik:{  &quot;queryPlanner&quot;: {    &quot;plannerVersion&quot;: 1,    &quot;namespace&quot;: &quot;formstore_db.formModel&quot;,    &quot;indexFilterSet&quot;: false,    &quot;parsedQuery&quot;: {      &quot;$and&quot;: [        {          &quot;formType&quot;: {            &quot;$eq&quot;: &quot;some_form&quot;          }        },        {          &quot;formFields.formInstanceNumber.value&quot;: {            &quot;$in&quot;: [              &quot;EXI123456789&quot;,              // ...              &quot;EXI987654321&quot;            ]          }        }      ]    },    &quot;winningPlan&quot;: {      &quot;stage&quot;: &quot;PROJECTION&quot;,      &quot;transformBy&quot;: {        &quot;formType&quot;: 1,        // ...        &quot;lastUpdateTime&quot;: 1      },      &quot;inputStage&quot;: {        &quot;stage&quot;: &quot;FETCH&quot;,        &quot;filter&quot;: {          &quot;$and&quot;: [            {              &quot;formType&quot;: {                &quot;$eq&quot;: &quot;some_form&quot;              }            },            {              &quot;formFields.formInstanceNumber.value&quot;: {                &quot;$in&quot;: [                  &quot;EXI123456789&quot;,                  // ...                  &quot;EXI987654321&quot;                ]              }            }          ]        },        &quot;inputStage&quot;: {          &quot;stage&quot;: &quot;IXSCAN&quot;,          &quot;keyPattern&quot;: {            &quot;_id&quot;: 1          },          &quot;indexName&quot;: &quot;_id_&quot;,          &quot;isMultiKey&quot;: false,          &quot;multiKeyPaths&quot;: {            &quot;_id&quot;: [ ]          },          &quot;isUnique&quot;: true,          &quot;isSparse&quot;: false,          &quot;isPartial&quot;: false,          &quot;indexVersion&quot;: 2,          &quot;direction&quot;: &quot;backward&quot;,          &quot;indexBounds&quot;: {            &quot;_id&quot;: [              &quot;[MaxKey, MinKey]&quot;            ]          }        }      }    },    &quot;rejectedPlans&quot;: [      {        &quot;stage&quot;: &quot;PROJECTION&quot;,        &quot;transformBy&quot;: {          &quot;formType&quot;: 1,          // ...          &quot;lastUpdateTime&quot;: 1        },        &quot;inputStage&quot;: {          &quot;stage&quot;: &quot;SORT&quot;,          &quot;sortPattern&quot;: {            &quot;_id&quot;: -1          },          &quot;inputStage&quot;: {            &quot;stage&quot;: &quot;SORT_KEY_GENERATOR&quot;,            &quot;inputStage&quot;: {              &quot;stage&quot;: &quot;FETCH&quot;,              &quot;filter&quot;: {                &quot;formType&quot;: {                  &quot;$eq&quot;: &quot;some_form&quot;                }              },              &quot;inputStage&quot;: {                &quot;stage&quot;: &quot;IXSCAN&quot;,                &quot;keyPattern&quot;: {                  &quot;formFields.formInstanceNumber.value&quot;: 1                },                &quot;indexName&quot;: &quot;formFields.formInstanceNumber.value_1&quot;,                &quot;isMultiKey&quot;: false,                &quot;multiKeyPaths&quot;: {                  &quot;formFields.formInstanceNumber.value&quot;: [ ]                },                &quot;isUnique&quot;: false,                &quot;isSparse&quot;: false,                &quot;isPartial&quot;: false,                &quot;indexVersion&quot;: 2,                &quot;direction&quot;: &quot;forward&quot;,                &quot;indexBounds&quot;: {                  &quot;formFields.formInstanceNumber.value&quot;: [                    &quot;[\\&quot;EXI123456789\\&quot;, \\&quot;EXI123456789\\&quot;]&quot;,                    // ...                    &quot;[\\&quot;EXI987654321\\&quot;, \\&quot;EXI987654321\\&quot;]&quot;                  ]                }              }            }          }        }      },      {        &quot;stage&quot;: &quot;PROJECTION&quot;,        &quot;transformBy&quot;: {          &quot;formType&quot;: 1,          // ...          &quot;lastUpdateTime&quot;: 1        },        &quot;inputStage&quot;: {          &quot;stage&quot;: &quot;SORT&quot;,          &quot;sortPattern&quot;: {            &quot;_id&quot;: -1          },          &quot;inputStage&quot;: {            &quot;stage&quot;: &quot;SORT_KEY_GENERATOR&quot;,            &quot;inputStage&quot;: {              &quot;stage&quot;: &quot;FETCH&quot;,              &quot;filter&quot;: {                &quot;formFields.formInstanceNumber.value&quot;: {                  &quot;$in&quot;: [                    &quot;EXI123456789&quot;,                    // ...                    &quot;EXI987654321&quot;                  ]                }              },              &quot;inputStage&quot;: {                &quot;stage&quot;: &quot;IXSCAN&quot;,                &quot;keyPattern&quot;: {                  &quot;formType&quot;: 1                },                &quot;indexName&quot;: &quot;formType_1&quot;,                &quot;isMultiKey&quot;: false,                &quot;multiKeyPaths&quot;: {                  &quot;formType&quot;: [ ]                },                &quot;isUnique&quot;: false,                &quot;isSparse&quot;: false,                &quot;isPartial&quot;: false,                &quot;indexVersion&quot;: 2,                &quot;direction&quot;: &quot;forward&quot;,                &quot;indexBounds&quot;: {                  &quot;formType&quot;: [                    &quot;[\\&quot;some_form\\&quot;, \\&quot;some_form\\&quot;]&quot;                  ]                }              }            }          }        }      }    ]  },  &quot;executionStats&quot;: {    &quot;executionSuccess&quot;: true,    &quot;nReturned&quot;: 75,    &quot;executionTimeMillis&quot;: 11651,    &quot;totalKeysExamined&quot;: 254851,    &quot;totalDocsExamined&quot;: 254851,    &quot;executionStages&quot;: {      &quot;stage&quot;: &quot;PROJECTION&quot;,      &quot;nReturned&quot;: 75,      &quot;executionTimeMillisEstimate&quot;: 10335,      &quot;works&quot;: 254852,      &quot;advanced&quot;: 75,      &quot;needTime&quot;: 254776,      &quot;needYield&quot;: 0,      &quot;saveState&quot;: 2212,      &quot;restoreState&quot;: 2212,      &quot;isEOF&quot;: 1,      &quot;invalidates&quot;: 0,      &quot;transformBy&quot;: {        &quot;formType&quot;: 1,        // ...        &quot;lastUpdateTime&quot;: 1      },      &quot;inputStage&quot;: {        &quot;stage&quot;: &quot;FETCH&quot;,        &quot;filter&quot;: {          &quot;$and&quot;: [            {              &quot;formType&quot;: {                &quot;$eq&quot;: &quot;some_form&quot;              }            },            {              &quot;formFields.formInstanceNumber.value&quot;: {                &quot;$in&quot;: [                  &quot;EXI123456789&quot;,                  // ...                  &quot;EXI987654321&quot;                ]              }            }          ]        },        &quot;nReturned&quot;: 75,        &quot;executionTimeMillisEstimate&quot;: 10325,        &quot;works&quot;: 254852,        &quot;advanced&quot;: 75,        &quot;needTime&quot;: 254776,        &quot;needYield&quot;: 0,        &quot;saveState&quot;: 2212,        &quot;restoreState&quot;: 2212,        &quot;isEOF&quot;: 1,        &quot;invalidates&quot;: 0,        &quot;docsExamined&quot;: 254851,        &quot;alreadyHasObj&quot;: 0,        &quot;inputStage&quot;: {          &quot;stage&quot;: &quot;IXSCAN&quot;,          &quot;nReturned&quot;: 254851,          &quot;executionTimeMillisEstimate&quot;: 120,          &quot;works&quot;: 254852,          &quot;advanced&quot;: 254851,          &quot;needTime&quot;: 0,          &quot;needYield&quot;: 0,          &quot;saveState&quot;: 2212,          &quot;restoreState&quot;: 2212,          &quot;isEOF&quot;: 1,          &quot;invalidates&quot;: 0,          &quot;keyPattern&quot;: {            &quot;_id&quot;: 1          },          &quot;indexName&quot;: &quot;_id_&quot;,          &quot;isMultiKey&quot;: false,          &quot;multiKeyPaths&quot;: {            &quot;_id&quot;: [ ]          },          &quot;isUnique&quot;: true,          &quot;isSparse&quot;: false,          &quot;isPartial&quot;: false,          &quot;indexVersion&quot;: 2,          &quot;direction&quot;: &quot;backward&quot;,          &quot;indexBounds&quot;: {            &quot;_id&quot;: [              &quot;[MaxKey, MinKey]&quot;            ]          },          &quot;keysExamined&quot;: 254851,          &quot;seeks&quot;: 1,          &quot;dupsTested&quot;: 0,          &quot;dupsDropped&quot;: 0,          &quot;seenInvalidated&quot;: 0        }      }    },    &quot;allPlansExecution&quot;: [      {        &quot;nReturned&quot;: 75,        &quot;executionTimeMillisEstimate&quot;: 3640,        &quot;totalKeysExamined&quot;: 76455,        &quot;totalDocsExamined&quot;: 76455,        &quot;executionStages&quot;: {          &quot;stage&quot;: &quot;PROJECTION&quot;,          &quot;nReturned&quot;: 75,          &quot;executionTimeMillisEstimate&quot;: 3640,          &quot;works&quot;: 76455,          &quot;advanced&quot;: 75,          &quot;needTime&quot;: 76380,          &quot;needYield&quot;: 0,          &quot;saveState&quot;: 817,          &quot;restoreState&quot;: 817,          &quot;isEOF&quot;: 0,          &quot;invalidates&quot;: 0,          &quot;transformBy&quot;: {            &quot;formType&quot;: 1,            // ...            &quot;lastUpdateTime&quot;: 1          },          &quot;inputStage&quot;: {            &quot;stage&quot;: &quot;FETCH&quot;,            &quot;filter&quot;: {              &quot;$and&quot;: [                {                  &quot;formType&quot;: {                    &quot;$eq&quot;: &quot;some_form&quot;                  }                },                {                  &quot;formFields.formInstanceNumber.value&quot;: {                    &quot;$in&quot;: [                      &quot;EXI123456789&quot;,                      // ...                      &quot;EXI987654321&quot;                    ]                  }                }              ]            },            &quot;nReturned&quot;: 75,            &quot;executionTimeMillisEstimate&quot;: 3640,            &quot;works&quot;: 76455,            &quot;advanced&quot;: 75,            &quot;needTime&quot;: 76380,            &quot;needYield&quot;: 0,            &quot;saveState&quot;: 817,            &quot;restoreState&quot;: 817,            &quot;isEOF&quot;: 0,            &quot;invalidates&quot;: 0,            &quot;docsExamined&quot;: 76455,            &quot;alreadyHasObj&quot;: 0,            &quot;inputStage&quot;: {              &quot;stage&quot;: &quot;IXSCAN&quot;,              &quot;nReturned&quot;: 76455,              &quot;executionTimeMillisEstimate&quot;: 40,              &quot;works&quot;: 76455,              &quot;advanced&quot;: 76455,              &quot;needTime&quot;: 0,              &quot;needYield&quot;: 0,              &quot;saveState&quot;: 817,              &quot;restoreState&quot;: 817,              &quot;isEOF&quot;: 0,              &quot;invalidates&quot;: 0,              &quot;keyPattern&quot;: {                &quot;_id&quot;: 1              },              &quot;indexName&quot;: &quot;_id_&quot;,              &quot;isMultiKey&quot;: false,              &quot;multiKeyPaths&quot;: {                &quot;_id&quot;: [ ]              },              &quot;isUnique&quot;: true,              &quot;isSparse&quot;: false,              &quot;isPartial&quot;: false,              &quot;indexVersion&quot;: 2,              &quot;direction&quot;: &quot;backward&quot;,              &quot;indexBounds&quot;: {                &quot;_id&quot;: [                  &quot;[MaxKey, MinKey]&quot;                ]              },              &quot;keysExamined&quot;: 76455,              &quot;seeks&quot;: 1,              &quot;dupsTested&quot;: 0,              &quot;dupsDropped&quot;: 0,              &quot;seenInvalidated&quot;: 0            }          }        }      },      {        &quot;nReturned&quot;: 0,        &quot;executionTimeMillisEstimate&quot;: 30,        &quot;totalKeysExamined&quot;: 135,        &quot;totalDocsExamined&quot;: 75,        &quot;executionStages&quot;: {          &quot;stage&quot;: &quot;PROJECTION&quot;,          &quot;nReturned&quot;: 0,          &quot;executionTimeMillisEstimate&quot;: 30,          &quot;works&quot;: 137,          &quot;advanced&quot;: 0,          &quot;needTime&quot;: 136,          &quot;needYield&quot;: 0,          &quot;saveState&quot;: 2212,          &quot;restoreState&quot;: 2212,          &quot;isEOF&quot;: 0,          &quot;invalidates&quot;: 0,          &quot;transformBy&quot;: {            &quot;formType&quot;: 1,            // ...            &quot;lastUpdateTime&quot;: 1          },          &quot;inputStage&quot;: {            &quot;stage&quot;: &quot;SORT&quot;,            &quot;nReturned&quot;: 0,            &quot;executionTimeMillisEstimate&quot;: 30,            &quot;works&quot;: 137,            &quot;advanced&quot;: 0,            &quot;needTime&quot;: 136,            &quot;needYield&quot;: 0,            &quot;saveState&quot;: 2212,            &quot;restoreState&quot;: 2212,            &quot;isEOF&quot;: 0,            &quot;invalidates&quot;: 0,            &quot;sortPattern&quot;: {              &quot;_id&quot;: -1            },            &quot;memUsage&quot;: 33914115,            &quot;memLimit&quot;: 33554432,            &quot;inputStage&quot;: {              &quot;stage&quot;: &quot;SORT_KEY_GENERATOR&quot;,              &quot;nReturned&quot;: 75,              &quot;executionTimeMillisEstimate&quot;: 20,              &quot;works&quot;: 136,              &quot;advanced&quot;: 75,              &quot;needTime&quot;: 61,              &quot;needYield&quot;: 0,              &quot;saveState&quot;: 2212,              &quot;restoreState&quot;: 2212,              &quot;isEOF&quot;: 0,              &quot;invalidates&quot;: 0,              &quot;inputStage&quot;: {                &quot;stage&quot;: &quot;FETCH&quot;,                &quot;filter&quot;: {                  &quot;formType&quot;: {                    &quot;$eq&quot;: &quot;some_form&quot;                  }                },                &quot;nReturned&quot;: 75,                &quot;executionTimeMillisEstimate&quot;: 20,                &quot;works&quot;: 135,                &quot;advanced&quot;: 75,                &quot;needTime&quot;: 60,                &quot;needYield&quot;: 0,                &quot;saveState&quot;: 2212,                &quot;restoreState&quot;: 2212,                &quot;isEOF&quot;: 0,                &quot;invalidates&quot;: 0,                &quot;docsExamined&quot;: 75,                &quot;alreadyHasObj&quot;: 0,                &quot;inputStage&quot;: {                  &quot;stage&quot;: &quot;IXSCAN&quot;,                  &quot;nReturned&quot;: 75,                  &quot;executionTimeMillisEstimate&quot;: 0,                  &quot;works&quot;: 135,                  &quot;advanced&quot;: 75,                  &quot;needTime&quot;: 60,                  &quot;needYield&quot;: 0,                  &quot;saveState&quot;: 2212,                  &quot;restoreState&quot;: 2212,                  &quot;isEOF&quot;: 0,                  &quot;invalidates&quot;: 0,                  &quot;keyPattern&quot;: {                    &quot;formFields.formInstanceNumber.value&quot;: 1                  },                  &quot;indexName&quot;: &quot;formFields.formInstanceNumber.value_1&quot;,                  &quot;isMultiKey&quot;: false,                  &quot;multiKeyPaths&quot;: {                    &quot;formFields.formInstanceNumber.value&quot;: [ ]                  },                  &quot;isUnique&quot;: false,                  &quot;isSparse&quot;: false,                  &quot;isPartial&quot;: false,                  &quot;indexVersion&quot;: 2,                  &quot;direction&quot;: &quot;forward&quot;,                  &quot;indexBounds&quot;: {                    &quot;formFields.formInstanceNumber.value&quot;: [                      &quot;[\\&quot;EXI123456789\\&quot;, \\&quot;EXI123456789\\&quot;]&quot;,                      // ...                      &quot;[\\&quot;EXI987654321\\&quot;, \\&quot;EXI987654321\\&quot;]&quot;                    ]                  },                  &quot;keysExamined&quot;: 135,                  &quot;seeks&quot;: 61,                  &quot;dupsTested&quot;: 0,                  &quot;dupsDropped&quot;: 0,                  &quot;seenInvalidated&quot;: 0                }              }            }          }        }      },      {        &quot;nReturned&quot;: 0,        &quot;executionTimeMillisEstimate&quot;: 1200,        &quot;totalKeysExamined&quot;: 27870,        &quot;totalDocsExamined&quot;: 27870,        &quot;executionStages&quot;: {          &quot;stage&quot;: &quot;PROJECTION&quot;,          &quot;nReturned&quot;: 0,          &quot;executionTimeMillisEstimate&quot;: 1200,          &quot;works&quot;: 27872,          &quot;advanced&quot;: 0,          &quot;needTime&quot;: 27871,          &quot;needYield&quot;: 0,          &quot;saveState&quot;: 2212,          &quot;restoreState&quot;: 2212,          &quot;isEOF&quot;: 0,          &quot;invalidates&quot;: 0,          &quot;transformBy&quot;: {            &quot;formType&quot;: 1,            // ...            &quot;lastUpdateTime&quot;: 1          },          &quot;inputStage&quot;: {            &quot;stage&quot;: &quot;SORT&quot;,            &quot;nReturned&quot;: 0,            &quot;executionTimeMillisEstimate&quot;: 1200,            &quot;works&quot;: 27872,            &quot;advanced&quot;: 0,            &quot;needTime&quot;: 27871,            &quot;needYield&quot;: 0,            &quot;saveState&quot;: 2212,            &quot;restoreState&quot;: 2212,            &quot;isEOF&quot;: 0,            &quot;invalidates&quot;: 0,            &quot;sortPattern&quot;: {              &quot;_id&quot;: -1            },            &quot;memUsage&quot;: 33914115,            &quot;memLimit&quot;: 33554432,            &quot;inputStage&quot;: {              &quot;stage&quot;: &quot;SORT_KEY_GENERATOR&quot;,              &quot;nReturned&quot;: 75,              &quot;executionTimeMillisEstimate&quot;: 1190,              &quot;works&quot;: 27871,              &quot;advanced&quot;: 75,              &quot;needTime&quot;: 27796,              &quot;needYield&quot;: 0,              &quot;saveState&quot;: 2212,              &quot;restoreState&quot;: 2212,              &quot;isEOF&quot;: 0,              &quot;invalidates&quot;: 0,              &quot;inputStage&quot;: {                &quot;stage&quot;: &quot;FETCH&quot;,                &quot;filter&quot;: {                  &quot;formFields.formInstanceNumber.value&quot;: {                    &quot;$in&quot;: [                      &quot;EXI123456789&quot;,                      // ...                      &quot;EXI987654321&quot;                    ]                  }                },                &quot;nReturned&quot;: 75,                &quot;executionTimeMillisEstimate&quot;: 1180,                &quot;works&quot;: 27870,                &quot;advanced&quot;: 75,                &quot;needTime&quot;: 27795,                &quot;needYield&quot;: 0,                &quot;saveState&quot;: 2212,                &quot;restoreState&quot;: 2212,                &quot;isEOF&quot;: 0,                &quot;invalidates&quot;: 0,                &quot;docsExamined&quot;: 27870,                &quot;alreadyHasObj&quot;: 0,                &quot;inputStage&quot;: {                  &quot;stage&quot;: &quot;IXSCAN&quot;,                  &quot;nReturned&quot;: 27870,                  &quot;executionTimeMillisEstimate&quot;: 0,                  &quot;works&quot;: 27870,                  &quot;advanced&quot;: 27870,                  &quot;needTime&quot;: 0,                  &quot;needYield&quot;: 0,                  &quot;saveState&quot;: 2212,                  &quot;restoreState&quot;: 2212,                  &quot;isEOF&quot;: 0,                  &quot;invalidates&quot;: 0,                  &quot;keyPattern&quot;: {                    &quot;formType&quot;: 1                  },                  &quot;indexName&quot;: &quot;formType_1&quot;,                  &quot;isMultiKey&quot;: false,                  &quot;multiKeyPaths&quot;: {                    &quot;formType&quot;: [ ]                  },                  &quot;isUnique&quot;: false,                  &quot;isSparse&quot;: false,                  &quot;isPartial&quot;: false,                  &quot;indexVersion&quot;: 2,                  &quot;direction&quot;: &quot;forward&quot;,                  &quot;indexBounds&quot;: {                    &quot;formType&quot;: [                      &quot;[\\&quot;some_form\\&quot;, \\&quot;some_form\\&quot;]&quot;                    ]                  },                  &quot;keysExamined&quot;: 27870,                  &quot;seeks&quot;: 1,                  &quot;dupsTested&quot;: 0,                  &quot;dupsDropped&quot;: 0,                  &quot;seenInvalidated&quot;: 0                }              }            }          }        }      }    ]  },  &quot;serverInfo&quot;: {    &quot;host&quot;: &quot;examplehost1&quot;,    &quot;port&quot;: 27017,    &quot;version&quot;: &quot;4.0.9&quot;,    &quot;gitVersion&quot;: &quot;fc525e2d9b0e4bceff5c2201457e564362909765&quot;  },  &quot;ok&quot;: 1,  &quot;operationTime&quot;: Timestamp(1671465640, 1),  &quot;$clusterTime&quot;: {    &quot;clusterTime&quot;: Timestamp(1671465640, 1),    &quot;signature&quot;: {      &quot;hash&quot;: BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;),      &quot;keyId&quot;: NumberLong(0)    }  }}Wynik analizy jest dość obszerny, jednak można go podsumować tak:  winningPlan czyli plan, który został wykonany posłużył się standardowym indeksem kolekcji:    {  &quot;inputStage&quot;: {    &quot;stage&quot;: &quot;IXSCAN&quot;,    &quot;keyPattern&quot;: {      &quot;_id&quot;: 1    },    &quot;indexName&quot;: &quot;_id_&quot;,    &quot;isMultiKey&quot;: false,    &quot;multiKeyPaths&quot;: {      &quot;_id&quot;: [ ]    },    &quot;isUnique&quot;: true,    &quot;isSparse&quot;: false,    &quot;isPartial&quot;: false,    &quot;indexVersion&quot;: 2,    &quot;direction&quot;: &quot;backward&quot;,    &quot;indexBounds&quot;: {      &quot;_id&quot;: [        &quot;[MaxKey, MinKey]      ]    }  }}        rejectedPlans czyli plany odrzucone zawierają plan wykorzystujący indeks, który naszym zdaniem powinien zostać użyty:    {  &quot;inputStage&quot;: {    &quot;stage&quot;: &quot;IXSCAN&quot;,    &quot;keyPattern&quot;: {      &quot;formFields.formInstanceNumber.value&quot;: 1    },    &quot;indexName&quot;: &quot;formFields.formInstanceNumber.value_1&quot;,    &quot;isMultiKey&quot;: false,    &quot;multiKeyPaths&quot;: {      &quot;formFields.formInstanceNumber.value&quot;: [ ]    },    &quot;isUnique&quot;: false,    &quot;isSparse&quot;: false,    &quot;isPartial&quot;: false,    &quot;indexVersion&quot;: 2,    &quot;direction&quot;: &quot;forward&quot;  }}      Co więcej, plan który został wybrany odwiedził 254851 dokumentów i kluczy, a zapytanie trwało 11651 ms:  {    &quot;executionStats&quot;: {      &quot;executionSuccess&quot;: true,      &quot;nReturned&quot;: 75,      &quot;executionTimeMillis&quot;: 11651,      &quot;totalKeysExamined&quot;: 254851,      &quot;totalDocsExamined&quot;: 254851    }  }Przy czym jeden z planów odrzuconych mógł zwrócić wynik po odwiedzeniu 75 dokumentów i 135 kluczy, a jego wykonanie szacowane było na 30 ms:{  &quot;nReturned&quot;: 0,  &quot;executionTimeMillisEstimate&quot;: 30,  &quot;totalKeysExamined&quot;: 135,  &quot;totalDocsExamined&quot;: 75}Dlaczego MongoDB nie używa indeksuW tym miejscu należy się zastanowić w jaki sposób MongoDB wybiera najlepszy plan zapytania. Z pierwszego punktu tego wpisu wiemy, że optymalizator bazuje na statystykach zbieranych podczas działania bazy. Potrzebujemy jeszcze wiedzieć w jaki sposób budowane są te statystyki. W opisywanym przypadku to właśnie tutaj kryje się rozwiązanie naszego problemu.Optymalizator MongoDB cache’uje plany zapytań. Plan zapytania, który wygrał (winningPlan) trafia do cache’a i po kolejnym zapytaniu, w którym okazał się planem wygrywającym staje się aktywny. Następne zapytanie o takim samym kształcie zostaje wykonane w oparciu o aktywny plan z cache’a. Algorytm wygląda tak:Źródło: www.mongodb.com - dostęp: 2023-10-17Kluczem w cachu planów zapytań jest kształt zapytania query-shape. Na kształt zapytania składają się:  predykat zapytania (czyli to po czym filtrujemy kolekcję),  projekcja,  sposób sortowania,  collation.Uzbrojeni w tę wiedzę przeanalizowaliśmy jakie zapytania kieruje do MongoDB nasza aplikacja. Okazało się, że w większości przypadków aplikacja odpytuje bazę o pojedynczą wartość pola formFields.formInstanceNumber.value. Tak więc w klauzuli in znajduje się jedna wartość. Dla takiej postaci zapytania optymalizator wybierał plan, który nie uwzględniał oczekiwanego przez nas indeksu. Taki plan trafiał do cache’a planów zapytań. Od czasu do czasu zdarzał się jednak klient systemu, dla którego zapytanie zawierało wiele wartości w klauzuli in. Kształt zapytania pozostawał ten sam więc MongoDB nadal używało planu, który znajdował się w cache’u. W ten sposób, dla klienta, który używał systemu w szerszym zakresie niż pozostali dostawaliśmy timeout. Rozwiązaniem tego problemu mogłoby być takie dobranie zapytań, aby klienci z pojedynczą wartością w klauzuli in posługiwali się innym kształtem zapytania niż klienci z wieloma wartościami. To wymagałoby jednak zmiany w kodach systemu.Wymuszenie użycia indeksu na podczas pracy bazyMongoDB daje nam jednak możliwość na wymuszenie użycia indeksu dla określonego kształtu zapytania. Taką konfigurację można zastosować na działającym systemie i działa ona natychmiast. Należy użyć polecenia: planCacheSetFilter. W naszej sytuacji użyliśmy następującej komendy:db.runCommand({  planCacheSetFilter: &quot;formModel&quot;,  query: {      &quot;formFields.formInstanceNumber.value&quot;: {        &quot;$in&quot;: [          &quot;EXI123456789&quot;,          // ...          &quot;EXI987654321&quot;        ]      },      &quot;formType&quot;: &quot;some_form&quot;    },          projection : {      &quot;formType&quot;: 1,      // ...      &quot;lastUpdateTime&quot;: 1    },    sort : {      &quot;_id&quot;: -1    },    indexes: [      {&quot;formFields.formInstanceNumber.value&quot;: 1}    ]})W ten sposób wymuszamy na MongoDB używanie indeksu na polu formFields.formInstanceNumber.value w zapytaniach o podanym kształcie. Być może zapytania dla pojedynczych wartości będą trochę mniej optymalne, ale za to dużo szybciej wykonają się zapytania dla dużej liczby wartości w klauzuli in.Uwaga! Zmiana ta działa do czasu restartu MongoDB, nie jest więc docelowym rozwiązaniem, ale daje czas na uzyskanie satysfakcjonującego rozwiązania w kodzie.Po ponowieniu zapytania otrzymaliśmy następujący wynik:{  &quot;queryPlanner&quot;: {    &quot;plannerVersion&quot;: 1,    &quot;namespace&quot;: &quot;formstore_db.formModel&quot;,    &quot;indexFilterSet&quot;: true,    &quot;parsedQuery&quot;: {      &quot;$and&quot;: [        {          &quot;formType&quot;: {            &quot;$eq&quot;: &quot;some_form&quot;          }        },        {          &quot;formFields.formInstanceNumber.value&quot;: {            &quot;$in&quot;: [              &quot;EXI123456789&quot;,              // ...              &quot;EXI987654321&quot;            ]          }        }      ]    },    &quot;winningPlan&quot;: {      &quot;stage&quot;: &quot;PROJECTION&quot;,      &quot;transformBy&quot;: {        &quot;formType&quot;: 1,        // ...        &quot;lastUpdateTime&quot;: 1      },      &quot;inputStage&quot;: {        &quot;stage&quot;: &quot;SORT&quot;,        &quot;sortPattern&quot;: {          &quot;_id&quot;: -1        },        &quot;inputStage&quot;: {          &quot;stage&quot;: &quot;SORT_KEY_GENERATOR&quot;,          &quot;inputStage&quot;: {            &quot;stage&quot;: &quot;FETCH&quot;,            &quot;filter&quot;: {              &quot;formType&quot;: {                &quot;$eq&quot;: &quot;some_form&quot;              }            },            &quot;inputStage&quot;: {              &quot;stage&quot;: &quot;IXSCAN&quot;,              &quot;keyPattern&quot;: {                &quot;formFields.formInstanceNumber.value&quot;: 1              },              &quot;indexName&quot;: &quot;formFields.formInstanceNumber.value_1&quot;,              &quot;isMultiKey&quot;: false,              &quot;multiKeyPaths&quot;: {                &quot;formFields.formInstanceNumber.value&quot;: [ ]              },              &quot;isUnique&quot;: false,              &quot;isSparse&quot;: false,              &quot;isPartial&quot;: false,              &quot;indexVersion&quot;: 2,              &quot;direction&quot;: &quot;forward&quot;,              &quot;indexBounds&quot;: {                &quot;formFields.formInstanceNumber.value&quot;: [                  &quot;[\\&quot;EXI123456789\\&quot;, \\&quot;EXI123456789\\&quot;]&quot;,                  // ...                  &quot;[\\&quot;EXI987654321\\&quot;, \\&quot;EXI987654321\\&quot;]&quot;                ]              }            }          }        }      }    },    &quot;rejectedPlans&quot;: [ ]  },  &quot;executionStats&quot;: {    &quot;executionSuccess&quot;: true,    &quot;nReturned&quot;: 75,    &quot;executionTimeMillis&quot;: 10,    &quot;totalKeysExamined&quot;: 136,    &quot;totalDocsExamined&quot;: 75,    &quot;executionStages&quot;: {      &quot;stage&quot;: &quot;PROJECTION&quot;,      &quot;nReturned&quot;: 75,      &quot;executionTimeMillisEstimate&quot;: 10,      &quot;works&quot;: 213,      &quot;advanced&quot;: 75,      &quot;needTime&quot;: 137,      &quot;needYield&quot;: 0,      &quot;saveState&quot;: 1,      &quot;restoreState&quot;: 1,      &quot;isEOF&quot;: 1,      &quot;invalidates&quot;: 0,      &quot;transformBy&quot;: {        &quot;formType&quot;: 1,        // ...        &quot;lastUpdateTime&quot;: 1      },      &quot;inputStage&quot;: {        &quot;stage&quot;: &quot;SORT&quot;,        &quot;nReturned&quot;: 75,        &quot;executionTimeMillisEstimate&quot;: 0,        &quot;works&quot;: 213,        &quot;advanced&quot;: 75,        &quot;needTime&quot;: 137,        &quot;needYield&quot;: 0,        &quot;saveState&quot;: 1,        &quot;restoreState&quot;: 1,        &quot;isEOF&quot;: 1,        &quot;invalidates&quot;: 0,        &quot;sortPattern&quot;: {          &quot;_id&quot;: -1        },        &quot;memUsage&quot;: 33914115,        &quot;memLimit&quot;: 335544320,        &quot;inputStage&quot;: {          &quot;stage&quot;: &quot;SORT_KEY_GENERATOR&quot;,          &quot;nReturned&quot;: 75,          &quot;executionTimeMillisEstimate&quot;: 0,          &quot;works&quot;: 137,          &quot;advanced&quot;: 75,          &quot;needTime&quot;: 61,          &quot;needYield&quot;: 0,          &quot;saveState&quot;: 1,          &quot;restoreState&quot;: 1,          &quot;isEOF&quot;: 1,          &quot;invalidates&quot;: 0,          &quot;inputStage&quot;: {            &quot;stage&quot;: &quot;FETCH&quot;,            &quot;filter&quot;: {              &quot;formType&quot;: {                &quot;$eq&quot;: &quot;some_form&quot;              }            },            &quot;nReturned&quot;: 75,            &quot;executionTimeMillisEstimate&quot;: 0,            &quot;works&quot;: 136,            &quot;advanced&quot;: 75,            &quot;needTime&quot;: 60,            &quot;needYield&quot;: 0,            &quot;saveState&quot;: 1,            &quot;restoreState&quot;: 1,            &quot;isEOF&quot;: 1,            &quot;invalidates&quot;: 0,            &quot;docsExamined&quot;: 75,            &quot;alreadyHasObj&quot;: 0,            &quot;inputStage&quot;: {              &quot;stage&quot;: &quot;IXSCAN&quot;,              &quot;nReturned&quot;: 75,              &quot;executionTimeMillisEstimate&quot;: 0,              &quot;works&quot;: 136,              &quot;advanced&quot;: 75,              &quot;needTime&quot;: 60,              &quot;needYield&quot;: 0,              &quot;saveState&quot;: 1,              &quot;restoreState&quot;: 1,              &quot;isEOF&quot;: 1,              &quot;invalidates&quot;: 0,              &quot;keyPattern&quot;: {                &quot;formFields.formInstanceNumber.value&quot;: 1              },              &quot;indexName&quot;: &quot;formFields.formInstanceNumber.value_1&quot;,              &quot;isMultiKey&quot;: false,              &quot;multiKeyPaths&quot;: {                &quot;formFields.formInstanceNumber.value&quot;: [ ]              },              &quot;isUnique&quot;: false,              &quot;isSparse&quot;: false,              &quot;isPartial&quot;: false,              &quot;indexVersion&quot;: 2,              &quot;direction&quot;: &quot;forward&quot;,              &quot;indexBounds&quot;: {                &quot;formFields.formInstanceNumber.value&quot;: [                  &quot;[\\&quot;EXI123456789\\&quot;, \\&quot;EXI123456789\\&quot;]&quot;,                  // ...                  &quot;[\\&quot;EXI987654321\\&quot;, \\&quot;EXI987654321\\&quot;]&quot;                ]              },              &quot;keysExamined&quot;: 136,              &quot;seeks&quot;: 61,              &quot;dupsTested&quot;: 0,              &quot;dupsDropped&quot;: 0,              &quot;seenInvalidated&quot;: 0            }          }        }      }    },    &quot;allPlansExecution&quot;: [ ]  },  &quot;serverInfo&quot;: {    &quot;host&quot;: &quot;examplehost1&quot;,    &quot;port&quot;: 27017,    &quot;version&quot;: &quot;4.0.9&quot;,    &quot;gitVersion&quot;: &quot;fc525e2d9b0e4bceff5c2201457e564362909765&quot;  },  &quot;ok&quot;: 1,  &quot;operationTime&quot;: Timestamp(1671537181, 9),  &quot;$clusterTime&quot;: {    &quot;clusterTime&quot;: Timestamp(1671537181, 9),    &quot;signature&quot;: {      &quot;hash&quot;: BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;),      &quot;keyId&quot;: NumberLong(0)    }  }}Widać że winningPlan używa wskazanego przez nas indeksu:  {    &quot;inputStage&quot;: {      &quot;stage&quot;: &quot;IXSCAN&quot;,      &quot;keyPattern&quot;: {        &quot;formFields.formInstanceNumber.value&quot;: 1      },      &quot;indexName&quot;: &quot;formFields.formInstanceNumber.value_1&quot;,      &quot;isMultiKey&quot;: false,      &quot;multiKeyPaths&quot;: {        &quot;formFields.formInstanceNumber.value&quot;: []      },      &quot;isUnique&quot;: false,      &quot;isSparse&quot;: false,      &quot;isPartial&quot;: false,      &quot;indexVersion&quot;: 2,      &quot;direction&quot;: &quot;forward&quot;    }  }a wykonanie zapytania trwało 10 milisekund. Podczas przetwarzania zapytania odwiedzonych zostało 136 kluczy i 75 dokumentów:  {    &quot;executionStats&quot;: {        &quot;executionSuccess&quot;: true,        &quot;nReturned&quot;: 75,        &quot;executionTimeMillis&quot;: 10,        &quot;totalKeysExamined&quot;: 136,        &quot;totalDocsExamined&quot;: 75    }  }WnioskiOczywisty wniosek płynący z przedstawionej tutaj sytuacji jest taki, że musimy brać pod uwagę to, że MongoDB nie musi używać utworzonego przez nas indeksu. Szczególną uwagę powinniśmy zwrócić na zapytania, które są uruchamiane ze znacząco różnymi zakresami danych wejściowych.Wniosek mniej oczywisty jest związany z użyciem circuit breakera. W naszym przypadku circuit breaker odcinał wykonywanie zapytań trwających dłużej niż 10 sekund. Ponieważ klienci, dla których w klauzuli in wykorzystywaliśmy dużo wartości ponawiali próby wywołania funkcjonalności, circuit breaker wyłączył wywoływanie tej funkcjonalności. To spowodowało, że również dla klientów z mniejszą liczbą wartości w klauzuli in funkcjonalność stała się niedostępna.tl;dr  Jeżeli używasz MongoDB włącz logowanie długich zapytań.  W przypadku problemów wydajnościowych z zapytaniem sprawdź plan wykonania zapytania dodając do polecenia .explain(&quot;allPlansExecution&quot;).  Jeżeli chcesz wymusić, aby MongoDB używało indeksu w określonym zapytaniu można to osiągnąć bez przerwy w pracy systemu za pomocą polecenia planCacheSetFilter.Źródła  Opis działania optymalizatora zapytań w MongoDB.  Sposób na logowanie długich zapytań w MongoDB.  Polecenie umożliwiające wymuszenie użycia określonego indeksu dla zapytania o podanym kształcie  Wyjaśnienie czym jest kształt zapytania query-shape",
"url": "/2023/10/24/jak-zmusic-mongodb-do-uzycia-indeksu-bez-zmiany-kodu.html",
"author": "Jakub Wilczewski",
"authorUrl": "/authors/jwilczewski.html",
"image": "jwilczewski.jpg",
"highlight": "/assets/img/posts/2023-10-24-jak-zmusic-mongodb-do-uzycia-indeksu-bez-zmiany-kodu/mongodb.webp",
"date": "24-10-2023",
"path": "pl/2023-10-24-jak-zmusic-mongodb-do-uzycia-indeksu-bez-zmiany-kodu"
}
,


"2023-10-06-czy-wiesz-jak-obslugiwac-bledy-w-angular-w-scentralizowany-sposob-html": {
"title": "Czy wiesz, jak obsługiwać błędy w Angular w scentralizowany sposób?",
"lang": "pl",
"tags": "angular errorhandler",
"content": "W Angularze istnieje ErrorHandler, jest to hook do scentralizowanej obsługi błędów. Domyślny ErrorHandler wyświetla jedynie błędy na konsoli. Jeżeli chcemy wyłapać i obsłużyć nieobsłużone błędy, należy dodać implementację ErrorHandlera. Pokażę, jak to zrobić na prostym przykładzie.W module należy dodać providera:providers: [    {provide: ErrorHandler, useClass: GlobalErrorHandler}]Przykładowa implementacja GlobalErrorHandlera z serwisem, który może być używany przez komponenty:export interface ErrorWrapper {  sourceModule: string;  error: Error;} @Injectable()export class ErrorService {  private error$: Subject&amp;lt;ErrorWrapper&amp;gt; = new Subject&amp;lt;ErrorWrapper&amp;gt;();   public publishError(error: ErrorWrapper): void {    this.error$.next(error);  }   public takeError$(): Observable&amp;lt;ErrorWrapper&amp;gt; {    return this.error$.asObservable();  } } @Injectable({  providedIn: &#39;root&#39;,})export class GlobalErrorHandler implements ErrorHandler {   constructor(private errorService: ErrorService) {  }   handleError(error: Error): void {    const wrappedError: ErrorWrapper = {sourceModule: &#39;application&#39;, error: error};    this.errorService.publishError(wrappedError);  } }Użycie w komponencie:@Component({  selector: &#39;app&#39;,  template: `        &amp;lt;div *ngIf=&quot;!(error$ | async) else error&quot;&amp;gt;          &amp;lt;p&amp;gt;Aplikacja działa!&amp;lt;/p&amp;gt;        &amp;lt;/div&amp;gt;        &amp;lt;ng-template #error&amp;gt;          &amp;lt;p&amp;gt;Błąd!&amp;lt;/p&amp;gt;        &amp;lt;/ng-template&amp;gt;  `})export class AppComponent {   error$: Observable&amp;lt;boolean&amp;gt; = this.errorService.takeError$().pipe(    startWith(false)  );   // ...}Powyższa implementacja jest tylko uproszczonym przykładem, ale pokazuje, jak można obsłużyć niespodziewane wyjątki.",
"url": "/2023/10/06/czy-wiesz-jak-obslugiwac-bledy-w-angular-w-scentralizowany-sposob.html",
"author": "Dorian Mejer",
"authorUrl": "/authors/dmejer.html",
"image": "dmejer.jpg",
"highlight": "/assets/img/posts/2023-10-06-czy-wiesz-jak-obslugiwac-bledy-w-angular-w-scentralizowany-sposob/error_handling.webp",
"date": "06-10-2023",
"path": "pl/2023-10-06-czy-wiesz-jak-obslugiwac-bledy-w-angular-w-scentralizowany-sposob"
}
,


"2023-09-22-czy-wiesz-po-co-stosuje-sie-sneaky-throws-z-biblioteki-lombok-html": {
"title": "Czy wiesz, po co stosuje się @SneakyThrows z biblioteki Lombok?",
"lang": "pl",
"tags": "java lombok sneakythrows",
"content": "Wyjątki w Javie dzielą się na checked exceptions oraz unchecked exceptions. Unchecked exception reprezentuje błąd w logice programu, który może wystąpić w dowolnym miejscu - przykładowo odwołanie się do nieistniejącego elementu tablicy spowoduje rzucenie wyjątku ArrayIndexOutOfBoundsException. Kompilator nie jest w stanie przewidzieć błędów logicznych, które pojawiają się dopiero w czasie wykonywania programu, dlatego nie może sprawdzać tego typu problemów w czasie kompilacji, co sprawia, że wyjątki te nie muszą być obsłużone przez programistę.Przykład unchecked exception - dzielenie przez zero wyrzuci wyjątek ArithmeticException:private static void divideByZero() {    int numerator = 1;    int denominator = 0;    int result = numerator / denominator;}Checked exception to wyjątek reprezentujący przewidywalną, błędną sytuację, która może wystąpić nawet w przypadku poprawnej logiki programu - przykładowo próba otwarcia pliku, który nie istnieje, spowoduje rzucenie wyjątku FileNotFoundException. Wyjątki tego rodzaju są weryfikowane w czasie kompilacji, dlatego Java zmusza nas do ich obsługi - albo poprzez słowo kluczowe throws, albo poprzez złapanie wyjątku w bloku try-catch:  Przykład obsługi przez słowo kluczowe throws - przekazanie wyjątku w dół stosu wywołań:    private static void openFile() throws FileNotFoundException {    File file = new File(&quot;Nieistniejacy_plik.txt&quot;);    FileInputStream stream = new FileInputStream(file);}        Przykład obsługi przez blok try-catch - złapanie wyjątku:    private static void openFile() {    File file = new File(&quot;Nieistniejacy_plik.txt&quot;);    try {        FileInputStream stream = new FileInputStream(file);    } catch (FileNotFoundException e) {        e.printStackTrace();    }}      Koncepcja sneaky throwsSneaky throws to koncepcja pozwalająca na rzucenie dowolnego checked exception bez jego jawnego definiowania w sygnaturze metody. Pozwala ona na ominięcie słowa kluczowego throws oraz imitowanie zachowania unchecked exception i jest możliwa, ponieważ obsługa wyjątków checked exception jest wymuszana tylko przez kompilator Javy.  W kodzie bajtowym każdy wyjątek może zostać rzucony z dowolnego miejsca i jest traktowany przez JVM tak samo - zostaje on propagowany w dół stosu wywołań. Od Javy 8 każde użycie throws T, gdzie T jest typem generycznym rozszerzajacym Throwable, oznacza, że metoda może rzucić unchecked exception. Dzięki temu możemy stworzyć metodę pomocniczą, która będzie rzucała wyjątek typu checked, jednak kompilator nie będzie wymagał jego przechwycenia.Metoda pomocnicza realizująca koncepcję sneaky throws:private static &amp;lt;T extends Throwable&amp;gt; void sneakyThrow(Throwable t) throws T {    throw (T) t;}Metodę tę możemy następnie dowolnie wykorzystać w naszym kodzie:private File getFile(String fileName) {   return null;} private void deleteFile(String fileName) {   File file = getFile(fileName);   if (file == null) {      sneakyThrow(new FileNotFoundException(&quot;Nie znaleziono pliku&quot;));   }   file.delete();} public void tryToDeleteNotExistingFile() {   try {      deleteFile(&quot;Nieistniejacy_plik.txt&quot;);   } catch (Exception exception) {      exception.printStackTrace();   }}Warto zauważyć, że w takiej sytuacji w metodzie tryToDeleteNotExistingFile() nie możemy złapać już wyjątku FileNotFoundException, ponieważ nie jest on zadeklarowany - możemy jedynie ratować się złapaniem bardziej ogólnego Exception.Lombok - adnotacja @SneakyThrowsBiblioteka Lombok udostępnia adnotację @SneakyThrows, która wykorzystuje powyższą sztuczkę i dzięki oszukaniu kompilatora pozwala rzucać checked exception bez deklarowania tego w sygnaturze metody.Przykład - użycie adnotacji @SneakyThrows:private File getFile(String fileName) {   return null;} @SneakyThrows(FileNotFoundException.class)private void deleteFile(String fileName) {   File file = getFile(fileName);   if (file == null) {      throw new FileNotFoundException(&quot;Nie znaleziono pliku&quot;);   }   file.delete();} public void tryToDeleteNotExistingFile() {   try {      deleteFile(&quot;Nieistniejacy_plik.txt&quot;);   } catch (Exception exception) {      exception.printStackTrace();   }}Do adnotacji @SneakyThrows można przekazać dowolną liczbę wyjątków. Jeśli nie podamy żadnego, to adnotacja ta uwzględni dowolny wyjątek. Należy pamiętać także o tym, że @SneakyThrows nie dziedziczy.Kiedy używać?Dokumentacja @SneakyThrows wspomina o dwóch częstych przypadkach użycia:  niepotrzebnie rygorystyczne interfejsy takie jak Runnable,  “niemożliwe” wyjątki, które nie powinny nigdy być rzucone np. ze względu na specyfikację JVM.Inną sytuacją, w której można zastanowić się nad użyciem @SneakyThrows są wyrażenia lambda - użycie tej adnotacji pozwoli zwiększyć czytelność, ponieważ nie będziemy musieli przejmować się łapaniem wyjątków w blok try-catch. Przykład z użyciem @SneakyThrows:@SneakyThrowsprivate static Instant sneakyParseStringDate(String date) {   return new SimpleDateFormat(&quot;yyyy-MM-dd&quot;).parse(date).toInstant();} public List&amp;lt;Instant&amp;gt; getInstants() {   return List.of(&quot;2022-05-18&quot;).stream().map(SneakyThrowsExample::sneakyParseStringDate)         .collect(Collectors.toList());}Ten sam przykład bez użycia @SneakyThrows:private static Instant nonSneakyParseStringDate(String date) throws ParseException {   return new SimpleDateFormat(&quot;yyyy-MM-dd&quot;).parse(date).toInstant();} public List&amp;lt;Instant&amp;gt; getInstants() {   return List.of(&quot;2022-05-18&quot;).stream().map(date -&amp;gt; {      try {         return nonSneakyParseStringDate(date);      } catch (ParseException e) {         throw new RuntimeException(e);      }   }).collect(Collectors.toList());}Dokumentacja  https://projectlombok.org/features/SneakyThrows",
"url": "/2023/09/22/czy-wiesz-po-co-stosuje-sie-sneaky-throws-z-biblioteki-lombok.html",
"author": "Kamil Dudek",
"authorUrl": "/authors/kdudek.html",
"image": "kdudek.jpg",
"highlight": "/assets/img/posts/2023-09-22-czy-wiesz-po-co-stosuje-sie-sneaky-throws-z-biblioteki-lombok/sneaky.jpg",
"date": "22-09-2023",
"path": "pl/2023-09-22-czy-wiesz-po-co-stosuje-sie-sneaky-throws-z-biblioteki-lombok"
}
,


"2023-09-08-czy-wiesz-jak-skonfigurowac-relacyjna-baze-danych-przy-uzyciu-testcontainers-html": {
"title": "Czy wiesz, jak skonfigurować relacyjną bazę danych przy użyciu TestContainers?",
"lang": "pl",
"tags": "testcontainers db2 junit5",
"content": "Czasem w testach baza H2 nie jest wystarczająca, np. gdy używamy specyficznych dla danej bazy mechanizmów. Wtedy możemy łatwo skonfigurować TestContainers, które dostarczy nam instancje prawdziwej bazy danych na czas testów.Jak to zrobić na przykładzie DB2, Junit5 i Spring?Dodajemy zależności:&amp;lt;dependency&amp;gt;   &amp;lt;groupId&amp;gt;org.testcontainers&amp;lt;/groupId&amp;gt;   &amp;lt;artifactId&amp;gt;testcontainers&amp;lt;/artifactId&amp;gt;   &amp;lt;version&amp;gt;1.15.3&amp;lt;/version&amp;gt;   &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;&amp;lt;dependency&amp;gt;   &amp;lt;groupId&amp;gt;org.testcontainers&amp;lt;/groupId&amp;gt;   &amp;lt;artifactId&amp;gt;db2&amp;lt;/artifactId&amp;gt;   &amp;lt;version&amp;gt;1.14.3&amp;lt;/version&amp;gt;   &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;Ustawiamy konfigurację testową, korzystając z JDBC URL scheme:datasource:  url: jdbc:tc:db2:///databasename  driver-class-name: org.testcontainers.jdbc.ContainerDatabaseDriverMusimy jeszcze zaakceptować licencje db2, w tym celu w resources dodajemy plik “container-license-acceptance.txt”, który powinien zawierać linię:ibmcom/db2:11.5.0.0ato wszystko, cieszymy się działającą, prawdziwą instancją bazy danych, której nie musimy ręcznie dostarczać.Mamy też możliwość uruchomienia skryptu lub funkcji inicjującej bazę danych:url: jdbc:tc:db2:///databasename?TC_INITSCRIPT=init.sql # Konfiguracja pliku inicjującego bazęurl: jdbc:tc:db2:///databasename?TC_INITFUNCTION=org.testcontainers.jdbc.JDBCDriverTest::sampleInitFunction  # Konfiguracja funkcji inicjującejTo najszybszy i najłatwiejszy sposób konfiguracji bazy danych z TestContainers. Jeśli potrzebujemy więcej konfiguracji lub też chcemy wykorzystać kontener z bazą w inny sposób, możemy dostarczyć go ręcznie.Warto wspomnieć o dużej wadzie tego rozwiązania - czas uruchomienia kontenera z DB2 to blisko 3 minuty.Przykłady innych konfiguracjiZa pomocą adnotacji z Junit5Należy dodać zależność:&amp;lt;dependency&amp;gt;   &amp;lt;groupId&amp;gt;org.testcontainers&amp;lt;/groupId&amp;gt;   &amp;lt;artifactId&amp;gt;junit-jupiter&amp;lt;/artifactId&amp;gt;   &amp;lt;version&amp;gt;1.14.3&amp;lt;/version&amp;gt;   &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;Przykład:@Testcontainersclass ExampleTest {    // instancja współdzielona pomiędzy wszystkie metody testowe   @Container   private static final Db2Container SHARED_CONTAINER = new Db2Container();    // instancja tworzona przed i niszczona po każdej metodzie testowej   @Container   private Db2Container         localContainer =         new Db2Container().withDatabaseName(&quot;foo&quot;).withUsername(&quot;foo&quot;).withPassword(&quot;secret&quot;);    @Test   void test() {      assertTrue(SHARED_CONTAINER.isRunning());      assertTrue(localContainer.isRunning());   }}Jako singleton@SpringBootTest(classes = ExampleApplication.class)@ContextConfiguration(initializers = ExampleApplicationTestBase.Initializer.class)public abstract class ExampleApplicationTestBase {    public static class Initializer implements ApplicationContextInitializer&amp;lt;ConfigurableApplicationContext&amp;gt; {       @Override      public void initialize(@NotNull ConfigurableApplicationContext configurableApplicationContext) {         TestPropertyValues values = TestPropertyValues.of(               &quot;spring.datasource.url=&quot; + db2Container.getJdbcUrl(),               &quot;spring.datasource.password=&quot; + db2Container.getPassword(),               &quot;spring.datasource.username=&quot; + db2Container.getUsername());         values.applyTo(configurableApplicationContext);      }   }    private static final Db2Container db2Container;    static {      db2Container =            new Db2Container().withPassword(&quot;inmemory&quot;)                  .withUsername(&quot;inmemory&quot;)                  .withInitScript(&quot;init.sql&quot;)                  .withExposedPorts(2424, 2480)                  .withLogConsumer(new Slf4jLogConsumer(log))                  .acceptLicense();      db2Container.start();   } }Przydatne linki:  https://www.testcontainers.org/modules/databases/jdbc/  https://www.testcontainers.org/test_framework_integration/junit_5/  https://www.testcontainers.org/modules/databases/db2/",
"url": "/2023/09/08/czy-wiesz-jak-skonfigurowac-relacyjna-baze-danych-przy-uzyciu-testcontainers.html",
"author": "Piotr Stachowiak",
"authorUrl": "/authors/pstachowiak.html",
"image": "pstachowiak.jpg",
"highlight": "/assets/img/posts/2023-09-08-czy-wiesz-jak-skonfigurowac-relacyjna-baze-danych-przy-uzyciu-testcontainers/containership.jpg",
"date": "08-09-2023",
"path": "pl/2023-09-08-czy-wiesz-jak-skonfigurowac-relacyjna-baze-danych-przy-uzyciu-testcontainers"
}
,


"2023-08-25-czy-wiesz-czym-sa-sekwencje-i-jak-ich-uzywac-html": {
"title": "Czy wiesz, czym są sekwencje i jak ich (nie) używać?",
"lang": "pl",
"tags": "sekwencje bazy danych",
"content": "W sql możemy utworzyć kolumnę tak, żeby jej wartość była automatycznie inkrementowana (dekrementowana), co jest przydatne zwłaszcza w przypadku klucza głównego. W pewnych sytuacjach możemy jednak zdecydować, żeby zamiast tego skorzystać z sekwencji.W sql można utworzyć sekwencję, która będzie generatorem kolejnych wartości numerycznych zgodnie ze swoją specyfikacją. Można określić m.in. wartość minimalną i maksymalną lub ich brak, wartość początkową, o ile zwiększać (lub zmniejszać) kolejne wartości, typ, cykliczność lub jej brak oraz to, czy korzystać z cache’a i jakiej wielkości. Te i inne parametry są często opcjonalne, bo mają swoje domyślne wartości i to jedna z pułapek, które opiszę.RóżniceNajważniejsze różnice między kolumną Identity a sekwencją:  sekwencja nie jest powiązana z tabelą - można jej potencjalnie używać do generowania wartości dla kilku tabel lub nawet niezależnie od jakiejkolwiek tabeli,  w przypadku sekwencji można wygenerować wartość lub nawet cały zakres bez dodawania wierszy do tabeli - używając NEXT VALUE FOR,  w przypadku sekwencji to my musimy kontrolować zgodność typów i zarządzać ich użyciem w aplikacji,  czasami w jednej tabeli nie można mieć dwóch kolumn Identity, ale można skorzystać z dwóch sekwencji - trudno jednak poruszać ten temat w oderwaniu od technologii i tak np. w PostgreSQL mamy dostępny typ serial - czyli starą implementację automatycznie generowanych unikalnych wartości, który jednak nie jest częścią standardu sql - i w przypadku serial nie ma ograniczenia na liczbę kolumn serial w jednej tabeli).O czym należy więc pamiętać, korzystając z sekwencji?Typ sekwencji powinien być tożsamy z typem kolumny - miej kontrolę nad możliwymi różnicamiPrzykład 1create sequence my_seq start with 101;Sekwencja my_seq będzie miała typ BIGINT w PostgreSQL, ale INTEGER w db2 - bo takie są domyślne wartości, a nie zdefiniowaliśmy AS data-type.Jeśli mamy kolumnę typu BIGINT, to prawdopodobnie zakładamy, że może osiągnąć duże wartości. Jeśli dla takiej kolumny użyjemy sekwencji typu INTEGER, otrzymamy błąd, gdy sekwencja dobije do maksymalnej wartości dla INTEGERa.Przykład 2create sequence TASK_SEQ as BIGINT minvalue 1 increment by 1 no maxvalue cycle; create table TASK (   ID integer not null primary key,   text VARCHAR(254) not null);Wydawałoby się, że w drugą stronę nie ma większego problemu. Jeśli sekwencja ma typ BIGINT, a kolumna INTEGER i sekwencja nie jest używana dla żadnej innej tabeli, to widocznie oceniliśmy, tworząc tabelę, że nigdy nie dojdziemy do tak dużych wartości, żeby mieć problem. Nie zawsze…Zakładając, że w tabeli TASK jest bardzo dużo zadań, których szybko się pozbywamy, uznaliśmy, że najlepsze będzie użycie cyklu - kiedy sekwencja dobije do maksymalnej wartości, generuje minimalną wartość (i odwrotnie w przypadku sekwencji malejącej). Nie musimy się wtedy martwić ograniczeniami maksymalnej wartości użytego typu numerycznego.Niestety nasza sekwencja przekroczy maksymalną wartość INTEGERa i nie zresetuje się jeszcze, bo jest typu BIGINT. Jednak insert wiersza z taką wartością nie będzie dozwolony.Pamiętaj o cache’owaniuPrzykład 3create sequence ORDER_SEQ    minvalue 1    increment by 1    no maxvalue    cache 1000;Warto pamiętać o zdefiniowaniu cache’a, jeśli to możliwe, a zależy nam na wydajności zapytań. Wówczas naraz zostanie zaalokowanych w pamięci więcej wartości i przy odpowiednio dużym cache’u osiągniemy wydajność insertów z zastosowaniem sekwencji zbliżoną do insertów z zastosowaniem kolumny identity. W takiej sytuacji możemy “zgubić” niektóre wcześniej scache’owane wartości w przypadku awarii i mogą się zdarzyć przerwy w wartościach kolejnych wierszy.",
"url": "/2023/08/25/czy-wiesz-czym-sa-sekwencje-i-jak-ich-uzywac.html",
"author": "Barbara Mitan",
"authorUrl": "/authors/bmitan.html",
"image": "bmitan.jpg",
"highlight": "/assets/img/posts/2023-08-25-czy-wiesz-czym-sa-sekwencje-i-jak-ich-uzywac/sequence.webp",
"date": "25-08-2023",
"path": "pl/2023-08-25-czy-wiesz-czym-sa-sekwencje-i-jak-ich-uzywac"
}
,


"2023-08-09-indexeddb-html": {
"title": "IndexedDB - narzędzie do przechowywania danych w przeglądarce",
"lang": "pl",
"tags": "indexeddb webstorage database javascript",
"content": "IndexedDB to wbudowana w przeglądarkę internetową baza danych typu NoSQL. Przechowuje ona dane lokalnie w przeglądarce,co pozwala na korzystanie z nich nawet wtedy, gdy urządzenie nie ma połączenia z internetem. Dzięki temu IndexedDBstanowi świetną opcję dla aplikacji internetowych i jest dobrą alternatywą dla Local Storage.W przeciwieństwie do Local Storage, dane w IndexedDB nie są automatycznie usuwane przez przeglądarkę. Są one trwale przechowywane do momentu, gdy użytkownik zdecyduje się usunąć je ręcznie lub gdy aplikacja, która korzysta zIndexedDB, wykona odpowiednie operacje usuwania.Dodatkowo IndexedDB pozwala na:  obsługę transakcji,  tworzenie zapytań typu range query,  obsługę indeksów,  przechowywanie znacznie większej ilości danych niż Local Storage i cookies.Maksymalna ilość danych przechowywanych w IndexedDBMaksymalna ilość danych, którą można przechowywać w IndexedDB różni się w zależności od przeglądarki internetowej, ale zwykle jest to kilka gigabajtów.FirefoxFirefox nakłada ograniczenia w zależności od rozmiaru profilu użytkownika na dysku:  10% całkowitego rozmiaru dysku dla profilu użytkownika,  10 GB dla wszystkich stron z tej samej domeny (limity grupowe),  Do 50% całkowitego rozmiaru dysku, z ograniczeniem do 8 TB dla stron z przyznaną pamięcią trwałą.Przykład:Jeśli nasze urządzenie ma dysk o pojemności 1000 GB, Firefox pozwoli źródłu na przechowanie:  10 GB danych (limit grupy dla tej samej domeny),  500 GB danych (50% całkowitego rozmiaru dysku).Chrome i przeglądarki oparte na projekcie open-source Chromium (np. Edge)Przeglądarki te pozwalają źródłu na zajęcie do 60% całkowitego rozmiaru dysku.Przykładowo, jeśli urządzenie ma dysk o pojemności 1 TB, przeglądarka pozwoli źródłu na wykorzystanie do 600 GB.SafariW Safari źródło otrzymuje początkowy limit 1 GB.Gdy źródło osiągnie ten limit, Safari prosi użytkownika o zgodę na przechowywanie większej ilości danych.Kompatybilność i wsparcie przeglądarekListę przeglądarek wspierających opisywany mechanizm możemy znaleźć tutaj.Tworzenie bazyW celu rozpoczęcia prac z IndexedDB musimy najpierw “otworzyć” bazę danych (połączyć się z nią).// Otwieranie połączenia z bazą danych. // Jeśli baza nie istnieje to zostanie utworzona.const dbName = &#39;MyDatabase&#39;;const dbVersion = 1;const request = indexedDB.open(dbName, dbVersion);Aby podejrzeć bazę danych, musimy otworzyć konsolę deweloperską (devTools), przejść do zakładki “Application”, następnie w sekcji “Storage” znajduje się “IndexedDB”, a w niej utworzone bazy danych.Możemy utworzyć wiele baz danych o różnych nazwach, ale istnieją one w ramach jednego źródła (ang. origin - protokół warstwy aplikacji, domena, port).Strony internetowe działające w ramach tej samej domeny mają dostęp do swoich własnych baz danych, ale nie mogą uzyskaćdostępu do baz danych utworzonych na innych domenach. Poniżej zamieszczam źródło, dla którego została utworzona naszabaza danych:WersjonowanieWersjonowanie w IndexedDB jest kluczowym mechanizmem do zarządzania zmianami w strukturze danych podczas aktualizacjiaplikacji. Zapewnia spójność danych i minimalizuje ryzyko konfliktów między różnymi wersjami aplikacji, a co za tymidzie, zapobiega powtórnemu tworzeniu istniejących elementów struktury danych.Aby wersjonować bazę danych w IndexedDB, możemy określić numer wersji bazy danych podczas jej otwierania, używającdrugiego parametru w metodzie open() (tak jak w kodzie powyżej). Jeśli numer wersji jest większy niż wersjaistniejącej bazy danych, zostanie wywołany odpowiedni callback aktualizacji, który umożliwi dodanie nowego object store lub indeksów.Warto zauważyć, że próba utworzenia object store lub indeksu, który już istnieje w bazie danych, spowodujezgłoszenie błędu przez przeglądarkę.Usuwanie utworzonej bazyindexedDB.deleteDatabase(dbName);Object storeGłówną koncepcją w IndexedDB jest “object store” (magazyn obiektów). Object store jest niezbędny do przechowywaniadanych w bazie. W innych bazach danych, odpowiednik object store często nazywany jest “tabelą” lub “kolekcją”. WIndexedDB dane są reprezentowane jako obiekty JavaScript, które można przekształcić do formatu JSON lub innego formatutekstowego przed zapisaniem ich w bazie danych.IndexedDB dodatkowo obsługuje przechowywanie danych w różnych formatach, takich jak typy wbudowane JavaScript (np.liczby, ciągi znaków, daty), tablice oraz dane binarne. Warto zaznaczyć, że obiekty, których nie można serializować, niemogą być umieszczone w bazie.Dla każdej wartości w object store musi istnieć unikalny klucz (identyfikator). Klucz musi być jednym z tych typów:liczba, data, ciąg znaków, dane binarne lub tablica. Klucze mogą być generowane automatycznie.Dodawanie object store, przykład:// Tworzenie object store o nazwie &#39;Customers&#39;, z polem &#39;id&#39; // jako kluczem podstawowymlet db = request.result;const customersObjectStoreKey = &#39;Customers&#39;;if (!db.objectStoreNames.contains(customersObjectStoreKey)) {    db.createObjectStore(customersObjectStoreKey, {keyPath: &#39;id&#39;});}keyPath to właściwość, która istnieje zawsze i zawiera unikalną wartość.Można również użyć generatora kluczy, takiego jak autoIncrement. Generator kluczy tworzy unikalną wartość dla każdegoobiektu dodanego do object store. Domyślnie, jeśli nie określimy klucza, IndexedDB tworzy i przechowuje go oddzielnie oddanych.db.createObjectStore(&#39;id&#39;, {autoIncrement: true});Usuwanie object storedb.deleteObjectStore(&#39;Customers&#39;);Operacje na danychWszystkie operacje na danych w IndexedDB muszą być wykonywane w ramach transakcji, co pozwala na zapewnienie spójnościdanych. Transakcja w IndexedDB jest zbiorem operacji, które wykonają się atomowo - wszystkie lub, w przypadku błędu, nie wykona się żadna z nich.Zarządzanie danymi w transakcyjny sposób pomaga w unikaniu konfliktów i utrzymaniuspójności danych nawet w przypadku równoczesnych operacji na bazie danych. Jest to ważne w przypadku aplikacji, w którychodbywa sie przetwarzanie równoległe (np. wielu użytkowników pracujących równolegle) lub wymagają pracy w trybie offline.Przykład tworzenia transakcji:const customersObjectStore = &#39;Customers&#39;;const transactionType = &#39;readwrite&#39;;const transaction = db.transaction(customersObjectStore, transactionType);// Pobranie object store dla klientów w celu wykonywania dowolnej operacjiconst customers = transaction.objectStore(customersObjectStoreKey);Typy transakcji:  readonly - umożliwia jedynie odczyt danych (opcja domyślna).  readwrite - umożliwia zarówno odczyt, jak i zapis danych, ale nie pozwala na tworzenie, usuwanie ani modyfikacjęobject store.Przykładowy kod dodający obiekt do naszej bazy danych:// Rozpoczynanie transakcji na object store &#39;Customers&#39;const transaction = db.transaction(customersObjectStoreKey, &#39;readwrite&#39;);const customers = transaction.objectStore(customersObjectStoreKey);// Dodawanie danych do object storeconst customer = { id: 1, name: &#39;Jan&#39;, surname: &#39;Kowalski&#39; };const transactioRequest = customers.add(customer);Podgląd bazy danych po dodaniu obiektu:Podstawowe operacje, które można wykonać w ramach transakcji to:  add(data, optionalKey) - dodawanie nowego rekordu do object store,  get(primaryKey) - pobieranie rekordu na podstawie klucza,  getAll(optionalConstraint) - odczytywanie wszystkich danych z object store,  put(data) - aktualizowanie lub dodawanie rekordu w object store,  put(data, optionalKey) - aktualizacja danych na podstawie istniejącego klucza,  delete(primaryKey) - usuwanie rekordu na podstawie klucza,  clear() - usuwanie wszystkich rekordów z object store.PodsumowanieW artykule przedstawione zostały podstawowe koncepcje i operacje na danych w kontekście IndexedDB. Zachęcam do zgłebieniatematu i skorzystania z dodatkowych źródeł zamieszczonych poniżej, które dostarczą bardziej zaawansowanych informacji natemat tego mechanizmu.Playgroundhttps://jsfiddle.net/wstolarski_consdata/awy94tsm/1/Źródła  https://javascript.info/indexeddb  https://www.freecodecamp.org/news/how-indexeddb-works-for-beginners/  https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API  https://developer.mozilla.org/en-US/docs/Web/API/Storage_API/",
"url": "/2023/08/09/indexeddb.html",
"author": "Wojciech Stolarski",
"authorUrl": "/authors/wstolarski.html",
"image": "wstolarski.jpg",
"highlight": "/assets/img/posts/2023-08-09-czy-wiesz-czym-jest-indexeddb/post_img.jpg",
"date": "09-08-2023",
"path": "pl/2023-08-09-indexeddb"
}
,


"2023-07-28-czy-wiesz-ze-mozesz-ulatwic-sobie-prace-korzystajac-z-internetowych-generatorow-stylow-html": {
"title": "Czy wiesz, że możesz ułatwić sobie pracę, korzystając z internetowych generatorów stylów?",
"lang": "pl",
"tags": "css generator",
"content": "Dzisiaj będzie o chodzeniu na skróty. Czasami przychodzi taki moment, że musimy dokonać karkołomnych czynów w css/scss. Zrobić jakąś ramkę, cień czy dzióbek do tooltipa. W tego typu sytuacjach możemy się popisać znajomością takich własności jak: radial-gradient, clip-path czy cubic-bezier. A co jeśli nie przerobiliśmy tutoriali do końca, ani nie mieliśmy okazji ich jeszcze użyć?W takim momencie warto mieć świadomość, że istnieją internetowe generatory stylów, które mogą przyspieszyć naszą pracę. Możemy za ich pomocą szybko wyklikać interesującą nas ramkę, a później wygenerowany kod przenieść do stylów. Warto oczywiście przed kopiowaniem zagłębić się w kod i go zrozumieć, a nie tylko bezmyślnie kopiować. Zrozumienie wygenerowanego kodu ułatwi nam w przyszłości dokonywanie podobnych zmian.Poniżej znajduje się kilka przykładowych internetowych generatorów:  Generowanie fancy ramek,  Obcinanie krawędzi,  Generowanie cieni pod obiektami i tekstami,  Generowanie krzywych do animacji.",
"url": "/2023/07/28/czy-wiesz-ze-mozesz-ulatwic-sobie-prace-korzystajac-z-internetowych-generatorow-stylow.html",
"author": "Piotr Grobelny",
"authorUrl": "/authors/pgrobelny.html",
"image": "pgrobelny.webp",
"highlight": "/assets/img/posts/2023-07-28-czy-wiesz-ze-mozesz-ulatwic-sobie-prace-korzystajac-z-internetowych-generatorow-stylow/css.jpg",
"date": "28-07-2023",
"path": "pl/2023-07-28-czy-wiesz-ze-mozesz-ulatwic-sobie-prace-korzystajac-z-internetowych-generatorow-stylow"
}
,


"2023-07-14-czy-wiesz-do-czego-sluzy-outlier-pattern-w-mongodb-html": {
"title": "Czy wiesz do czego służy Outlier Pattern w MongoDB?",
"lang": "pl",
"tags": "mongodb outlier pattern",
"content": "Wzorzec ten jest stosowany, kiedy większość danych można i warto zamodelować w jeden sposób, ale dla niewielkiego odsetka dokumentów, które odstają od normy, będzie to nieakceptowalne lub niemożliwe.PrzykładPrzedstawmy wzorzec Outlier na przykładzie. Załóżmy, że mamy kolekcję books, w której przechowujemy książki z podstawowymi danymi. Chcemy też łatwo wyciągnąć informację, kto kupił daną książkę - żeby na tej podstawie rekomendować inne książki.Możemy zdecydować się na zapisywanie identyfikatorów użytkowników w naszym dokumencie książki zamiast osobno. Dla większości książek będzie to najwydajniejsze rozwiązanie.{    &quot;_id&quot;: 12,    &quot;title&quot;: &quot;MongoDB Patterns&quot;,    ...,    &quot;customers_purchased&quot;: [&quot;user00&quot;, &quot;user01&quot;, &quot;user02&quot;] }Może się jednak okazać, że dla bestsellerów przekroczymy dopuszczalny maksymalny rozmiar dokumentów - 16 MB. Jeśli zdecydujemy się wyciągnąć te informacje z books do osobnej kolekcji, to skończymy z rozwiązaniem działającym dla wszystkich przypadków, ale niewydajnym dla ponad 99% z nich.Chcąc mieć wydajność pierwszego rozwiązania bez jego ograniczeń, z dodatkową obsługą tylko dla niewielkiej liczby dokumentów, wprowadzamy wzorzec Outlier.Nadal większość dokumentów będzie przechowywać identyfikatory w polu customers_purchased i będą wyglądać dokładnie tak jak pierwszy przykład. Jeśli liczba użytkowników, która kupiła daną książkę, przekroczy określony przez nas limit, np. 1000, kolejnych użytkowników zaczynamy przechowywać osobno, a do tego dokumentu dodamy flagę has_extras z wartością true.{    &quot;_id&quot;: 13,    &quot;title&quot;: &quot;Clean code&quot;,    ...,    &quot;customers_purchased&quot;: [&quot;user00&quot;, &quot;user05&quot;, &quot;user06&quot;, ...],    &quot;has_extras&quot;: &quot;true&quot;}{  &quot;book_id&quot;: 13,  &quot;customers_purchased&quot;: [&quot;user6045&quot;, &quot;user3451&quot;, &quot;user1242&quot;, ...]}Musimy zadbać o to, żeby dociągnąć potrzebne informacje dla dokumentów z has_extras. Warto jednak robić to w jednym miejscu w kodzie, a we wszystkich innych ta flaga i cały design powinny być tak naprawdę transparentne i nie mieć wpływu na resztę kodu aplikacji.ZastosowaniaInne zastosowania to relacje (polubienia, obserwowanie) w sieciach społecznościowych czy recenzje filmów. Popularne osoby, filmy, książki mogą zaburzyć standardowe zastosowania i typową liczbę powiązań.Źródła  https://www.mongodb.com/developer/products/mongodb/outlier-pattern/  https://www.mongodb.com/blog/post/6-rules-of-thumb-for-mongodb-schema-design",
"url": "/2023/07/14/czy-wiesz-do-czego-sluzy-outlier-pattern-w-mongodb.html",
"author": "Barbara Mitan",
"authorUrl": "/authors/bmitan.html",
"image": "bmitan.jpg",
"highlight": "/assets/img/posts/2023-07-14-czy-wiesz-do-czego-sluzy-outlier-pattern-w-mongodb/outlier.webp",
"date": "14-07-2023",
"path": "pl/2023-07-14-czy-wiesz-do-czego-sluzy-outlier-pattern-w-mongodb"
}
,


"2023-06-28-czy-wiesz-ze-w-angular-16-pojawia-sie-sygnaly-html": {
"title": "Czy wiesz, że w Angular 16 pojawią się sygnały?",
"lang": "pl",
"tags": "angular signals",
"content": "Sygnały to nowa koncepcja w Angular, upraszcza ona tworzenie reaktywnych komponentów. Mogą one w przyszłości doprowadzić do usunięcia Zone.js z Angular.Przykład utworzenia i odczytywania wartości z sygnału:// tworzenie sygnałuname = signal(&#39;Szczupły Zbyszek&#39;);// odczytywanie wartościname();// ustawienie nowej wartościname.set(&#39;Piękna Marysia&#39;);Do utworzenia sygnału potrzebna jest zawsze początkowa wartość. Obecną wartość z sygnału można odczytać wywołując sygnał jak funkcję.Przykład prostego komponentu:@Component({    selector: &#39;simple-name-signal&#39;,    standalone: true,    imports: [CommonModule],    template: `        &amp;lt;p&amp;gt;{{ name() }}&amp;lt;/p&amp;gt;        &amp;lt;button (click)=&quot;generateName()&quot;&amp;gt;Generate name&amp;lt;/button&amp;gt;    `,    changeDetection: ChangeDetectionStrategy.OnPush,})export class SimpleNameSignalComponent {    name = signal(&#39;Szczupły Zbyszek&#39;);        constructor(private generatorSerivce: GeneratorService) {}        generateName() {        this.name.set(this.generatorSerivce.generateName());    }}Jeżeli wartość sygnału name nie ulegnie zmianie, komponent nie zostanie ponownie wyrysowany.Sygnał zawsze posiada wartość, dlatego możliwe jest odczytanie ostatniej wartości np. w logice obsługi eventów:@Component({    selector: &#39;simple-name-signal&#39;,    standalone: true,    imports: [CommonModule],    template: `        &amp;lt;p&amp;gt;{{ data() }}&amp;lt;/p&amp;gt;        &amp;lt;button (click)=&quot;doSomething()&quot;&amp;gt;Do something&amp;lt;/button&amp;gt;    `,    changeDetection: ChangeDetectionStrategy.OnPush,})export class SimpleNameSignalComponent {        data = signal(&#39;Szczupły Zbyszek&#39;);        doSomething() {    console.log(this.data()); // wywołanie this.data(), zwróci ostatnią wartość, bez rekalkulacji!    }}Sygnały oferują możliwość agregowania, tak jak combineLatest z rxjs. Przykład:// rxjsname = new BehaviorSubject(&#39;gorgeous Zbyszek&#39;);counter = new BehaviorSubject(0);concat$ = combineLatest([this.name, this.counter]).pipe(    tap((data) =&amp;gt; {        console.log(&#39;RXJS computing concat&#39;, data);    }),    map(([name, counter]) =&amp;gt; `${name}, counter: ${counter}`));this.name.next(&#39;Grześ&#39;);this.name.next(&#39;Grześ&#39;);// output// RXJS computing concat [&quot;Grześ&quot;, 0]// RXJS computing concat [&quot;Grześ&quot;, 0]// signalname = signal(&#39;gorgeous Zbyszek&#39;);counter = signal(0);concat = computed(() =&amp;gt; {    console.log(&#39;SIGNAL computing concat&#39;);    return `${this.name()}, counter: ${this.counter()}`;});this.name.set(&#39;Grześ&#39;);this.name.set(&#39;Grześ&#39;);// output:// SIGNAL computing concatSygnały działają jak producer-consumer. Jeżeli ustawiamy nową wartość na sygnale, konsumer - name() otrzyma powiadomienie o zmianie i funkcja concat zostanie przeliczona.W przypadku rxjs zostaną wyemitowane dwa eventy. Wykorzystując dodatkowy operator - distinctUntilChanged, wynik byłby taki sam jak w przypadku sygnałów.A jak to wpłynie na NgRx? Nadal toczy się dyskusja (link, link). Proponowana jest możliwość stworzenia SignalStore oraz pobierania selektorów, które zwrócą sygnał:data = this.store.selectSignal(selectData);Przydatne linki  Playground - komponent rxjs oraz komponent oparty o sygnały,  https://itnext.io/angular-signals-the-future-of-angular-395a69e60062  https://www.freecodecamp.org/news/angular-signals/  https://dev.to/this-is-angular/signals-what-this-means-for-ngrx-eho",
"url": "/2023/06/28/czy-wiesz-ze-w-angular-16-pojawia-sie-sygnaly.html",
"author": "Dorian Mejer",
"authorUrl": "/authors/dmejer.html",
"image": "dmejer.jpg",
"highlight": "/assets/img/posts/2023-06-28-czy-wiesz-ze-w-angular-16-pojawia-sie-sygnaly/angular.png",
"date": "28-06-2023",
"path": "pl/2023-06-28-czy-wiesz-ze-w-angular-16-pojawia-sie-sygnaly"
}
,


"2023-04-05-zarzadzanie-wersjami-w-package-json-html": {
"title": "Czy wiesz, jak zarządzać wersjami w package.json?",
"lang": "pl",
"tags": "package.json npm",
"content": "package.json jest plikiem, który opisuje każdy projekt oparty o Node.js. Jedną z jego najważniejszych ról jest przechowywanie informacji o zależnościach projektu. Wersję zależności można wskazać na kilka sposobów używając do tego operatorów - ten wpis przybliża ich znaczenie. Większość operatorów jest przydatna, jeżeli zależna paczka stosuje semantyczne wersjonowanie (major.minor.patch).Operatory &amp;gt;=, &amp;gt;, =, &amp;lt;, &amp;lt;=, ||  &amp;gt;=1.6.7 oznacza wersje od 1.6.7 w górę,  &amp;gt;=1.6.7 &amp;lt;2.0.0 wersje od 1.6.7 do 2.0.0, bez 2.0.0,  &amp;lt;=2.0.0 wersje do 2.0.0, włącznie z 2.0.0,  1.1.1 || &amp;gt;=1.1.3 &amp;lt;2.0.0 1.1.1, większa od 1.1.3 do 2.0.0, gdy 1.1.2 powodowała problemy i chcemy ją pominąć.Operator -  1.6.7 - 2.0.0, to samo co &amp;gt;=1.6.7 &amp;lt;=2.0.0 (czyli, od X do Y włącznie),  1 - 2.0.0, to samo co &amp;gt;=1.0.0 &amp;lt;=2.0.0 (pierwsza wersja zakresu uzupełniana jest zerami),  1 - 2, to samo co &amp;gt;=1.0.0 &amp;lt;3.0.0 (druga wersja w zakresie, wszystko poza wersją “major” może ulec zmianie).Operator * lub x  1.*, to samo co &amp;gt;=1.0.0 &amp;lt;2.0.0,  1.2.x, to samo co &amp;gt;=1.2.0 &amp;lt;1.3.0.Operator ~Umożliwia zmianę wersji “patch”, gdy ustawiona jest na wersji “minor” lub zmianę wersji “major”, gdy ustawiona na “major”  ~1.6.7 oznacza to samo co &amp;gt;=1.6.7 &amp;lt;1.7.0,  ~1.6 jest tym samym co &amp;gt;=1.6.7 &amp;lt;1.7.0, to samo co 1.6.* (pozwala na zmianę wersji “patch”),  ~1 jest tożsame z &amp;gt;=1.0.0 &amp;lt;2.0.0, oraz 1.* (pozwala na zmianę wersji “minor”),  ~1.2.3-beta.10, to samo co &amp;gt;=1.2.3-beta.10 &amp;lt;2.0.0, ale np: 1.2.3-beta.11 wpadnie do zakresu, natomiast 1.2.4-beta.1 już nie.Operator ^Umożliwia zmianę tylko pierwszej niezerowej wersji. Zmiana pierwszej niezerowej wersji często uznawana jest za “breaking-change”, ^ nas przed tym chroni.  ^1.2.3 jest tym samym co &amp;gt;=1.2.3 &amp;lt;2.0.0 oraz 1.*,  ^0.2.3 oznacza to samo co &amp;gt;=0.2.3 &amp;lt;0.3.0, i to samo co 0.*,  ^0.0.3, to samo co &amp;gt;=0.0.3 &amp;lt;0.0.4, oraz to samo co 0.0.*.",
"url": "/2023/04/05/zarzadzanie-wersjami-w-package-json.html",
"author": "Dorian Mejer",
"authorUrl": "/authors/dmejer.html",
"image": "dmejer.jpg",
"highlight": "/assets/img/posts/2023-04-05-zarzadzanie-wersjami-w-package-json/keyboard.jpg",
"date": "05-04-2023",
"path": "pl/2023-04-05-zarzadzanie-wersjami-w-package-json"
}
,


"2023-02-15-uiux-tools-html": {
"title": "Figma - narzędzie do projektowania interfejsu użytkownika",
"lang": "pl",
"tags": "ui/ux figma",
"content": "Figma - jest bardzo popularnym i mocno rozwijającym się narzędziem do projektowania i prototypowania zaawansowanych interfejsów aplikacji mobilnych oraz stron internetowych. W Figmie mogą współpracować projektanci, marketerzy, menedżerowie oraz programiści, wspólnie tworząc lub też omawiając na bieżąco wygląd powstającej aplikacji. Figma zyskała swoją przewagę na rynku przed innymi rozwiązaniami dzięki interaktywnym widokom, prostemu i intuicyjnemu interfejsowi oraz bogatej funkcjonalności.InterfejsInterfejs Figmy składa się z wielu różnych elementów pogrupowanych według funkcjonalności. W narzędziu tym łatwo jest tworzyć różnorodne kształty o dowolnym stopniu trudności, które możemy skalować, eksportować i na ich podstawie tworzyć zaawansowane komponenty widoków. Można użyć grafiki wektorowej, gdy trzeba tworzyć proste ilustracje, w tym przyciski, logo, ikonki. Figma pozwala również na tworzenie dynamicznych efektów np. klikalnych przycisków, rozwijanych list, tworzenie animacji przejść przez widoki lub też interakcji komponentów z użytkownikiem.KomponentyKomponenty w Figmie pomagają zastosować zmiany do grupy elementów. Oszczędza to czas projektanta podczas zmiany całej makiety. Załóżmy, że stworzyliśmy makietę składającą się z 50 widoków, a klient chciał zmienić w nim kolor przycisków. W przypadku komponentów wystarczy wprowadzić zmianę do jednego obiektu, po czym nowy kolor zostanie zastosowany do wszystkich przycisków jednocześnie.Aby utworzyć komponent, wystarczy zaznaczyć te elementy lub grupy elementów, z których chcemy go utworzyć, po czym kliknąć prawym przyciskiem myszy i wybrać “Create Component” (lub Ctrl+Alt+K). Teraz tworząc kopię, uzyskujemy komponenty potomne. Wszystkie zmiany dotyczące rodzica zostaną przeniesione na komponenty od niego pochodzące.Auto LayoutFunkcja Auto Layout jest jedną z najważniejszych i najczęściej używanych właściwości w Figmie, która odróżnia ją od innych narzędzi projektowych. Krótko mówiąc, Auto Layout pozwala na strukturyzację komponentów i ramek w sposób, który może automatycznie się rozszerzać lub zmniejszać, dzięki czemu obiekt złączony z innych obiektów dostosowuje się do wielkości jego zawartości lub też w inną stronę. Auto Layout pozwala na określenie kierunku ułożenia elementów (pionowo lub poziomo), pozwala także na określenie dynamicznego ustawiania odstępów, ograniczenia wielkości, szerokości, długości obiektów oraz automatyczne wyrównywanie położenia. Jeżeli znasz się na stylowaniu CSS, to możesz słusznie zauważyć pewne podobieństwo Auto Layout do Flexbox.Aby zastosować Auto Layout wystarczy wydzielić obiekty, które chcemy uzależnić od siebie, po czym wcisnąć skrót klawiszowy Shift+A albo wybrać opcję Auto Layout w prawym menu.PrototypowanieFunkcje prototypowania w Figmie umożliwiają tworzenie interaktywnych przepływów pomiędzy makietami, co przybliża makietę do prawdziwej aplikacji w porównaniu do zwykłych statycznych ekranów. Prototypy replikują sposób, w jaki użytkownicy mogą wchodzić w interakcję z zaprojektowanym interfejsem aplikacji. Możemy łączyć elementy na wybranym ekranie lub też poszczególne ekrany między sobą w celu utworzenia przepływu, czyli pełnej  interaktywnej ścieżki. Np. w przypadku makiety witryny e-Commerce, jesteśmy w stanie zamodelować za pomocą przepływów wszystkie możliwe interakcje użytkownika - tworzenie konta, dodawanie elementów do koszyka, wylogowania się itd.Przepływ składa się z trzech części:  Hotspot (punkt wejściowy) - to jest miejsce, w którym użytkownik rozpoczyna interakcję. Możemy utworzyć punkt wejściowy na dowolnym przycisku, ikonce lub nagłówku.  Connection (połączenie) - strzałka, która łączy hotspot z miejscem docelowym. Zarówno interakcja, jak i animacja są określane przez połączenie.  Destination (punkt wyjściowy) - miejsce docelowe, kolejny krok w przepływie, w którym kończy się połączenie.Po utworzeniu połączenia można zdefiniować rodzaj interakcji, czyli jakie działanie uruchomi utworzony przepływ. Może to być naciśnięcie/najechanie myszką, wciśnięcie klawisza, interakcja w momencie puszczenia myszki lub wiele innych. Jest też możliwość zdefiniowania akcji, czyli w jaki sposób dany przepływ zostanie wykonany. Możemy wybrać spośród wielu różnych akcji, m.in. nawigacji, zmiany obiektu (interaktywny komponent), otwierania linku, przejścia wstecz lub też przewijania do poszczególnych miejsc.Rozwiązania konkurencyjne wobec FigmySketchJest oprogramowaniem znanym większości projektantów interfejsu użytkownika. Od lat uznawany za złoty standard w projektowaniu makiet, jest docelowym konkurentem Figmy, jednak dostęp do niego mają wyłącznie użytkownicy korzystający z systemu macOS. Dodatkowo, by móc cieszyć się z jego pełnej wersji, musimy płacić pewną kwotę roczną.Różnica Figma a Sketch            Różnice      Figma      Sketch                  Kompatybilność      Przeglądarka internetowa      System operacyjny macOS              Cennik      Wersja darmowa, professional, organization oraz enterprice      Jednorazowa wpłata albo opłata per edytor w Sketch Teams              Style      Tworzenie style dla różnorodnych elementów, które mogą być mieszane i dopasowywane do innych zestawów elementów      Ma dwa typy stylów: tekst lub warstwa. Elementy są ustalane dla każdego ze styli              Symbole i komponenty      Komponenty są przepływowe, gdy główny komponent jest zmieniany, to każda instancja utworzona na podstawie komponentu głównego się zmieni      Wszelkie modyfikacje wprowadzane na zakładce Symbols automatycznie odnoszą się do każdego wystąpienia tego symbolu              Narzędzia wektorowe      Vector Networks w Figmie pozwala projektantowi rysować wektorowe ścieżki w dowolnym kierunku bez konieczności połączenia się z punktem pierwotnym      Podczas korzystania z narzędzi wektorowych w Sketchu, projektant obowiązkowo musi połączyć koniec ciągu punktów z początkiem              Pluginy      Wprowadzono w 2019 roku. Zachęca programistów do tworzenia większej liczby pluginów      Oferuje większa liczbę pluginów od innych firm              Prototypowanie      Wbudowana integracja z Principle. Obsługuje overlays podczas łączenia obiektów oraz ma większa liczbę triggerów używanych podczas prototypowania      Obsługuje większość zaawansowanych aplikacji do prototypowania              Współpraca      Oparta na chmurze, obsługuje wielu projektantów do pracy i edycji dokumentu jednocześnie      W 2021 roku wprowadzono możliwość współpracy różnych projektantów w tym samym czasie      Zalety Sketch wobec Figmy - według twórców Sketcha  Zawiera potężną natywną aplikację Mac.  Pełny przepływ pracy w trybie offline.  Użytkownik sam kontroluje, kto widzi jego pracę.  Ścisła kontrola nadpisywania dla systemów projektowych.  Otwarty format plików.  Zaawansowane zarządzanie kolorami.  Praca z lokalnymi dokumentami.  Szablony obszaru roboczego.  Brak limitu rozmiaru dokumentu (jedyne ograniczenie poprzez RAM).  Rozliczenia bez niespodzianek.  Kompletna platforma jedynie za 9$/miesiąc.  Karta kredytowa nie jest wymagana do pełnego funkcjonowania bezpłatnego okresu próbnego.  Niezalażność.Zalety Figma wobec Sketch - według twórców Figmy  Szybsza dzięki chmurze.  Multiplatformowa.  Mniej znaczy więcej - używając Sketch musisz również korzystać z InVision, Abstract lub Zeplin. Figma ma to wszystko w sobie.  Lepsza praca zespołowa - wszyscy współpracownicy w Sketch muszą mieć płatną subskrypcję. Funkcja współpracy w Sketch jest bardzo świeża i jej użycie, tak samo jak użycie całego oprogramowania Sketh, jest możliwe tylko na komputerach Mac.  Lepsza wydajność dzięki 2D WebGL renderingu.  Bardzo prosta migracja - import plików Sketch do Figmy automatycznie przekształca Symbole w Komponenty i utrzymuje warstwy w nienaruszonym stanie.PenPotJest pierwszą Open Source platformą do projektowania i prototypowania interfejsów. To bardzo świeża aplikacja dostępna w przeglądarce, której interfejs mocno przypomina interfejs konkurencyjnej Figmy. Jednak z tego względu, że technologia dopiero się rozwija, odczuwalny jest brak większości funkcjonalności:  W PenPot nie ma możliwości utworzenia komponentów, czyli obiektów potomnych, które zmieniają się wraz z dokonanymi zmianami w komponencie rodzica.  Brak Auto Layout’u.  Brak pluginów (W Figmie mamy mnóstwo różnych pluginów wspomagających tworzenie makiet - np. Iconify, Unsplash itd.)Quant UXTak samo jak w przypadku PenPot, mamy tu do czynienia z narzędziem Open Source dostępnym w przeglądarce. Interfejs narzędzia Quant UX jest zbliżony do interfejsu Figmy i pozwala tworzyć różne obiekty na podstawie kształtów. Wyróżnia się ogromnym wyborem wcześniej utworzonych widget’ów, możemy wykorzystać gotowe komponenty i na ich podstawie stworzyć makietę. Wśród gotowych widgetów mamy takie obiekty jak: przyciski BootStrap’owe, wykresy, checkbox, slider, date picker i wiele innych. Możemy utworzyć własny widget i dodać go do kolekcji, żeby potem re-użyć. Narzędzie Quant UX, w porównaniu do Figmy, mocno koncentruje się na interaktywności i testowaniu. Możemy włączyć tryb Symulacji, po czym wszystkie dodane widget’y będą klikalne, np. lista się rozwinie, checkbox’y można będzie zaznaczyć itd. Quant UX pozwala na pisanie skryptów w języku JavaScript, wprowadzając logikę i przekazanie danych pomiędzy obiektami makiety za pomocą Databinding’u. Testowanie w tym narzędziu możemy przeprowadzić za pomocą tak zwanej Heat Map, która pokazuje statystyki, w jakie miejsca w aplikacji użytkownicy klikają najrzadziej/najczęściej. Jeżeli zależy nam na szybkim utworzeniu makiety z wysoką interaktywnością elementów oraz możliwością przetestowania zachowania użytkowników, Quant UX może być dobrym rozwiązaniem.Zestaw materiałów do nauki z narzędzia Figma:Kursy Figma na Udemy:  https://www.udemy.com/course/complete-web-designer-mobile-designer-zero-to-mastery/  https://www.udemy.com/course/figma-ux-ui-design-user-experience-tutorial-course/Tutoriale Figma na Youtube:  https://www.youtube.com/watch?v=kbZejnPXyLM  https://www.youtube.com/watch?v=FTFaQWZBqQ8  https://www.youtube.com/watch?v=Gu1so3pz4bAŹródła:  https://www.figma.com/about/  https://www.figma.com/pricing/  https://www.youtube.com/watch?v=yzO31hMTkus&amp;amp;t=479s  https://www.imaginarycloud.com/blog/figma-vs-sketch/  https://kinsta.com/blog/figma-vs-sketch/  https://www.sketch.com/vs/figma/  https://www.figma.com/figma-vs-sketch/",
"url": "/2023/02/15/uiux-tools.html",
"author": "Juris Lavrinovics",
"authorUrl": "/authors/jlavrinovics.html",
"image": "jlavrinovics.jpg",
"highlight": "/assets/img/posts/2023-02-15-uiux-tools/uiux-tools.jpg",
"date": "15-02-2023",
"path": "pl/2023-02-15-uiux-tools"
}
,


"2022-12-15-memory-java-html": {
"title": "Dobór limitów pamięci w Javie (w kontenerach i nie tylko)",
"lang": "pl",
"tags": "Java oom memory limit kubernetes openshift",
"content": "Geneza problemuSłowem wstępu: nasz projekt działa na OpenShifcie, który jest niczym innym jak Kubernetesem z dodatkowymi bajerami. Z tego też tytułu wszystko tutaj dzieje się w kontenerach. Natomiast część wniosków, do których doszliśmy, aplikuje się nie tylko do procesów Javy chodzących w kontenerze, ale też Javy odpalonej “po staremu”, bez kontenerów.A teraz do rzeczy: od pewnego czasu w tym projekcie obserwowaliśmy problemy z pamięcią - kontenery zaczęły być ubijane przez systemowy OOM Killer. Zwracam tutaj uwagę na różnicę pomiędzy standardowym javowym OOM Exception a systemowym OOM Killerem - to są dwie różne rzeczy. Kompletnie jednak nam się to nie spinało, tj. widzieliśmy kilka rozbieżności:  W momencie ubijania kontenera przez systemowy OOM killer, większość znanych nam narzędzi do monitoringu (z samym OpenShiftem na czele) wskazywała, że ten kontener jeszcze nie dobił do limitu przydzielonej mu pamięci. Np. limit pamięci dla kontenera to 640MB, a systemowy OOM zabijał go w momencie, kiedy kontener teoretycznie bierze 600MB. Co się stało z tymi 40MB?  W ramach kontenera chodzi wyłącznie jeden proces - Java. Posługując się poleceniem ps aux, możemy sprawdzić, ile zajmuje rzeczywistej pamięci, patrząc na kolumnę RSS. Na ogół potrafi to być spore przybliżenie (albo i wprost - mało prawdziwa wartość) z uwagi na współdzielenie pamięci pomiędzy procesami, jednak w przypadku kontenera, na którym chodzi pojedynczy proces, jest to dobra metryka. I tutaj dochodzimy do drugiej rozbieżności: RSS procesu Javy wskazywał na zużycie pamięci większe o jakieś 100-150MB niż by to wynikało z sumy wartości Heapa oraz Off-Heapa podawanej przez JVM w Native Memory Tracking.AnalizaKrótko mówiąc, ginęło nam w porywach kilkaset MB pamięci per kontener. Przy kilkuset kontenerach, które są odpalone na naszych środowiskach, sumuje się to do sporego problemu. Żeby się doliczyć tej zaginionej pamięci, musieliśmy znaleźć odpowiedzi na kilka pytań:W jaki sposób faktycznie jest liczone zużycie pamięci przez kontener z punktu widzenia systemu operacyjnego?Sprawa jest trochę bardziej zawiła niż w przypadku zwykłych procesów chodzących na hoście. Czym w ogóle są kontenery? Wyizolowanym procesem. Wbrew teoriom, na które można trafić, kontenery nie mają praktycznie nic wspólnego z maszynami wirtualnymi, nie ma w nich żadnej warstwy emulacji sprzętu, ani tym bardziej systemu. W szczególności, w kontenerze nie chodzi cały system operacyjny - kontenery współdzielą kernel z hostem, a procesy chodzące w ramach kontenera są niczym innym, jak zwykłymi procesami chodzącymi na hoście, jedynie w izolacji. I ta izolacja jest tutaj kluczowa. Bez wdawania się w zbyt duże szczegóły, za izolację (i tak w sumie za działanie kontenerów w ogóle) odpowiadają dwa mechanizmy kernela – namespaces oraz cgroups. W szczególności interesuje nas tutaj ten drugi - cgroups. Jest to mechanizm, który ogranicza zasoby (np. dostępną pamięć) procesom lub grupom procesów. W dużym uproszczeniu każdy kontener jest “zamknięty” w ramach własnej cgroupy. A ponieważ systemowy OOM killer patrzy właśnie na zużycie pamięci liczone z punktu widzenia cgroups, to kluczowe staje się ustalenie, jak zużycie pamięci kontenera wygląda z punktu widzenia cgroups.Wszelkie komendy w stylu htop, docker stats, itp. są tutaj niewystarczające. Istnieje natomiast coś w rodzaju odpowiednika htop, który pokazuje zużycie pamięci dla poszczególych cgroup: systemd-cgtop. Przykładowe wyjście cgtopa wygląda tak:Control Group            Procs   %CPU   Memory  Input/s Output/s/                           49   44.1    26.9G        -        -kubepods.slice               -   15.7    18.2G        -        -kubepod…urstable.slice       -   15.7    17.8G        -        -system.slice                32   16.6     7.7G        -        -system.…urnald.service       1    5.9     3.7G        -        -system.…e/crio.service       4    1.3     3.4G        -        -kubepod…d4422dd7.slice       -    2.1   928.3M        -        -kubepod…f83d3bc3.slice       -    0.6   638.1M        -        -kubepod…fb85c98b.slice       -    0.5   637.5M        -        -kubepod…07c41430.slice       -    0.3   630.1M        -        -Widzimy tutaj poszczególne cgroupy, które między innymi odpowiadają poszczególnym kontenerom lub podom, wraz z rzeczywiście zajętą przez nie pamięcią. W zależności od rozwiązania (czy są to np. gołe dockerowe kontenery, czy kubernetes) identyfikator cgroupy może - ale nie musi - odpowiadać identyfikatorowi kontenera. W przypadku gołego dockera będzie to ten sam identyfikator. Generalnie zachęcam, żeby teraz sobie każdy lokalnie odpalił byle jaki kontener dockerowy i podejrzał go w systemd-cgtop.Natomiast w przypadku kubernetesa to nie będzie ten sam identyfikator - przede wszystkim z tego powodu, że na podzie może chodzić więcej niż jeden kontener, a cgroupa jest tworzona dla całego poda, a nie każdego kontenera z osobna (choć to też, ale o tym za chwilę). Najprościej ten identyfikator poda, odpowiadający identyfikatorowi cgroupy, pobrać z yamla, np.:oc get pod myapp-289-prt6s -o yaml | grep uid(ewentualnie kubectl zamiast oc) Z tak pobranym identyfikatorem możemy w systemd-cgtop namierzyć realne zużycie pamięci poda, np:systemd-cgtop | grep ed9802ccI tutaj jak na dłoni będzie widać pierwszy rozjazd, pomiędzy tym, co pokazuje np. monitoring OpenShifta, a realnym zużyciem pamięci poda z punktu widzenia cgroupsów. A jako, że systemowy OOM killer patrzy dokładnie na tę samą wartość, którą zgłaszają cgroupy, to jest to pierwsze miejsce, gdzie w końcu można podejrzeć, ile tak naprawdę pamięci, z punktu widzenia systemu operacyjnego, zajmuje pod.Skąd zatem bierze się ten rozjazd, gdzie uciekają nam te dodatkowe megabajty powodujące OOM? Stoją za tym co najmniej dwie rzeczy.Cache dyskowycgroupy do zużycia pamięci liczą też cache dyskowy. Innymi słowy, jeśli proces uruchomiony w danym kontenerze wykonuje dużo zapisów/odczytów z dysku, to cgroupy będą liczyły rozmiar cache’a dyskowego tego procesu do ogólnego zużycia pamięci przez kontener. To jest szczególnie niebezpieczne w przypadku odpalania baz danych w kontenerach (pomijam tutaj całą ważną dyskusję nt. tego, czy odpalanie baz w kontenerach to dobry czy zły pomysł ;)). Może dojść do sytuacji, w której realnie zużyta pamięć przez kontener będzie kilkukrotnie wyższa niż ta, którą widać np. w docker stats. Natomiast w przypadku “zwykłych” kontenerów, gdzie mamy odpalonego, powiedzmy, spring boota, na ogół nie stanowi to większego problemu lub są to pomijalne wartości.Dodatkowe kontenery w ramach pojedynczej cgroupycgroupy są w rzeczywistości strukturą drzewiastą, tj. w ramach istniejących cgroup mogą istnieć kolejne, zagnieżdżone cgroupy. Tę strukturę można obejrzeć poleceniem systemd-cgls. Szczególnie ma to znaczenie w przypadku limitu zasobów. Aktualne zużycie np. pamięci przez cgroupę jest równe sumie zużyć potomnych cgroup. Wyszukując w wyjściu systemd-cgls naszego poda, prawdopodobnie traficie na sytuację, że w ramach cgroupy odpowiadającej waszemu podowi będzie istniało jeszcze kilka innych cgroup.Mogliście słyszeć, że Kubernetes porzucił Dockera jako runtime kontenerów (w skrócie, wciąż można używać obrazów zbudowanych dockerem na kubernetesie/openshifcie, ale do samego uruchamiania kontenerów wykorzystywany będzie już np. CRI-O). Jeśli chodzi o CRI-O (z którego korzystamy w naszym projekcie), razem z podem tworzonych jest jeszcze kilka dodatkowych kontenerów, które zajmują się przede wszystkim monitorowaniem “głównych” kontenerów należących do poda. Wszystkie te kontenery - te zdefiniowane przez nas w podzie oraz te dołożone przez CRI-O - żyją w ramach tej samej cgroupy.Dlaczego to jest problem?Po pierwsze, te dodatkowe kontenery też zajmą zasoby, a w szczególności pamięć! I to właśnie stąd bierze się rozjazd pomiędzy tym, co pokazuje np. monitoring OpenShifta w kontekście zużycia pamięci przez poda, a tym, co realnie pokazuje cgtop. OpenShift pokazuje zużycie pamięci jedynie “naszych” kontenerów, zupełnie pomijając kontenery CRI-O.Jak sprawdzić, ile zasobów wciągają bonusowe kontenery CRI-O? Jako że wiemy już, że cgroupy są strukturą drzewiastą, to pora wrócić do systemd-cgtop. Domyślnie pokazuje on cgroupy do trzeciego poziomu zagnieżdżenia. Wykonując systemd-cgtop –depth=5, zobaczymy też nasze bonusowe kontenery wraz z ich konsumpcją zasobów. Empirycznie doszliśmy do tego, że na ogół zajmują one kilkadziesiąt MB (choć potrafią spuchnąć nawet do około 100MB). Naturalnie pojawia się pytanie, czy dla tych kontenerów istnieją jakieś limity? Otóż… nie. Jak w ogóle sprawdzić limit pamięci dla cgroupy? Słyszeliście o tym, że na linuksach wszystko jest plikiem? No to trzeba namierzyć odpowiedni plik. Informacje o cgroupach zapisane są w /sys/fs/cgroup/. Struktura plików odpowiada drzewiastej strukturze cgroup. Chcąc poznać limit pamięci dla cgroupy/kontenera (nieważne, czy “naszego”, czy bonusowego od CRI-O), wystarczy puścić:cat /sys/fs/cgroup/memory/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poded9802cc_105e_4114_8e97_0d580d361e15.slice/crio-cd168e1ea893458a8f414fe4742d9d266087b3366f4b0f8c0ce9e00810be651d.scope/memory.limit_in_bytesgdzie poszczególne identyfikatory katalogów odpowiadają identyfikatorom cgroup na kolejnych poziomach zagnieżdżenia.Sprawdźmy zatem, jak wygląda ten limit dla jednego z kontenerów CRI-O (w bajtach):9223372036854771712Trochę sporo. Innymi słowy, kontenery CRI-O nie mają ustawionego limitu pamięci!I tu dochodzimy do sedna. Otóż limit zasobów cgroupy dla poda jest liczony jako suma limitów bezpośrednio zdefiniowanych kontenerów. Przykładowo, mam zdefiniowane dwa kontenery w podzie, każdy ma limit 256 MB pamięci. Sumarycznie limit pamięci dla poda (a więc i dla całej cgroupy) będzie wynosił 512MB. Nigdzie w tym limicie nie są uwzględnione dodatkowe kontenery CRI-O. I to właśnie z tego powodu systemowy OOM killer potrafi uwalić kontener, mimo że wcale nie doszedł do zdefiniowanego na nim limitu pamięci. Po prostu sumaryczne zużycie pamięci przez cgroupę poda (a więc pamięć zużyta przez nasz kontener/y oraz dodatkowa pamięć zjadana przez kontenery CRI-O) przekracza zdefiniowany na niej limit, który był policzony wyłącznie na bazie naszych kontenerów. W tym momencie system operacyjny odpala OOM killera, który ubije proces zabierający najwięcej pamięci. Praktycznie zawsze będzie to nasz kontener. Dla przykładu, mamy poda, którego limit pamięci to 640MB. Nasz kontener zajmuje 620MB, i w tym momencie odpala się OOM killer, ponieważ pamięć zajęta przez nasz kontener + kontenery CRI-O przekroczyła 640MB, a nasz kontener był pierwszy do odstrzału.Reasumując, aby poznać faktyczne zużycie pamięci przez poda należy zajrzeć do sysemd-cgtop -–depth=5. W ten sposób dowiemy się nie tylko, ile w rzeczywistości zajmują nasze kontenery, ale też ile w bonusie dorzuciło od siebie CRI-O. I to tłumaczy pierwszy z rozjazdów wymienionych na samej górze - cache dyskowy oraz dodatkowe kontenery CRI-O nie są nigdzie uwzględnione i praktycznie żadne monitoringi ich nie wychwytują, a potrafią rozdmuchać pamięć kontenerów oraz poda.Pora na drugi rozjazd, związany już wyłącznie z Javą.Jak policzyć pamięć zużytą przez Javę?Wszyscy słyszeli o heapie, ale potencjalnie nie wszyscy wiedzą o off-heapie. W największym uproszczeniu: heap to obszar pamięci, gdzie trafiają tworzone obiekty (te tworzone przez nas przy użyciu new, np. new ImportantService(), ale też te tworzone zakulisowo przez framework czy kontener aplikacji) i obszar ten podlega czyszczeniu przez GC. Z kolei off-heap to miejsce, gdzie trafiają rzeczy, którymi zarządza JVM, a nie my - skompilowane klasy, stack wywołań wątku i tym podobne. Off heap nie łapie się na GC. Istnieją też co prawda sposoby, aby z poziomu kodu władować nasze własne obiekty do off-heapa, ale w kontekście tego artykułu niewiele to zmienia.Posumowany rozmiar tych dwóch obszarów powinien nam powiedzieć, ile pamięci sumarycznie zajmuje nasz proces Javy. Jak zatem to sprawdzić? Metod jest kilka, ale polecamy jedną: native memory tracking. Jest to narzędzie, które pozwala na zrzucenie dosyć niskopoziomowego raportu z pamięci HotSpota. Żeby stało się dostępne, trzeba dorzucić dodatkowy parametr podczas uruchamiania: -XX:NativeMemoryTracking=detail. Tylko uwaga - literatura twierdzi, że włączenie native memory trackingu może mieć około dziesięcioprocentowy narzut na wydajność.Kiedy mamy już włączony NMT, pora na wykonanie zrzutu. Robi się to tak:jcmd &amp;lt;PID&amp;gt; VM.native_memoryPrzykładowy zrzut z jednej aplikacji naszego projektu wygląda tak:Native Memory Tracking:Total: reserved=1943037KB, committed=610501KB-                 Java Heap (reserved=327680KB, committed=265820KB)                            (mmap: reserved=327680KB, committed=265820KB)-                     Class (reserved=1196453KB, committed=167509KB)                            (classes #28683)                            (  instance classes #27130, array classes #1553)                            (malloc=6565KB #106806)                            (mmap: reserved=1189888KB, committed=160944KB)                            (  Metadata:   )                            (    reserved=141312KB, committed=140720KB)                            (    used=135131KB)                            (    free=5589KB)                            (    waste=0KB =0.00%)                            (  Class space:)                            (    reserved=1048576KB, committed=20224KB)                            (    used=17072KB)                            (    free=3152KB)                            (    waste=0KB =0.00%)-                    Thread (reserved=102239KB, committed=12863KB)                            (thread #99)                            (stack: reserved=101764KB, committed=12388KB)                            (malloc=361KB #596)                            (arena=114KB #196)-                      Code (reserved=255775KB, committed=103615KB)                            (malloc=8087KB #29935)                            (mmap: reserved=247688KB, committed=95528KB)-                        GC (reserved=2104KB, committed=1908KB)                            (malloc=1032KB #4341)                            (mmap: reserved=1072KB, committed=876KB)-                  Compiler (reserved=1046KB, committed=1046KB)                            (malloc=914KB #2701)                            (arena=133KB #5)-                  Internal (reserved=1198KB, committed=1198KB)                            (malloc=1166KB #2425)                            (mmap: reserved=32KB, committed=32KB)-                     Other (reserved=302KB, committed=302KB)                            (malloc=302KB #39)-                    Symbol (reserved=28055KB, committed=28055KB)                            (malloc=25969KB #368221)                            (arena=2086KB #1)-    Native Memory Tracking (reserved=8867KB, committed=8867KB)     (malloc=509KB #7225)     (tracking overhead=8358KB)-        Shared class space (reserved=17148KB, committed=17148KB)                            (mmap: reserved=17148KB, committed=17148KB)-               Arena Chunk (reserved=179KB, committed=179KB)                            (malloc=179KB)-                   Logging (reserved=4KB, committed=4KB)                            (malloc=4KB #192)-                 Arguments (reserved=19KB, committed=19KB)                            (malloc=19KB #531)-                    Module (reserved=1365KB, committed=1365KB)                            (malloc=1365KB #6576)-              Synchronizer (reserved=594KB, committed=594KB)                            (malloc=594KB #5027)-                 Safepoint (reserved=8KB, committed=8KB)                            (mmap: reserved=8KB, committed=8KB)Najbardziej interesujące są tutaj wartości commited, pokazujące faktycznie używaną pamięć.Co możemy z tego zrzutu wyczytać? Przede wszystkim, że z punktu widzenia samego JVMa, proces zajmuje 610501KB, czyli około 596MB pamięci.Ciekawymi wartościami są też te dotyczące heapa oraz wszystkich pozostałych rzeczy składających się na off-heap. Warto zdawać sobie sprawę, że offheap bez problemu potrafi wciągnąć kilkaset MB i trzeba to wziąć pod uwagę w trakcie strojenia pamięci. Nie będziemy się tu jednak wgłębiać we wszystkie składowe off heapa.Spójrzmy natomiast na wyjście ps aux dotyczące tej samej aplikacji.USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND1000630+       1  0.8  2.0 2283920 721032 ?      Ssl  Nov22  12:27 Java -Dspring.config.location=classpath:/applic....I tutaj widzimy drugi rozjazd - według NMT proces bierze niecałe 600MB RAMu, podczas gdy według systemu operacyjnego jest to około 700MB. Gdzie podziało się 100MB?Musimy zejść na poziom niżej niż JVM, żeby pokazać, gdzie leży problem. Alokacją pamięci na linuksie domyślnie zajmuje się malloc() pochodzący z glibc (a tak na serio, to malloc() w glibc wywołuje aktualnie podlinkowaną implementację alokatora, domyślnie ptmalloc2). Do glibc należy też zwalnianie do systemu nieużywanej pamięci. Zacznijmy od tego, jak alokowana jest pamięć przez glibc - proces ten generalnie należy do skomplikowanych i ma trochę niuansów, ale tutaj go uprościmy i skupimy się na ogólnej zasadzie działania.W aplikacji jednowątkowej sprawa jest prosta - ponieważ nie może dojść do sytuacji, w której dwa wątki naraz poproszą o pamięć, to może istnieć jeden worek pamięci, z którego pobierana jest pamięć. Sytuacja się komplikuje w aplikacjach wielowątkowych, czyli takich, z którymi przede wszystkim mamy do czynienia. Kilka wątków naraz może poprosić o pamięć, w związku z czym jeden worek z pamięcią staje się niewystarczający, ponieważ na moment alokowania pamięci trzeba założyć locka na worek, aby dwa wątki nie próbowały zaalokować tego samego obszaru pamięci. Ale kiedy pojawiają się locki, to istnieje też ryzyko czekania i zagłodzenia wątku, który czeka na pamięć, co może się mocno odbić wydajnościowo na aplikacji. Z tego też tytułu, dla aplikacji wielowątkowych tworzonych jest więcej worków, tak zwanych Aren (rzeczywistość jest tu bardziej zawiła, Areny składają się z Heapów, które z kolei są podzielone na Chunki, ale dla uproszczenia będziemy traktować Areny jako worki ciągłej pamięci). Kiedy wątek prosi o pamięć, to najpierw sprawdzana jest ostatnio użyta przez ten wątek Arena, a jeśli ona okaże się być zablokowana przez inny wątek, to odnajdywana jest pierwsza niezablokowana Arena, i z niej alokowana jest pamięć. Jeśli natomiast nie uda się znaleźć niezablokowanej Areny, to tworzona jest nowa. Dzięki temu, że istnieje więcej niż jeden worek (aka Arena), ryzyko czekania na locka jest zdecydowanie niższe, co pozytywnie wpływa na wydajność alokowania pamięci, a tym samym wydajność aplikacji ogólnie.Póki co brzmi to rozsądnie.Problem pojawia się w momencie zwalniania pamięci. W dużym uproszczeniu, areny działają trochę jak stos - zalokowane kawałki pamięci umieszczane są na górze tego stosu. Wyobraźmy sobie więc, że trzy wątki alokują trzy kawałki pamięci - nazwijmy je A, B i C - w tej właśnie kolejności. Najpierw A, potem B, na końcu C. Na górze stosu będzie leżało C. Dwa pierwsze wątki zwalniają pamięć, a więc kawałki A oraz B. Czy pamięć zajęta przez kawałki A oraz B zostaje oddana do systemu operacyjnego? Nie. Pomimo tego, że aplikacja przestała już używać tej pamięci, to z punktu widzenia systemu ona wciąż należy do aplikacji. Dzieje się tak, ponieważ domyślna implementacja glibc oddaje do systemu operacyjnego pamięć wyłącznie z samej góry stosu! Innymi słowy, kawałki pamięci A oraz B zostaną zwrócone systemowi operacyjnemu dopiero w momencie, kiedy aplikacja zwolni kawałek C - dopiero wtedy na górze stosu znajdzie się nieużywana pamięć, a glibc odda całą nieużywaną pamięć z góry stosu do systemu. Taki model zarządzania pamięcią powoduje, że z łatwością dochodzi do sytuacji, kiedy w głębi stosu znajduje się sporo nieużywanek pamięci, której nie da się oddać do systemu operacyjnego, ponieważ jakiś wątek aplikacji wciąż używa pamięci znajdującej się “wyżej” na stosie. Co prawda glibc podejmuje próby defragmentacji Aren, i w pewien sposób mityguje to problem, to jednak go nie rozwiązuje - przeciętnie obciążona aplikacja zawsze będzie chomikowała pewną ilość pamięci, z której w żaden sposób już nie korzysta. W skrócie, ten algorytm jest nastawiony na poprawę wydajności działania aplikacji, kosztem zwiększonego zużycia pamięci.I to właśnie tutaj uciekło nam 100MB pamięci z powyższego przykładu - jest to pamięć nieużywana już przez aplikację, ale wciąż niezwrócona do systemu.Jak można temu zaradzić?Pierwsza opcja to w ogóle tego nie ruszać - tego rodzaju nadwyżka w wielu aplikacjach może być po prostu akceptowalna. Natomiast im większa aplikacja, im więcej wątków, tym ten rozjazd będzie większy. W trakcie prowadzenia tego śledztwa widzieliśmy w internecie przypadki, kiedy ta niezwrócona do systemu pamięć szła w gigabajty.Druga opcja to wymiana domyślnego alokatora pamięci. Trochę tego jest do wyboru, np. jemalloc albo tcmalloc. Każdy alokator ma własne algorytmy związane z alokowaniem i zwalnianiem pamięci, i oczywiście każdy twierdzi, że ich algorytmy są najlepsze. ;) Udało nam się przetestować aplikację z jemalloc i wyniki były obiecujące - rozmiar niezwróconej pamięci do systemu spadł mniej więcej o połowę, do około 50MB. Natomiast wymiana domyślnych alokatorów może być nieco karkołomną opcją, albo wręcz nie wszędzie możliwą.Trzecią opcją jest zatem strojenie domyślnego malloca. Najważniejszą opcją, która pozwala na sterowanie zachowaniem malloca pod kątem zwalniania nieużywanej pamięci, jest maksymalna liczba Aren dostępnych dla procesu. Domyślnie maksymalna liczba Aren wynosi 8 * liczba core’ów. Czyli dla 8 core’ów procesy będą miały do dyspozycji 64 Areny - sporo. Im więcej aren, tym większa szansa na chomikowanie pamięci. Z drugiej strony, im mniej aren tym większa szansa na zagłodzenie wątku czekającego na pamięć, co odbije się na ogólnej wydajności aplikacji. Ustawiając zmienną środowiskową MALLOC_ARENA_MAX, możemy kontrolować maksymalną liczbę aren. W przypadku Javy, wiele artykułów w internecie poleca ścięcie tego parametru do 2 lub 4. Wykonaliśmy próbę, ścinając tę wartość do 2. Aplikacja, która wcześniej chomikowała ponad 100MB pamięci, zaczęła chomikować jedynie kilkanaście MB. Natomiast nie istnieje tutaj ścisła matematyka - ograniczenie liczby aren zmniejsza rozmiar chomikowanej pamięci, jednak nie da się uniwersalnie przeliczyć tego na konkretnie zaoszczędzone megabajty - będzie to indywidualne dla każdej aplikacji. Manewrowanie tym parametrem powinno też zakończyć się testami wydajnościowymi, które potwierdzą, że rozwiązując problem pamięci, nie wywołaliśmy problemu z wydajnością.Mimo tego ewidentnego tradeoffu, jest to dobra informacja - dzięki temu udało się nam finalnie doliczyć całej pamięci oraz wyjaśnić drugi rozjazd z samego początku tego artykułu. A to z kolei pozwala świadomie dobierać takie parametry jak maksymalny rozmiar heapa czy limit pamięci dla kontenera.Rozkład zużycia pamięciPoniższy diagram pokazuje, co faktycznie składa się na zużycie pamięci dla przeciętnej javowej aplikacji odpalonej na kubernetesie. Schodząc wgłąb diagramu można też wyczytać, jak wygląda rozkład pamięci w przypadku uruchomienia aplikacji javowej na “zwykłych” kontenerach (ramka “Application container”) oraz w przypadku uruchomienia aplikacji zupełnie z pominięciem kontenerów (ramka “JVM”).Elementy narysowane przerywaną linią są elementami raczej logicznymi i same w sobie nie konsumują ramu (ewentualnie ich narzut jest pomijalny). Elementy z ciągłym obramowaniem konsumują fizyczną pamięć i należy ją wziąć pod uwagę podczas strojenia pamięci.Jak dobrać parametry pamięci?W świecie kontenerów sprowadza się to w praktyce do wyboru dwóch wartości - limitu pamięci dla kontenera i maksymalnego rozmiaru heapa. I to właśnie sumaryczny rozmiar heapa oraz offheapa jest tutaj najważniejszą metryką, która powinna determinować limit kontenera, nigdy na odwrót.Oczywiście rozmiar heapa i offheapa pozostaje kwestią specyficzną dla każdej aplikacji. Niewielkie aplikacje spring bootowe, które nie wykorzystują za bardzo zewnętrznych libów (poza spring bootem oczywiście ;)), np. będące jedynie prostą przelotką do innych serwisów, spokojnie poradzą sobie z heapem ściętym do 32MB. Natomiast nawet tak proste aplikacje na dzień dobry mogą wciągnąć około 100MB off heapa, głównie na klasy i wątki. Im większa aplikacja, im więcej zależności, tym bardziej obie te wartości będą rosnąć. W przypadku spring bootowych aplikacji naszego projektu standardem jest off heap wahający się pomiędzy 300 a 400MB. Trzeba też brać poprawkę na to, że rozmiar offheapa nie będzie stały, a w szczególności może rosnąć z czasem, wraz z wątkami czy kompilowaniem do kodu maszynowego kolejnych partii bytecode’u. Wydaje się, że ciężko poznać optymalny rozmiar heapa oraz docelowy rozmiar offheapa inaczej niż empirycznie. Można się oczywiście podpierać doświadczeniem, ale ostatecznie każda aplikacja jest inna, a wciągnięcie choćby jednej dodatkowej zależności może w skrajnym przypadku kompletnie zmienić obraz tych wartości.Wiedząc, ile pamięci potrzebujemy na heapa oraz offheapa, pozostała matematyka sprowadza się do podliczenia narzutów. Jeśli mówimy o prostych aplikacjach bezstanowych lub kiedy aplikacja nie chodzi w kontenerze, możemy właściwie zignorować kwestię cache’a dyskowego. Pozostaje zatem kwestia pamięci kiszonej przez glibc oraz w przypadku kubernetesa, kontenerów CRI-O. W przypadku CRI-O wydaje się, że warto zostawić co najmniej kilkadziesiąt MB zapasu, w porywach do 100. W przypadku pamięci kiszonej przez glibc, dla aplikacji, które zużywają średnio w okolicy 300MB heapa, konfiguracyjnie (zmienną środowiskową MALLOC_ARENA_MAX) da się zejść znacznie poniżej 100MB kiszonej pamięci, do około kilkunastu-kilkudziesięciu MB. Jednak dla większych aplikacji, utylizujących więcej pamięci, ta zakiszona pamięć będzie odpowiednio większa. Po doliczeniu narzutów otrzymamy limit pamięci dla kontenera.Można się więc pokusić o taki wzór:limit kontenera = heap + offheap + glibc + disk cache (dla kontenerów) + crio (dla kubernetesa)Sporą karierę ostatnimi czasy zrobiło procentowe ustawianie rozmiaru heapa, w zależności od rozmiaru pamięci dostępnej dla kontenera, przy pomocy flagi XX:MaxRAMPercentage. Sami zresztą użyliśmy jej w naszym projekcie. Motywacja jest oczywista - znacznie wygodniej zarządzać jedną wartością niż dwiema. Uzbrojeni jednak w powyższą wiedzę twierdzimy, że w większości przypadków jest zbyt duże uproszczenie, ponieważ zmieniając limit kontenera, wpływamy w ten sposób na właściwie wszystkie obszary pamięci, które warto wziąć pod uwagę - heap, offheap, glibc, crio-o. Co więcej, chcąc zwiększyć dostępną pamięć, np. na “wyciek” glibc, musimy też zmienić procentowy udział heapa w pamięci kontenera, żeby przy okazji nie rozszerzyć heapa, co w praktyce i tak oznacza przeliczanie od nowa procentowych wartości heapa. Przy okazji uniezależniając rozmiar heapa od rozmiaru kontenera, prościej analizuje się problemy związane generalnie z OOM - czy to systemowym, czy javowym, dzięki temu, że wszystkie obszary pamięci naraz nie zmieniają rozmiaru. Ostatecznie więc znalezienie uniwersalnej wartości XX:MaxRAMPercentage wydaje się na tyle trudne, że przychylamy się do staroświeckiego Xmx. No chyba, że mamy tyle RAMu, że każdy kontener może mieć sporą górkę pamięci - ale wtedy cały ten artykuł mija się z celem. ;)",
"url": "/2022/12/15/memory-java.html",
"author": "Marcin Mergo",
"authorUrl": "/authors/mmergo.html",
"image": "mmergo.webp",
"highlight": "/assets/img/posts/2022-12-01-memory-java/memory.jpg",
"date": "15-12-2022",
"path": "pl/2022-12-01-memory-java"
}
,


"2022-11-07-jaxb-html": {
"title": "Czy wiesz jak poprawnie tworzyć kontekst JAXB i instancję Marshallera/Unmarshallera?",
"lang": "pl",
"tags": "jaxb java marshaller unmarshaller",
"content": "W naszych projektach często używamy JAXB w celu mapowania danych przesyłanych kopertach SOAP-owych na obiekty javowe. Aby wykonać takie mapowanie należy:Utworzyć kontekst JAXBJAXBContext jaxbContext = JAXBContext.newInstance(User.class);Utworzyć marshaller/unmarshaller (w zależności do tego czy mapujemy obiekt javy na xml, czy w drugą stronę):Marshaller marshaller = jaxbContext.createMarshaller();// lubUnmarshaller unmarshaller = jaxbContext.createUnmarshaller();Wykonać mapowanie:Writer writer = new StringWriter();marshaller.marshal(user, writer);// lubJAXBElement&amp;lt;User&amp;gt; userElement = unmarshaller.unmarshal(source(xml), User.class);Ważne!Tworzenie kontekstu JAXB jest operacją czasochłonną, a doświadczenie pokazuje, że czas wywołania znacznie wzrasta wraz z liczbą wątków, które próbują wykonać współbieżnie tę operację. Dlatego zgodnie z dokumentacją, nie należy tworzyć nowej instancji kontekstu przy każdym wywołaniu mapowania. Kontekst JAXB jest bezpieczny przy współbieżnym wywołaniu. Należy go tworzyć np. w konstruktorze, metodzie inicjalizującej, jako statyczne pole klasy: To avoid the overhead involved in creating a JAXBContext instance, a JAXB application is encouraged to reuse a JAXBContext instance. An implementation of abstract class JAXBContext is required to be thread-safe, thus, multiple threads in an application can share the same JAXBContext instance.Bardzo ważne!W przeciwieństwie do kontekstu JAXB sam marshaller/unmarshaller nie jest bezpieczny przy współbieżnym wywołaniu! Operację tworzenia marshallera i unmarshallera trzeba powtarzać przy każdym wywołaniu. Posługiwanie się tym samym obiektem może prowadzić do nieprzewidzianych rezultatów przy większej niż jeden liczbie wątków. Co więcej, nie wyłapiemy tego błędu w testach jednostkowych i testach funkcjonalnych.Jeżeli podczas testów wydajnościowych okaże się, że operacje tworzenia marshallera/unmarshallera zabierają zbyt dużo czasu można rozważyć utworzenie puli takich obiektów. Musi być ona zaimplementowana w taki sposób, aby różne wątki nie używały tego samego obiektu w tym samym czasie.",
"url": "/2022/11/07/jaxb.html",
"author": "Jakub Wilczewski",
"authorUrl": "/authors/jwilczewski.html",
"image": "jwilczewski.jpg",
"highlight": "/assets/img/posts/2022-11-08-jaxb/jaxb.jpg",
"date": "07-11-2022",
"path": "pl/2022-11-08-jaxb"
}
,


"2022-10-14-k8s-raspberry-pi-html": {
"title": "Domowy klaster Kubernetesa na Raspberry Pi",
"lang": "pl",
"tags": "k8s kubernetes devops raspberry-pi",
"content": "W tym artykule pokażę jak postawić klaster kubernetesa na popularnych płytkach Raspberry Pi. Czas post pandemicznego kryzysu półprzewodników, niedoboru płytek oraz ich horrendalnych cen na serwisach aukcyjnych, to idealny moment na taki poradnik ;)Przygotowanie płytekPotrzebne będą co najmniej dwie płytki Raspberry Pi 4+. W ostateczności worker node można postawić na wersji 3B. Niższewersje płytek są na architekturze 32-bit. O ile sam klaster można na niej postawić, o tyle aplikacji praktycznie nie ma. Jako bazę wykorzystamy system Raspberry Pi OS w wersji 64-bit. Można użyćoczywiście dowolnego systemu, byle był 64-bit.Ze strony raspberrypi.com pobieramywersję lite systemu, rozpakowujemy archiwum i kopiujemy na kartę microSd (klasy 10).Zakładając, że karta jest reprezentowana przez urządzenie /dev/sdb:dd if=wypakowanyObraz.img of=/dev/sdb bs=4M conv=fsyncBardzo ważne, aby karty sd były w dobrym stanie (idealnie, gdyby były nowe). Najistotniejsza jest karta użyta na maszynie master. Po dłuższym czasie użytkowania (około 2 lat) karta, którą miałem na głównej maszynie uległa degradacji. W efekcie w czasie inicjalizacji klastra otrzymywałem timeouty na requestach do etcd. Pomogła wymiana karty na nową.Po uruchomieniu płytki przechodzimy przez wstępną konfigurację (zależnie od wersji systemu pojawi się konfigurator lub od razu zostaniem zalogowaniu na użytkownika ‘pi’).Na początku przystępujemy do konfiguracji ssh. Otwieramy plik /etc/ssh/sshd_config i ustawiamy opcję PermitRootLoginna yes. Następnie przechodzimy na konto root za pomocą sudo su i ustawiamy hasło wykorzystując passwd. Możemytakże nie ustawiać hasła i pozostawić domyślna wartość prohibit-password. W takim przypadku należy skopiować na płytkę klucz publiczny.Na koniec włączamy usługę ssh:systemctl start ssh &amp;amp;&amp;amp; systemctl enable sshAlternatywnie można wykorzystać narzędzie raspi-config i włączyć ssh przechodząc do Interface Options &amp;gt; SSH.Jeśli planujemy bezprzewodowe połączenie płytek, możemy skorzystać z tego narzędzia do konfiguracji połączenia —sekcja: System OptionsNiezależnie od wybranego interfejsu połączenia musimy zadbać o to, aby adresy IP maszyn były stałe. W tym celu otwieramyplik /etc/dhcpcd.conf usuwamy całą zawartość i podajemy:interface eth0static ip_address=192.168.1.203/24static routers=192.168.1.1static domain_name_servers=192.168.1.1 8.8.8.8 8.8.4.4Oczywiście adresy i interfejs dostosowujemy do własnych wymagań. Dla połączenia bezprzewodowego domyślnym interfejsemjest wlan0. Osobiście cały klaster trzymam w podsieci, aby uniknąć konfliktów z innymi urządzeniami. Wykorzystujędedykowaną płytkę jako bramę do klastra.Pozostało zrestartować usługę dhcpcd:systemctl restart dhcpcdNa koniec weryfikujemy, czy mamy połączenie pomiędzy wszystkimi płytkami i każda z nich ma dostęp do internetu.Po konfiguracji należy upewnić się, że w /etc/resolv.conf wśród serwerów dns nie mamy 127.0.0.1. Pozostawienie tego wpisu spowoduje błędy poda CoreDNS. Posiada on mechanizmy wykrywania pętli.Konfiguracja węzłówPoniższe instrukcje należy wykonać dla każdej maszyny.Konfiguracja systemuRozpoczynamy od instalacji potrzebnych pakietów:apt-get update &amp;amp;&amp;amp; apt-get installlsb-release \\apt-transport-https \\ca-certificates \\software-properties-common \\arptables \\ebtables \\libseccomp2 \\jqNastępnie musimy zapewnić dostępność modułów jądra overlay i br_netfilter. W tym celu tworzymyplik /etc/modules-load.d/k8s.conf:overlaybr_netfilterW ten sposób po każdym restarcie moduły będą wczytane automatycznie.Aby możliwa była komunikacja między podami musimy jeszcze stworzyć plik /etc/sysctl.d/k8s.conf:net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward=1Moduł overlay pozwala na wykorzystanie systemu plików overlay, dzięki czemu kontenery będą miały “swoją” wersję systemu plików (więcej o overlayFs tutaj. Moduł br_netfilter wraz z ustawieniami z /etc/sysctl.d/k8s.conf pozwoli na komunikację między podami. Ruch odbywający się między kontenerami w ramach emulowanego mostu (bridge network) będzie podlegał regułom zawartym w iptables węzła (za konstrukcję reguł odpowiada kube-proxy). net.ipv4.ip_forward pozwala na przekierowanie pakietów z jednego interfejsu sieciowego na inny (system działa ja router).Na koniec dodajemy do /boot/cmdline.txt: cgroup_enable=cpuset cgroup_enable=memory swapaccount=1cpuset oraz memory w kontekście k8s pozwala na ograniczenie zasobów poszczególnym podom. Więcej o control groups tutaj. Włączenie swapaccount powoduje śledzenie i ograniczenie wykorzystania pamięci swap w ramach kontenerów. Istotne jest to z tego względu, że k8s nie wspiera obecnie pamięci swap (wsparcie alpha od wersji 1.22). Dlatego też na każdym nodzie musimy wyłączyć pamięć swap za pomocą komendy.systemctl disable dphys-swapfile.serviceJeśli swap nie będzie wyłączony, inicjalizacja klastra zakończy się błędem.Instalacja containerdOd wersji 1.20 kubernetes przestał wspierać Dockera jako domyślne środowisko uruchamiania dla kontenerów. Docker nie implementuje standardu CRI. Był wspierany przez kuberenetesa, gdyż nie było wystarczająco dobrych/popularnychalternatyw (no i na początku standard CRI nawet nie istniał). Obecnie sytuacja jest nieco inna. W obecnym kształcie docker jest tylko pośrednikiem między k8s, acontainerd. Dodatkowo wymaga to utrzymywania dodatkowej warstwy implementującej CRI - docker-shim (został usunięty w wersji 1.24 więcej informacji tutaj).Na naszym klastrze zainstalujemy containerd.Binarki containerd pobieramy z oficjalnego repozytorium i umieszczamy ja w katalogu /usr/local.https://github.com/containerd/containerd/releases/download/vWERSJA/containerd-WERSJA-linux-arm64.tar.gzW moim przypadku WERSJA=1.6.6.Następnie tworzymy katalog /etc/containerd i uzupełniamy go domyślną konfiguracją za pomocą:containerd config default &amp;gt; /etc/containerd/config.tomlW wygenerowanym pliku odnajdujemy SystemdCgroup = false i zmieniamy na SystemdCgroup = true.Następnie tworzymy katalog /usr/local/lib/systemd/system/ i zapisujemy w nim definicję usługi dostępnąnahttps://raw.githubusercontent.com/containerd/containerd/main/containerd.serviceContainerd zarządza obrazami, siecią i storagem.W kolejnym kroku instalujemy program runc.https://github.com/opencontainers/runc/releases/download/vWERSJA_RUNC/runc.arm64W moim przypadku WERSJA_RUNC=1.1.3 i instalujemy ją pod ścieżką /usr/local/sbin/runc z prawami dostępu 755.Runc to narzędzie najniższego poziomu, odpowiada za tworzenie i uruchamianie kontenerów.W repozytorium containerd znaleźć można także archiwa z pluginami CNI. Nie musimy ich specjalnie instalować. W dalszych krokach będziemy aplikować do k8s manifest konkretnego pluginu. Po zaaplikowaniu, binarki zainstalowane zostaną automatycznie za pomocą initConteinera (przynajmniej jeśli chodzi o pluginy, które testowałem).Pozostaje nam już tylko przeładować usługi i włączyć usługę containerd:systemctl daemon-reload &amp;amp;&amp;amp; systemctl enable --now containerdPo wszystkim restartujemy system.Instalacja kubeadm, kubectl i kubeletDodajemy klucz repozytorium k8s:curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -Tworzymy pliki /etc/apt/sources.list.d/k8s.list:deb http://apt.kubernetes.io/ kubernetes-xenial mainI instalujemy aplikacje:apt-get update &amp;amp;&amp;amp; apt-get install kubeadm kubectl kubeletPo pomyślnej instalacji uruchamiamy usługę kubelet:systemctl restart kubeletKonfiguracja masteraMając już przygotowaną bazę dla wszystkich węzłów, możemy przejść do inicjalizacji mastera. Wykonujemy polecenie:kubeadm init Po poprawnej inicjalizacji zwrócone zostanie polecenie wraz z tokenem, pozwalające na przyłączenie workerów doklastra. Polecenie to można także wygenerować, uruchamiając na masterze:kubeadm token create --print-join-command 2&amp;gt;/dev/nullNa koniec pozostaje nam zainstalować jeden z dostępnych pluginów CNI, wymaganych do działania klastra k8s. Osobiście testowałem pluginy:  wave (manifest - w url manifestu musimy wskazać konkretną wersję pozyskaną za pomocą $(kubectl version | base64 | tr -d &#39;\\n&#39;))  flannel (manifest)  calico (manifest)W przypadku pluginu flannel musimy do komendy iniciującej klaster dodać parametr:kubeadm init --pod-network-cidr=10.244.0.0/16Jest to domyślna pula adresów rezerwowana dla podów w pluginie flannel. Z jakiegoś powodu nie jest on w stanie wykryć puli zaalokowanej przez k8s przy inicjalizacji.Co prawda jest to mało prawdopodobne w przypadku domowego klastra, ale należy pamiętać, że pule adresów wskazane w ramach parametrów pod-network-cidr (jeśli nie podamy k8s sam zaalokuje adresy) i service-cidr (dla tego parametru domyślną wartością jest 10.96.0.0/12) nie powinny być zajęte.Przed zaaplikowaniem manifestu przygotowujemy konfigurację dla kubectl. W tym celu kopiujemyplik /etc/kubernetes/kubelet.conf pod ścieżkę /root/.kube/configNiezależnie od tego, który plugin wybierzemy, aplikujemy manifest za pomocą:kubectl apply -f ścieżka/do/manifestuKonfiguracja workeraKonfiguracja workera sprowadza się do wywołania komendy wygenerowanej w czasie inicjalizacji klastra, np.:kubeadm join 192.168.1.101:6443 --token et57r8.rezqsvudyqfd0c3z --discovery-token-ca-cert-hash sha256:01e7e5a9a30b7a6d2b8a04465f50c5fe8c43f595b7e30cd325dfb494bd69b624Po dodaniu wszystkich workerów do klastra, na masterze wykonujemy komendę:kubect get nodes W odpowiedzi powinniśmy otrzymać:k8s-master-1   Ready    control-plane   165m   v1.24.2k8s-worker-3   Ready    &amp;lt;none&amp;gt;          159m   v1.24.2k8s-worker-4   Ready    &amp;lt;none&amp;gt;          159m   v1.24.2k8s-worker-5   Ready    &amp;lt;none&amp;gt;          159m   v1.24.2k8s-worker-6   Ready    &amp;lt;none&amp;gt;          159m   v1.24.2k8s-worker-7   Ready    &amp;lt;none&amp;gt;          159m   v1.24.2I to wszystko, jeśli chodzi o podstawowy klaster k8s. Pozostaje poczekać na start wszystkich podów. Możemy tozweryfikować za pomocą:kubectl get pods --all-namespaceskube-system            coredns-6d4b75cb6d-cztbh                    1/1     Running            0                167mkube-system            coredns-6d4b75cb6d-w7n5l                    1/1     Running            0                167mkube-system            etcd-k8s-master-1                           1/1     Running            7                167mkube-system            kube-apiserver-k8s-master-1                 1/1     Running            7                167mkube-system            kube-controller-manager-k8s-master-1        1/1     Running            4                167mkube-system            kube-flannel-ds-5b29d                       1/1     Running            0                162mkube-system            kube-flannel-ds-75sqv                       1/1     Running            0                162mkube-system            kube-flannel-ds-gvr52                       1/1     Running            0                162mkube-system            kube-flannel-ds-jhfnm                       1/1     Running            0                162mkube-system            kube-flannel-ds-n5pnn                       1/1     Running            0                162mkube-system            kube-flannel-ds-vjczl                       1/1     Running            0                167mkube-system            kube-proxy-6p8hf                            1/1     Running            0                162mkube-system            kube-proxy-6zmff                            1/1     Running            0                162mkube-system            kube-proxy-75p5b                            1/1     Running            0                162mkube-system            kube-proxy-7msjd                            1/1     Running            0                167mkube-system            kube-proxy-ph9hf                            1/1     Running            0                162mkube-system            kube-proxy-q7bxw                            1/1     Running            0                162mkube-system            kube-scheduler-k8s-master-1                 1/1     Running            7                167mW kolejnej części zaprezentuję jak skonfigurować dodatkowe elementy klastra (ingress, dns, loadbalancer itd.), tak aby wycisnąć z niego maksimum możliwości.Wszystkie kroki (poza etapem konfiguracji płytek) można znaleźć tutaj w formie playbooka Ansible.",
"url": "/2022/10/14/k8s-raspberry-pi.html",
"author": "Bartosz Radliński",
"authorUrl": "/authors/bradlinski.html",
"image": "bradlinski.webp",
"highlight": "/assets/img/posts/2022-10-03-k8s-raspberry-pi/cluster.jpg",
"date": "14-10-2022",
"path": "pl/2022-07-01-k8s-raspberry-pi"
}
,


"2022-10-11-healthcheck-html": {
"title": "Czy wiesz co to healthcheck i jak z niego korzystać?",
"lang": "pl",
"tags": "liveness probe heathcheck kubernetes",
"content": "Health check pozwala na monitorowanie działania aplikacji. Mechanizm ten w Kubernetesie pozwala sprawdzić nie tylko, czykontener działa (czy node port jest dostępny), ale także czy aplikacja wciąż działa (czy odpowiada). W tym wpisieporuszany jest temat mechanizmu Liveness Probe.Liveness ProbeSłuży do identyfikacji, czy aplikacja uruchomiona na kontenerze jest sprawna. W przypadku identyfikacji braku działaniaaplikacji kontener zostaje automatycznie zrestartowany. Detekcja działania aplikacji sprowadza się do cyklicznegowysłania zapytań HTTP na wcześniej zdefiniowany endpoint wystawiony w aplikacji lub wywoływania komendy w kontenerze.Fragment pliku .yaml z konfiguracją Liveness Probe’a opartą na zapytaniach HTTP:spec:  containers:    livenessProbe:      httpGet:        (...)        path: /health-check # ścieżka do health-checka        port: 8080 # port na którym wystawiony jest kontener      initialDelaySeconds: 3 # po ilu sekundach od uruchomienia kontenera włączyć health-checka       periodSeconds: 3 # co ile serwować zapytanie do /health-checkPrzykład interaktywny:Poniżej przedstawiono polecenia, dzięki którym można samodzielnie zobaczyć, jak działa Liveness Probe. Zamieszczonyprzykład zawiera definicję Poda, który po 30 sekundach od uruchomienia przestaje odpowiadać na health-checka. W tymprzypadku zdecydowano się na definicję health-checka przy użyciu wywołania komendy w kontenerze.Uruchomienie kontenera:kubectl apply -f https://k8s.io/examples/pods/probe/exec-liveness.yamlPrzy użyciu poniższego polecenia możliwe jest śledzenie stanu poda. Należy zauważyć, że po czasie 30 sekund oduruchomienia, pod zostaje zrestartowany, dlatego, że usunięty zostaje plik, który odpowiada za poprawne zwracanieodpowiedzi dla health-checka:watch -n 1 kubectl get podsAby prześledzić historię zdarzeń (restartu) wykonajmy polecenie:kubectl describe pod liveness-execJeśli nastąpił przynajmniej jeden restart, polecenie zwraca informację o przyczynie restartu:(...)Normal   Pulled     78s                kubelet            Successfully pulled image &quot;k8s.gcr.io/busybox&quot; in 990.617406msWarning  Unhealthy  35s (x3 over 45s)  kubelet            Liveness probe failed: cat: can&#39;t open &#39;/tmp/healthy&#39;: No such file or directoryNormal   Killing    35s                kubelet            Container liveness failed liveness probe, will be restartedNormal   Pulling    5s (x2 over 79s)   kubelet            Pulling image &quot;k8s.gcr.io/busybox&quot;(...)Źródła:  https://github.com/wardviaene/kubernetes-course  https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/  https://www.educba.com/kubernetes-liveness-probe",
"url": "/2022/10/11/healthcheck.html",
"author": "Krzysztof Czarnecki",
"authorUrl": "/authors/kczarnecki.html",
"image": "kczarnecki.jpg",
"highlight": "/assets/img/posts/2022-10-11-healthcheck/healthcheck.jpg",
"date": "11-10-2022",
"path": "pl/2022-10-11-healthcheck"
}
,


"2022-08-18-cloud-functions-2-html": {
"title": "Google Cloud Functions (2nd gen) - co nowego wprowadza?",
"lang": "pl",
"tags": "google cloud function cloud functions gen 2 google cloud platform gcp googlecloud serverless eventarc",
"content": "W lutym tego roku Google wprowadziło w wersji poglądowej (public preview) nową generację usługi Cloud Functions (2nd gen). Architektura Google Cloud Function (2nd gen) została oparta o Cloud Run oraz Eventarc, co wprowadza kilka ciekawych funkcjonalności względem poprzedniej generacji. W sierpniu tego roku druga generacja funkcji przeszła z wersji poglądowej do ogólnodostępnej (general availability, GA).Cloud Functions 2nd genźródło: https://cloud.google.com - dostęp: 2022-08-15Co nowego?Wydłużony czas działania funkcjiW przypadku funkcji wyzwalanych eventem zwiększono maksymalny czas procesowania z 9 do 10 minut, natomiast w przypadku funkcji wyzwalanych żądaniami HTTP zwiększono ten czas z 9 do aż 60 minut. Domyślny czas wynosi 1 minutę, można go zwiększyć za pomocą parametru --timeout. Dłuższy czas procesowania może przydać się np. w przypadku przetwarzania danych z Cloud Storage do BigQuery.Więcej informacji…Zwiększone maksymalne zasoby dla funkcjiW nowej generacji możemy utworzyć instancję z 4 vCPU / 16GB RAM (w przypadku poprzedniej generacji maksymalnie 2 vCPU / 8GB RAM). W wersji poglądowej pojawiła się również nowa opcja z 8 vCPU / 32GB RAM.Więcej informacji…Wersjonowanie funkcji i dzielenie ruchuCzyli coś co daje nam architektura Cloud Run. Możemy wersjonować funkcję i kierować procentowo ruch na różne jej wersje, co pozwala przetestować nową wersję na części użytkowników (Canary Deployment, A/B Testing). Dzięki temu możemy również bardzo szybko przywrócić poprzednią wersję funkcji.Więcej informacji…Przetwarzanie współbieżneW pierwszej generacji instancja funkcji przetwarzała jednocześnie tylko jedno żądanie, w nowej generacji możemy zdefiniować parametr --concurrency i ustalić liczbę jednocześnie przetwarzanych żądań. Dzięki temu możemy zmniejszyć minimalną liczbę instancji i zaoszczędzić czas przy tworzeniu nowych (cold start), co w praktyce przekłada się na niższe koszty. Maksymalna wartość parametru jest uzależniona od środowiska uruchomieniowego, a w przypadku ustawienia wartości większej niż 1 (domyślna wartość) instancja funkcji musi posiadać minimum 1 vCPU.Więcej informacji tutaj oraz tutaj…Minimalna liczba “rozgrzanych” instancjiMożemy zdefiniować liczbę instancji, które mają być cały czas gotowe do obsługi żądań. Pozwala to skrócić czas obsługi żądania w przypadku tworzenia nowej instancji (cold start). Warto pamiętać również o tym, że płacimy za cały czas działania funkcji, również za zimny start.Więcej informacji…Natywne wsparcie dla EventarcNowa generacja wprowadza natywne wsparcie dla platformy Eventarc, co rozszerza listę dostępnych wyzwalaczy funkcji o ponad 125 nowych. Dla porównania, pierwsza generacja obsługuje jedynie 7 wyzwalaczy, nie licząc żądań HTTP. Możemy na przykład utworzyć funkcję wyzwalaną zapytaniem BigQuery, która wysyła powiadomienie na Slacka w przypadku zbyt długich zapytań. Eventarc jest zgodny ze standardem CloudEvents, co pozwala uniknąć tzw. vendor locka. Eventarc wspiera również CMEK (customer-managed encryption keys), co umożliwia szyfrowanie eventów za pomocą kluczy zarządzanych przez nas samych.Więcej informacji…Nowe regionyFunkcje drugiej generacji są dostępne we wszystkich regionach w których dostępna jest pierwsza generacja oraz w dwóch dodatkowych: europe-north1 (Finlandia), europe-west4 (Holandia).Więcej informacji…Możliwość migracji funkcjiFunkcje drugiej generacji, z racji wykorzystywania architektury Cloud Run, umożliwiają łatwe przeniesienie ich do Cloud Run a nawet do Kubernetesa.PodsumowanieNowa generacja wprowadza bardzo wiele ciekawych zmian, pozwalających wycisnąć z funkcji jeszcze więcej jednocześnie optymalizując koszty i czasy odpowiedzi. Warto przetestować nowe funkcje na własną rękę, zaczynając np. od Getting started with Cloud Functions (2nd gen).",
"url": "/2022/08/18/cloud-functions-2.html",
"author": "Michał Hoja",
"authorUrl": "/authors/mhoja.html",
"image": "mhoja.jpg",
"highlight": "/assets/img/posts/2022-08-cloud-functions-2/clouds.jpg",
"date": "18-08-2022",
"path": "pl/2022-08-31-cloud-functions-2"
}
,


"2022-08-09-beacon-api-html": {
"title": "Czy wiesz, co to Beacon API?",
"lang": "pl",
"tags": "javascript ecmascript ES2021 Beacon API",
"content": "Beacon API pozwala na wysyłanie asynchronicznych, nieblokujących żądań. W odróżnieniu od XMLHttpRequest czy Fetch API, przeglądarka gwarantuje wykonanie żądań, nawet gdy użytkownik zamknie przeglądarkę. Idealnie nadaje się do wysyłania analityk czy logów z przeglądarki na serwer.Jak wygląda APIAPI składa się z jednej metody:navigator.sendBeacon(url, data?);Beacon wysyła żądanie POST, w parametrze data można przekazać FormData, Blob, ArrayBuffer, ArrayBufferView albo URLSearchParams. SendBeacon zwraca true, jeżeli przeglądarka z sukcesem dodała żądanie do kolejki wysyłek. Odpowiedź na żadanie jest pomijana.Przykład wysłania JSON jako Blob:const blob = new Blob([JSON.stringify({wolna: &#39;ukraina&#39;})], {type : &#39;application/json&#39;})navigator.sendBeacon(&#39;/api/log&#39;, blob);Zobacz więcej:  https://w3c.github.io/beacon/  https://developer.mozilla.org/en-US/docs/Web/API/Navigator/sendBeacon  https://caniuse.com/beacon",
"url": "/2022/08/09/beacon-api.html",
"author": "Dorian Mejer",
"authorUrl": "/authors/dmejer.html",
"image": "dmejer.jpg",
"highlight": "/assets/img/posts/2022-08-09-beacon-api/beacon-api.jpg",
"date": "09-08-2022",
"path": "pl/2022-08-09-beacon-api"
}
,


"2022-07-14-es2021-operators-html": {
"title": "Czy wiesz, że w ES2021 zostały wprowadzone operatory logicznego przypisania oraz nullish coalescing?",
"lang": "pl",
"tags": "javascript ecmascript ES2021",
"content": "Wersja 12 EcmaScript wprowadzona w czerwcu 2021 roku wprowadziła ciekawe, choć dość egzotycznie wyglądające operatory przypisania. Są nimi &amp;amp;&amp;amp;=, ||=, oraz ??=. Przyjrzyjmy się, jak działają:Logical AND assignment (&amp;amp;&amp;amp;=)Ten operator dokona przypisania tylko, jeśli zmienna, do której chcemy dokonać przypisania jest “prawdziwa” (ang. truthy).let truthy = 1;let falsy = 0;truthy &amp;amp;&amp;amp;= 10;falsy &amp;amp;&amp;amp;= 10;// Wynik:// truthy === 10// falsy === 0Logical OR assignment (||=)Ten operator dokona przypisania tylko, jeśli zmienna, do której chcemy dokonać przypisania jest “nieprawdziwa” (ang. falsy).let truthy = 1;let falsy = 0;truthy ||= 10;falsy ||= 10;// Wynik:// truthy === 1// falsy === 10Logical nullish assignment (??=)Ten operator dokona przypisania tylko, jeśli zmienna, do której chcemy dokonać przypisania jest null bądź undefined.let nullish = undefined;let notNullish = &quot;jestem stringiem&quot;;nullish ??= 10;notNullish ??= 10;// Wynik:// nullish === 10// notNullish === &quot;jestem stringiem&quot;",
"url": "/2022/07/14/es2021-operators.html",
"author": "Marcin Chrapkowicz",
"authorUrl": "/authors/mchrapkowicz.html",
"image": "mchrapkowicz.jpg",
"highlight": "/assets/img/posts/2022-07-14-es2021-operators/es2021-operators.jpg",
"date": "14-07-2022",
"path": "pl/2022-07-14-es2021-operators"
}
,


"2022-05-25-resilience-html": {
"title": "Czy wiesz czym są Resilience4j, CircuitBreaker, RateLimiter i inne?",
"lang": "pl",
"tags": "Resilience4j CircuitBreaker RateLimiter Java Spring Boot",
"content": "Co to i po co?Aplikacja nie odpowiada, pod leży, baza padła…Chyba nikogo nie dziwi, że aplikacje czasem nie działają. Jedną z najlepszych rzeczy, jaką możemy zrobić to przygotować się na taki stan i ograniczyć straty w pozostałych częściach systemu.Z pomocą przychodzi nam biblioteka Resilience4j, czyli lekka biblioteka udostępniająca narzędzia dla tworzenia systemów tolerujących awarię, zainspirowana przez Netflix Hystrix, ale zaprojektowana dla Javy 8 i programowania funkcjonalnego.Główne moduły biblioteki to: CircuitBreaker, Bulkhead, RateLimiter, Retry, TimeLimiter oraz Cache. Biblioteka opiera się na dekoratorach, których możemy użyć na lambdzie, metodzie, lub interfejsie funkcyjnym.Zobaczmy, co one oferują i jak można ich użyć.Jak możemy tego użyć?Każdego z modułów można użyć jako osobnej zależności, ale tu przedstawimy najszybszy i najprostszy sposób użycia Resilencje4j w połączeniu ze SpringBoot.Aby dodać w naszej aplikacji Resilience4j, wystarczy dodać zależność na resilience4j-spring-boot2 (dodatkowo wymagane są  spring-boot-starter-actuator oraz spring-boot-starter-aop).&amp;lt;dependency&amp;gt;    &amp;lt;groupId&amp;gt;io.github.resilience4j&amp;lt;/groupId&amp;gt;    &amp;lt;artifactId&amp;gt;resilience4j-spring-boot2&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;W ten sposób w aplikacji możemy korzystać z CircuitBreaker, Bulkhead, RateLimiter, Retry oraz TimeLimiter. Funkcjonalności możemy konfigurować w konfiguracji aplikacji oraz dodawać dekoratory za pomocą adnotacji. Możliwa jest także utworzenie i konfiguracja modułów bezpośrednio w kodzie.CircuitBreakerW dosłownym tłumaczeniu bezpiecznik, który otwiera się, gdy wykryje, że funkcjonalność, zwykle zewnętrzna usługa, jest tymczasowo niedostępna. Pozwala to odciążyć usługę i pozwolić jej na powrót do świata żywych.CircuitBreaker ma 3 stany:  zamknięty - normalne działanie, w trakcie którego monitorujemy działanie,  otwarty - stan po wykryciu niedostępności, przez określony czas zwracany jest błąd bez wywoływania oznaczonego kodu,  w połowie otwarty -  stan występujący po otwarciu w którym sprawdzamy czy funkcjonalność zaczęła działać.Konfiguracja CircuitBreaker:resilience4j.circuitbreaker:    instances:        backendA:            registerHealthIndicator: true            slidingWindowSize: 100        backendB:            registerHealthIndicator: true            slidingWindowSize: 10            permittedNumberOfCallsInHalfOpenState: 3            slidingWindowType: TIME_BASED            minimumNumberOfCalls: 20            waitDurationInOpenState: 50s            failureRateThreshold: 50            eventConsumerBufferSize: 10            recordFailurePredicate: io.github.robwin.exception.RecordFailurePredicateNajważniejszymi parametrami, które możemy konfigurować, są rodzaj i wielkość okna, w którym sprawdzamy procent błędów, oraz parametr określający, od jakiej ilości błędów otwieramy nasz bezpiecznik. Po skonfigurowaniu instancji CircuitBreakera możemy dodać adnotację na metodzie, która nas interesuje.Adnotacja:@CircuitBreaker(name = &quot;backendA&quot;, fallbackMethod = &quot;fallback&quot;)public Mono&amp;lt;String&amp;gt; method(String param1) {    return Mono.error(new NumberFormatException());}public Mono&amp;lt;String&amp;gt; fallback(String param1, IllegalArgumentException e) {    log.error(&quot;Error for backendA&quot;, e);}RetryJeśli usługi, które wywołujemy, są idempotentne i nie jesteśmy pewni czy udało się wywołać daną funkcjonalność, możemy pokusić się o ponowienie takiej operacji, licząc na to, że za chwilę zacznie działać.Moduł Retry pozwala zrobić to w łatwy sposób.Konfiguracja Retry:resilience4j.retry:    instances:        backendA:            maxAttempts: 3            waitDuration: 10s            enableExponentialBackoff: true            exponentialBackoffMultiplier: 2            retryExceptions:                - org.springframework.web.client.HttpServerErrorException                - java.io.IOException            ignoreExceptions:                - io.github.robwin.exception.BusinessExceptionUżycie Retry:@Retry(name = &quot;backendA&quot;, fallbackMethod = &quot;fallback&quot;)public Mono&amp;lt;String&amp;gt; method(String param1) {    return Mono.error(new NumberFormatException());}Time LimitterModuł TimeLimiter pozwala nam na ograniczenie maksymalnego czasu trwania żądania.Konfiguracja Retry:resilience4j.timelimiter:    instances:        backendA:            timeoutDuration: 2s            cancelRunningFuture: trueRate LimiterJeśli nie chcemy zbyt mocno obciążać danej części systemu, możemy ograniczyć liczbę wywołań danej metody w czasie.Konfiguracja Retryresilience4j.ratelimiter:    instances:        backendA:            limitForPeriod: 10            limitRefreshPeriod: 1s            timeoutDuration: 0            registerHealthIndicator: true            eventConsumerBufferSize: 100Składanie wielu dekoratorówWszystkie z wymienionych dekoratorów możemy stosować równocześnie.@CircuitBreaker(name = BACKEND, fallbackMethod = &quot;fallback&quot;)@RateLimiter(name = BACKEND)@Bulkhead(name = BACKEND)@Retry(name = BACKEND, fallbackMethod = &quot;fallback&quot;)@TimeLimiter(name = BACKEND)public Mono&amp;lt;String&amp;gt; method(String param1) {    return Mono.error(new NumberFormatException());}",
"url": "/2022/05/25/resilience.html",
"author": "Piotr Stachowiak",
"authorUrl": "/authors/pstachowiak.html",
"image": "pstachowiak.jpg",
"highlight": "/assets/img/posts/2022-05-25-resilience/resilience.jpg",
"date": "25-05-2022",
"path": "pl/2022-05-25-resilience"
}
,


"2022-05-16-ngrok-html": {
"title": "Czy wiesz, że możesz łatwo udostępnić swojego localhosta innym osobom (ngrok)?",
"lang": "pl",
"tags": "javascript",
"content": "Czasami dochodzi do takiej sytuacji, gdy chcemy komuś pokazać to, co mamy aktualnie odpalone na naszym sprzęcie na localhost. Możemy udostępnić ekran, ale ogranicza to możliwości interakcji.Zdarza się również, że coś, nad czym aktualnie pracujemy, chcielibyśmy odpalić na fizycznym telefonie bez większej zabawy z siecią, aby telefon miał dostęp do tych samych zasobów co komputer, na którym pracujemy.Problemy te rozwiązują narzędzia, które tunelują ruch z naszego localhosta przez internet* do innego urządzenia.Takim narzędziem jest ngrok. Konfigurowanie i pierwsze odpalenie nie są trudne:  zakładamy konto, aby otrzymać unikalny token, dzięki któremu będziemy mogli uruchomić aplikację,  ściągamy wymagane pliki,  autoryzujemy się podając wcześniej otrzymany token oraz odpalamy aplikację (możemy przekazać który port chcemy tunelować)./ngrok http 8080Zrzut z działającej aplikacji:Widzimy między innymi jaki adres został wygenerowany, dzięki któremu będziemy mogli dostać się do naszych zasobów (Forwarding) oraz jakie zapytania zostały już wysłane (HTTP Requests).Teraz ktoś wchodząc na http://104e-46-187-241-0.ngrok.io zobaczy to, co my widzimy na naszym localhoście.*Ruch odbywa się przez internet, więc za każdym razem, kiedy decydujemy się na takie tunelowanie, trzeba się zastanowić jakie dane będziemy upubliczniać.",
"url": "/2022/05/16/ngrok.html",
"author": "Piotr Grobelny",
"authorUrl": "/authors/pgrobelny.html",
"image": "pgrobelny.webp",
"highlight": "/assets/img/posts/2022-05-16-ngrok/ngrok.jpg",
"date": "16-05-2022",
"path": "pl/2022-05-16-ngrok"
}
,


"2022-03-18-npx-html": {
"title": "Czy wiesz, czym jest i jak działa npx?",
"lang": "pl",
"tags": "npm npx node",
"content": "Czym jest npx?npx to przydatne narzędzie CLI (Command Line Interface) pozwalające wykonywać skrypty pochodzące z repozytorium npm bez zbędnej konfiguracji. Co ciekawe, można w ten sposób wywołać polecenie z pakietu niezainstalowanego na maszynie lokalnej.Przykładowe zastosowanie:npx create-react-app moja-aplikacjaPowyższe polecenie tymczasowo zainstaluje pakiet create-react-app na maszynie i go wywoła. Po zakończeniu wykonywania, w systemie nie będzie żadnej informacji o pakiecie, co można potwierdzić wykonując chociażby polecenie which create-react-app.Obsługa pakietów globalnychPakiety npm bardzo często są tworzone jako globalne, aby zaraz po instalacji mogły być wykonywane. To podejście powoduje, że nie można mieć więcej, niż jednej wersji tego samego globalnego pakietu w systemie. npx z kolei rozwiązuje ten problem, gdyż potrafi wykonywać skrypty zainstalowane w node_modules dla konkretnego projektu.Jak zainstalować npx?npx jest wbudowany w npm od wersji 5.2.0. Istnieje również możliwość instalacji samego npx poprzez poniższe polecenie:npm install -g npx",
"url": "/2022/03/18/npx.html",
"author": "Marcin Chrapkowicz",
"authorUrl": "/authors/mchrapkowicz.html",
"image": "mchrapkowicz.jpg",
"highlight": "/assets/img/posts/2022-03-16-npx/keyboard.jpg",
"date": "18-03-2022",
"path": "pl/2022-03-16-npx"
}
,


"2022-02-20-porownanie-jezykow-cloud-functions-html": {
"title": "Czy język programowania i region mają wpływ na wydajność Google Cloud Functions?",
"lang": "pl",
"tags": "google cloud function google cloud platform gcp cold start googlecloud serverless",
"content": "Jeżeli chociaż raz zastanawiałeś się, w jakim języku programowania napisać funkcję w Google Cloud, to w tym wpisie postaram się pomóc w podjęciu tej decyzji.Na warsztat weźmiemy wszystkie dostępne na ten moment środowiska uruchomieniowe dla Google Cloud Functions i porównamy czasy odpowiedzi oraz zimne starty (tzw. cold starts).Porównamy nie tylko środowiska uruchomieniowe, ale również regiony w których osadzone są funkcje.MotywacjaPisząc swoją pierwszą funkcję w GCP zastanawiałem się, w jakim języku ją napisać? Przecież to prosta funkcja, mogę ją napisać w każdym dostępnym języku. Pisać w Javie, której używam na co dzień? A może w Node.js? Przecież TypeScript też jest dla mnie codziennością…Motywacją do przeprowadzenia testów był przede wszystkim brak odpowiedzi na moje pytania oraz brak porównań środowisk uruchomieniowych dla Cloud Functions w Internecie.Środowisko testoweGoogle co chwilę rozszerza listę obsługiwanych środowisk uruchomieniowych, dlatego zależało mi na tym, żeby porównanie funkcji było łatwe do przeprowadzenia w przyszłości, z uwzględnieniem nowych języków. Chcąc zautomatyzować całą procedurę i środowisko testowe, wraz z kolegą Jackiem Grobelnym przygotowaliśmy projekt pt. Google Coud Functions Comparison.Do automatyzacji wykorzystany został Terraform, za pomocą którego przygotowywane jest całe środowisko testowe. Wszystkie osadzane funkcje są definiowane w konfiguracji, dlatego w prosty sposób można uruchomić środowisko testujące wybrane języki oraz regiony.Testy czasów odpowiedzi zostały napisane w Gatlingu, który listę funkcji odczytuje z tej samej konfiguracji, przez co nie wymaga żadnej dodatkowej ingerencji. Testy zimnych startów wykonywane są natomiast bezpośrednio przez kod napisany w Scali, a wyniki wyświetlane są w formie tabeli ASCII.Kody wszystkich funkcji znajdują się w folderze /functions i są to podstawowe funkcje odpowiadające “Hello World”, takie same jak przykładowe funkcje tworzone z poziomu Cloud Console.Projekt znajduję się na GitHubie - link do repozytorium.Metodyka testowaniaW testach wykorzystałem funkcje uruchomione w następujących środowiskach uruchomieniowych:  .NET Core 3.1  Go 1.13  Java 11  Node.js 14  PHP 7.4  Python 3.9  Ruby 2.7Każda funkcja posiadała maksymalnie jedną instancję, przydzieloną pamięć 128 MB i została uruchomiona w regionach:  europe-west3 (Frankfurt, Germany, Europe)  us-central1 (Council Bluffs, Iowa, North America)  asia-east2 (Hong Kong, APAC)W celu porównania czasów odpowiedzi, każda funkcja była wywoływana przez 10 minut i 20 równoległych użytkowników. Ilość wykonanych żądań jest zatem uzależniona od samej funkcji oraz Gatlinga (a w zasadzie, to mojego laptopa).Testy zimnych startów zostały wykonane z zapewnieniem braku istnienia aktywnej instancji. Test polegał na wykonaniu 10 żądań do każdej z funkcji, a następnie porównaniu czasu pierwszej odpowiedzi do średniej arytmetycznej czasów pozostałych 9 odpowiedzi.Każdy test uruchomiłem dwa razy, o tej samej godzinie czasu polskiego dla wszystkich regionów oraz o tej samej godzinie czasu lokalnego w danym regionie. Wszystkie żądania były wykonywane z mojej stacji roboczej w Poznaniu.Przyjąłem nazewnictwo język interpretowany dla języków skryptowych i kompilowanych (nie korzystających z maszyny wirtualnej) oraz język uruchamiany w maszynie wirtualnej dla języków kompilowanych i uruchamianych w maszynie wirtualnej.Czasy odpowiedziGodziny uruchomienia testów  Run #1 - środek tygodnia, 17-23 czasu polskiego  Run #2 - środek tygodnia, 20:00 czasu lokalnego w danym regionie:          europe-west3 (Frankfurt, Germany, Europe) - 20:00 o 20:00 czasu polskiego      us-central1 (Council Bluffs, Iowa, North America) - 20:00 o 03:00 czasu polskiego (-7h)      asia-east2 (Hong Kong, APAC) - 20:00 o 13:00 czasu polskiego (+7h)      Wyniki    Run #1    Run #2                                        Runtime                Region                Requests                Min [ms]                95th pct [ms]                Max [ms]                Mean [ms]                Std Dev [ms]                                                        nodejs14                europe-west3                20796                85                909                1591                576                182                                        go113                europe-west3                25681                83                856                7607                466                213                                        java11                europe-west3                20408                83                1078                2675                587                259                                        python39                europe-west3                20810                83                893                1601                575                160                                        ruby27                europe-west3                25818                82                711                1791                464                156                                        dotnet3                europe-west3                17489                84                1003                3912                685                218                                        php74                europe-west3                25162                84                793                1494                476                160                                        nodejs14                us-central1                16341                192                1100                2789                733                244                                        go113                us-central1                19738                192                1017                1757                607                213                                        java11                us-central1                16545                190                1400                7796                724                339                                        python39                us-central1                14907                192                1200                2302                804                248                                        ruby27                us-central1                17968                193                1091                3559                667                229                                        dotnet3                us-central1                14444                192                1197                3407                830                240                                        php74                us-central1                16172                193                1104                2088                741                192                                        nodejs14                asia-east2                25112                352                780                2086                477                142                                        go113                asia-east2                28831                350                587                1604                415                112                                        java11                asia-east2                22617                352                1093                5098                530                292                                        python39                asia-east2                20441                373                889                1786                586                146                                        ruby27                asia-east2                28630                349                589                1872                418                106                                        dotnet3                asia-east2                21549                364                896                3197                556                184                                        php74                asia-east2                26001                351                715                1786                461                140                                                                Runtime                Region                Requests                Min [ms]                95th pct [ms]                Max [ms]                Mean [ms]                Std Dev [ms]                                                        nodejs14                europe-west3                20487                80                992                1516                585                191                                        go113                europe-west3                26846                83                796                2314                446                175                                        java11                europe-west3                22988                82                906                2400                521                224                                        python39                europe-west3                21902                85                806                1496                547                155                                        ruby27                europe-west3                27023                82                703                2064                443                163                                        dotnet3                europe-west3                17760                85                995                2800                675                189                                        php74                europe-west3                22418                82                890                1712                534                167                                        nodejs14                us-central1                18468                188                1006                1625                649                197                                        go113                us-central1                20845                185                905                1805                575                183                                        java11                us-central1                17753                188                1290                3199                675                303                                        python39                us-central1                16048                188                1197                2322                747                232                                        ruby27                us-central1                18306                185                1013                2020                655                218                                        dotnet3                us-central1                17756                187                1153                3825                676                290                                        php74                us-central1                16902                188                1098                2102                709                199                                        nodejs14                asia-east2                22854                349                835                2309                524                157                                        go113                asia-east2                28844                346                594                1749                415                112                                        java11                asia-east2                21243                347                1109                5398                564                316                                        python39                asia-east2                21013                367                805                1902                570                147                                        ruby27                asia-east2                27958                343                604                1849                428                127                                        dotnet3                asia-east2                21802                357                887                3104                549                172                                        php74                asia-east2                25581                348                711                2224                468                151                        RegionyWyniki pierwszego testu nieco mnie zaskoczyły, ponieważ bardzo dobrze wypadły tutaj funkcje osadzone w Azji (a nie najbliżej mojej geolokalizacji, jak się spodziewałem).Dlatego postanowiłem wykonać test ponownie, aby wykluczyć różnice w czasie (ponieważ o godzinie 20:00 czasu Polskiego, w Hong Kongu była godzina 03:00). Dzięki temu mogłem sprawdzić, czy wpływ na wyniki ma tutaj fakt, że w środku nocy obciążenie centrum danych może być mniejsze.Drugi test wykluczył jednak kwestie godziny w danym regionie, ponieważ i tym razem Azja wypadła najlepiej. Z powodu odległości można zaobserwować znacznie wyższe minimalne i maksymalne czasy odpowiedzi, jednak średnio były one i tak nieco niższe niż w przypadku Frankfurtu. W ciągu 10 minut udało się wykonać sumarycznie więcej żądań.Najgorzej wypadł region w USA, gdzie pomimo niższych minimalnych czasów odpowiedzi, średnio były one znacznie wyższe (co idealnie pokazuje kolumna z 95 percentylem). W efekcie funkcje uruchomione w USA obsłużyły zauważalnie mniejszą liczbę żądań.Testy starałem się wykonać w środku tygodnia, aby były jak najbardziej wiarygodne. Pod uwagę należy wziąć jednak fakt, że funkcje były bardzo prymitywne - jedyne co robiły, to odpowiadały “Hello World”. Całkiem możliwe, że w przypadku funkcji do których wysyłamy lub które zwracają jakieś dane, wyniki byłyby zupełnie inne. Zależało mi jednak na sprawdzeniu prostych funkcji, ponieważ w tym przypadku łatwo jest porównać środowiska uruchomieniowe (w przypadku bardziej złożonych implementacji duży wpływ na wydajność mogłyby mieć wykorzystane zewnętrzne zależności czy biblioteki).Podsumowując, gdybym chciał uruchomić prostą funkcję i zależałoby mi na tym, aby obsłużyła jak największy ruch, prawdopodobnie wybrałbym któryś z regionów w Azji.Środowiska uruchomienioweW przypadku środowisk uruchomieniowych spodziewałem się, że języki interpretowane dają lepsze wyniki niż języki uruchamiane w wirtualnej maszynie.Wyniki testu częściowo potwierdziły moje podejrzenia, ponieważ najszybciej odpowiadały funkcje napisane w Go, Ruby czy PHP. Dużym zaskoczeniem były dla mnie wyniki Node.js, które są dość przeciętne. Spodziewałem się że JavaScript uplasuje się w czołówce, jednak wyniki były bardziej zbliżone do języków uruchamianych w maszynie wirtualnej.Kompletnie nie zdziwiły mnie za to wyniki funkcji napisanych w Javie czy .NET, jednak nie spisywałbym ich na straty. Środowiska uruchomieniowe wykorzystujące maszyny wirtualne (takie jak właśnie Java - JVM, czy .NET - CLR) potrafią optymalizować uruchomiony kod, jednak nie zrobią tego od razu, ponieważ potrzebują w tym celu zebrać odpowiednią ilość statystyk. Całkiem możliwe, że funkcje które obsługują bardzo dużo żądań w czasie ciągłym (czyli takim, dzięki któremu instancja funkcji będzie żyła bardzo długo) osiągnęłyby z czasem lepsze wyniki.Jakie z tego wnioski? Jeżeli piszemy prostą funkcję i nie zależy nam na wydajności (albo spodziewamy się małego ruchu), śmiało możemy napisać ją w języku programowania, który znamy najlepiej. Jeżeli jednak zależy nam na obsłudze jak największej ilości żądań (i jednocześnie wiemy, że instancja funkcji nie będzie długowieczna), najlepszym wyborem będą języki, które nie są uruchamiane w wirtualnej maszynie.Zimne startyGodziny uruchomienia testów  Run #1 - środek tygodnia, 22 czasu polskiego  Run #2 - środek tygodnia, 20:00 czasu lokalnego w danym regionie:          europe-west3 (Frankfurt, Germany, Europe) - 20:00 o 20:00 czasu polskiego      us-central1 (Council Bluffs, Iowa, North America) - 20:00 o 03:00 czasu polskiego (-7h)      asia-east2 (Hong Kong, APAC) - 20:00 o 13:00 czasu polskiego (+7h)      Wyniki    Run #1    Run #2                                        Runtime                Region                1st time [ms]                avg remaining [ms]                diff [ms]                                                        nodejs14                europe-west3                1176                49                1127                                        go113                europe-west3                447                43                404                                        java11                europe-west3                54                101                -47                                        python39                europe-west3                895                59                836                                        ruby27                europe-west3                997                41                956                                        dotnet3                europe-west3                1054                95                959                                        php74                europe-west3                679                45                634                                        nodejs14                us-central1                1123                250                873                                        go113                us-central1                922                227                695                                        java11                us-central1                1638                284                1354                                        python39                us-central1                1254                259                995                                        ruby27                us-central1                1430                193                1237                                        dotnet3                us-central1                1536                261                1275                                        php74                us-central1                1271                211                1060                                        nodejs14                asia-east2                1228                381                847                                        go113                asia-east2                869                375                494                                        java11                asia-east2                1227                398                829                                        python39                asia-east2                1433                375                1058                                        ruby27                asia-east2                1126                388                738                                        dotnet3                asia-east2                1023                410                613                                        php74                asia-east2                904                420                484                                                                Runtime                Region                1st time [ms]                avg remaining [ms]                diff [ms]                                                        nodejs14                europe-west3                409                49                360                                        go113                europe-west3                61                46                15                                        java11                europe-west3                164                120                44                                        python39                europe-west3                98                53                45                                        ruby27                europe-west3                53                45                8                                        dotnet3                europe-west3                325                95                230                                        php74                europe-west3                169                43                126                                        nodejs14                us-central1                1214                203                1011                                        go113                us-central1                269                230                39                                        java11                us-central1                819                284                535                                        python39                us-central1                411                249                162                                        ruby27                us-central1                310                215                95                                        dotnet3                us-central1                1022                284                738                                        php74                us-central1                1004                240                764                                        nodejs14                asia-east2                880                496                384                                        go113                asia-east2                394                409                -15                                        java11                asia-east2                409                454                -45                                        python39                asia-east2                406                387                19                                        ruby27                asia-east2                405                409                -4                                        dotnet3                asia-east2                510                450                60                                        php74                asia-east2                448                398                50                        RegionyW odróżnieniu od wyników czasów odpowiedzi, w przypadku zimnych startów najlepiej wypadł region, który był najbliżej mojej geolokalizacji. Zarówno pierwsza odpowiedź oraz średnia pozostałych, była najniższa w przypadku funkcji osadzonych we Frankfurcie. Najgorzej natomiast wypadła Azja, co pokrywałoby się z poprzednim testem, ponieważ funkcje w Azji miały najwyższy minimalny czas odpowiedzi.Podobnie jak w poprzednim teście, różnice w czasie między regionami nie miały żadnego znaczenia.Na co się zdecydować, biorąc pod uwagę wyniki tych testów? Jeżeli potrzebujemy funkcji, która jest rzadko wywoływana (jej instancja jest krótko żyjąca) i zależy nam na jak najszybszej odpowiedzi, najlepiej powinny sprawdzić się funkcje uruchomione najbliżej geolokalizacji użytkownika końcowego. Dzięki bliskości centrum danych, nie tracimy czasu na przesyłanie żądania i odpowiedzi, a wąskim gardłem jest tutaj zimny start, czyli czas potrzebny na uruchomienie instancji funkcji.Środowiska uruchomienioweTak samo jak w poprzednim teście, w przypadku różnic między środowiskami uruchomieniowymi spodziewałem się lepszych wyników w przypadku funkcji napisanych w językach interpretowanych, od tych uruchamianych w maszynie wirtualnej. Wyniki jednak mnie zaskoczyły, ponieważ czasami jakieś środowisko uruchomieniowe wypadało bardzo dobrze, a czasami dużo gorzej.W przypadku zimnych startów na pewno większą rolę odgrywa region, w którym osadzimy funkcję, niż język w którym ją napiszemy. Języki programowania, po których spodziewałem się lepszych wyników (Python i Ruby) nie wypadły wcale lepiej od Javy i .NET, które w teorii powinny potrzebować więcej czasu na uruchomienie.Patrząc na wyniki testu, nie potrafię jednoznacznie stwierdzić w jakim języku napisałbym funkcję, aby zapewnić jak najkrótszy zimny start. Sytuacja mogłaby ulec zmianie w przypadku bardziej złożonych implementacji i wykorzystania zewnętrznych zależności/bibliotek, ponieważ ich rozmiar i implementacja mogłyby odgrywać tutaj kluczową rolę.PodsumowanieNa zakończenie chciałbym zaznaczyć, że wykonane przeze mnie testy dotyczyły jedynie prostych implementacji funkcji, a wyniki mogłyby być inne w przypadku bardziej złożonych implementacji lub przesyłania większej ilości danych. Mimo wszystko najczęściej spotykam się z bardzo prostymi funkcjami i z tego też powodu przeprowadziłem takie testy. Starając się porównać środowiska uruchomieniowe, musiałem zapewnić zbliżoną implementację funkcji, aby wykluczyć wpływ dostępnych bibliotek i zależności na wyniki.Zachęcam również do przeprowadzania własnych testów za pomocą narzędzia Google Coud Functions Comparison, ponieważ jak widać, wyniki potrafią być zaskakujące i nieoczywiste.",
"url": "/2022/02/20/porownanie-jezykow-cloud-functions.html",
"author": "Michał Hoja",
"authorUrl": "/authors/mhoja.html",
"image": "mhoja.jpg",
"highlight": "/assets/img/posts/2022-02-porownanie-jezykow-programowania-cloud-functions/clouds.jpg",
"date": "20-02-2022",
"path": "pl/2022-02-21-porownanie-jezykow-cloud-functions"
}
,


"2022-02-15-smarthome-html": {
"title": "Inteligentny dom dla programisty, czyli DIY Smart Home",
"lang": "pl",
"tags": "smarthome homeassistant zigbee2mqtt zigbee homekit heysiri heygoogle heyalexa",
"content": "Jakiś czas temu trafiłem na artykuł, którego głównym tematem był wzrost popularności Smart Home w czasach pandemii. Od tamtej pory obiecuję sobie, że w końcu się zbiorę i w tzw. pigułce opiszę, z jakich urządzeń ja korzystam, co udało mi się z ich pomocą osiągnąć i jakie problemy napotkałem.Integracje / AutomatyzacjeZanim jednak przejdę do detali i wszystkich technicznych kwestii związanych ze Smart Home, opowiem o swoich przykładach automatyzacji inteligentnego domu dla programisty.OświetleniePrzygodę ze Smart Home zacząłem od oświetlenia, a inspiracją do zmian były przełączniki w niewygodnych miejscach.Na start wziąłem przedpokój, jako idealne miejsce do zastosowania czujnika ruchu. Teraz wystarczy wejść, aby światło z automatu się zapaliło i zgasiło po około 40 sekundach bez ruchu. Oczywiście nie „odciąłem” standardowego włącznika na ścianie, a jego użycie zaprogramowałem tak, żeby ręczne włączenie nie brało pod uwagę braku ruchu i było zapalone „na stałe”.Moje czujniki ruchu działają na baterie i są bardzo małe, można więc je sprytnie ukryć, zachowując estetykę pomieszczenia.Poniżej przykład czujnika ruchu, który zapala światło w przedpokoju. Umieściłem go nad lewą szafką, dzięki czemu jest niewidoczny.Doświadczenie z czujnikami ruchu wykorzystałem też w innych miejscach. Dla przykładu jeden z nich zapala taśmę LED pod łóżkiem w sypialni, dając ciekawy efekt.Kolejne pomieszczenie, w którym chciałem zadbać o ergonomię związaną z użyciem przełącznika oświetlenia, była kuchnia.Tak jak wcześniej, standardowy przełącznik na ścianie pozostawiłem, ale dołożyłem drugi „przyklejany” na magnes pod okapem kuchennym.Może nieco mniej smart niż poprzednie rozwiązanie, za to komfort zdecydowanie większy.Dosyć często zapominam o wyłączeniu oświetlenia w kuchni, planuję więc dołożyć jeszcze czujnik, który po kilku minutach bez ruchu automatycznie wyłączy światło nad blatem.Kolejna oświetleniowa automatyzacja, którą warto rozważyć, to taka zależna od zachodu słońca (zmiennego w zależności od daty i położenia geograficznego).Automatyzacja „zapala mi” zewnętrzne lampki świąteczne po zachodzie słońca, a wyłącza tuż przed północą.I jeszcze jedna „oświecona” podpowiedź na koniec. Z użyciem odpowiedniej automatyzacji, bazującej na położeniu telefonów domowników, za każdym razem, kiedy wszyscy są poza domem, mam pewność, że wszystkie światła są pogaszone.OgrzewanieW moim inteligentnym domu zmieniło się też sporo w kwestii ogrzewania.Poranek zawsze zaczyna się od wpuszczenia świeżego powietrza, wiec otwarcie okna z automatu zakręca głowice na kaloryferach,a odkręca po 10 minutach od ich zamknięcia w celu unormowania temperatury.Podobnie jak w przypadku oświetlenia tak i tutaj, gdy wszyscy domownicy są poza domem, to upewniam się, że grzanie jest zakręcone.Ostrzeżenia / alarmyZ użyciem Smart Home i wcześniej wspomnianych czujników ruchu, możemy stworzyć własny system alarmowy,który pod nieobecność domowników włączy alarm i przekaże nam odpowiedni komunikat.W moim przypadku bardzo przydatne okazały się czujniki informujące o wycieku.Przy użytkowaniu zmywarki okazało się, że na jednym z połączeń jest mała nieszczelność i dopiero pod koniec zmywania pojawiały się pojedyncze kropelki, które czujnik na szczęście wykrył.Innym razem, w tajemniczy sposób pękło połączenie silikonowe między wanną a ścianą, przez co pod prysznicem powoli zbierała się woda.Tak jak w przypadku zmywarki, tak i tutaj były to dosłownie pojedyncze kropelki.Do przetarcia wystarczał jeden listek ręcznika papierowego.Nie mniej jednak dzięki alarmowi wiedziałem, że coś trzeba poprawić.Centrum sterowania Smart HomeMając już zestaw narzędzi do samodzielnego wdrożenia Smart Home (nasze DoItYourself), wszystko możemy spiąć tak, aby nasz telefon stał się „centrum sterowania” inteligentnego domu dla programisty.Możemy też korzystać z coraz bardziej popularnego sterowania głosowego.Z odpowiednimi urządzeniami wystarczy „Hey Google” lub „Hey Siri” oraz konkretna komenda, aby nasz asystent włączył światło, zmienił jego kolor czy zasłonił okna.Poniżej przykład z integracji HomeKit na telefonach wyposażonych w iOS.Jeśli ktoś posiada stary telefon, tablet czy czytnik kindle z wifi, może mu nadać nowe życie. Takie nasze programistyczne zero waste ;)Stare urządzenie można umieścić w centralnym miejscu, prezentując aktualny stan temperatury, światła lub tego, co nam nasza kreatywność podpowie.Wydaje mi się, że ogranicza nas tylko wyobraźnia. Poniżej znowu moje przykłady:Co siedzi pod „maską”?Zanim zaczniesz ze Smart Home, przejrzyj dostępne rozwiązania i zwróć uwagę na ich ograniczenia.Wiadomo, że najłatwiej uczyć się na błędach i sukcesach innych, opiszę więc swoją ścieżkę do optymalnego rozwiązania.Na starcie wybrałem Xiaomi, bo było dostępnych najwięcej urządzeń w przyzwoitych cenach.Dodatkowo miałem już czujnik do kwiatka i żarówkę RGB od tego producenta.Kupiłem bramkę Xiaomi Gateway v3 działającą po Zigbee oraz Bluetooth i kilka czujników.Niedługo później zaczęły się schody.Okazało się, że do sensownego działania bramka potrzebuje stałego dostępu do internetu, komunikuje się z chińskimi serwerami (bo taką wersję kupiłem) i pojawiają się opóźnienia na automatyzacjach.Wtedy zrobiłem mały krok wstecz i zacząłem szukać rozwiązania, które w pełni będzie działało w sieci lokalnej oraz będzie obsługiwało sensory ZigBee, które już mam.Tak trafiłem na HomeAssistant i tzw. uniwersalne bramki Zigbee, do których można podłączyć większość sprzętu od różnych producentów.Bramka ZigbeeW kwestii bramek uniwersalnych popularnych jest kilka układów. Tym razem, przed zakupem dokładniej prześledziłem opinie użytkowników.Po analizie wybór padł na ZZH. To właśnie to urządzenie miało najlepsze oceny użytkowników i spełniło moje oczekiwania.Mam to urządzenia od jakichś dwóch lat i dla mnie to był bardzo dobry wybór. Nigdy nie miałem z nim problemów.Komputer pod automatyzacjęSkoro mamy już koncepcje i narzędzia dla naszego Smart Home, przyszedł czas na wybór software,który będzie obsługiwać uniwersalną bramkę oraz pozwoli nam tworzyć automatyzację.Software trzeba na czymś zainstalować, więc trzeba wybrać odpowiedni komputer. Tutaj też miałem małe potknięcie.Akurat miałem wolne Raspberry Pi2 i postanowiłem z niego skorzystać, pomimo że producenci poniższego softu na swoich stronach wprost to odradzali.Jak się szybko przekonałem, mieli rację.Pomijam już problemy techniczne z zasilaniem mojego egzemplarza Raspberry, które po jakimś czasie freez’owało…Większą bolączką okazał się ponowny start HomeAssistant’a, potrzebujący kilkunastu minut, aby podnieść całe oprogramowanie do Smart Home.To było stanowczo za długo, a urządzeń oraz automatyzacji miałem zdecydowanie mniej niż teraz.Aktualnie korzystam z wirtualki z 2GB RAM, ale wiem, że Raspberry Pi4 też spokojnie daje radę.Zigbee2MQTTCzujniki oraz uniwersalną bramkę trzeba jakoś sparować. Zigbee2MQTT jest oprogramowaniem, które za pomocą uniwersalnej bramki łączy się z urządzeniami ZigBee, publikuje ich zdarzenia na kolejkę MQTT,a także w drugą stronę, zdarzenia z kolejki wysyła do urządzeń.HomeAssistantJak już mamy coś, co komunikuje się z naszymi urządzeniami za pomocą bramki, to możemy wyświetlić stan naszych urządzeń, a także na ich podstawie zrobić automatyzacje,do tego właśnie służy HomeAssistant. Na tym etapie warto wspomnieć, że instalację Zigbee2MQTT oraz MQTT osobno można pominąć i skorzystać z rozwiązań dostarczanych lub instalowanych przez wyklikanie ich w HomeAssitant.Ja poszedłem jednak drogą osobnych komponentów. Jak zaczynałem swoją przygodę, natrafiłem na informację, że Zigbee2MQTT wspiera więcej urządzeń niż sam HomeAssitant, a dodatkowo zyskałem możliwość niezależnej aktualizacji poszczególnych komponentów.Urządzenia wykonawcze, czujniki, przyciski i tak dalej…Tę sekcję podzielę na trzy podsekcje, ponieważ w swoim Smart Home korzystam głównie z trzech sposobów komunikacji.Pierwszy ze sposobów to Wi-Fi, drugi to Bluetooth, trzeci to Zigbee.Komunikacja po Wi-FiZaletą tego sposobu komunikacji jest jego powszechność i łatwość integracji. Każdy ma Wi-Fi, obsługa jest łatwa, wadą natomiast jest zasięg.Nie każdy ma odpowiednie wzmacniacze sygnału i może się okazać, że zasięg w wybranym miejscu jest mało stabilny, przez co urządzenie też nie działa, tak jakbyśmy tego chcieli.Dodatkowo Smart Home to też ciągle rosnąca liczba urządzeń, która może spowodować problemy wydajnościowe na „pierwszym lepszym” punkcie dostępowym.Ze swojego doświadczenia mogę powiedzieć, że pomimo bardzo dobrego zasięgu zdarzyło mi się kilka problemów z urządzeniami Wi-Fi.Ostatni z problemów, który pamiętam to taki, że przekaźnik Shelly zintegrowany po MQTT przestał być widoczny przez HomeAssistant’a po zaniku zasilania.Wymagane było odpowiednie skonfigurowanie i wydaje mi się, że od tamtej pory jest już ok.Poniżej podrzucam listę przykładowych urządzeń działających po Wi-Fi, których używam w swoim Smart Home:Taśma LED YeelightZalety — ładnie świeci, możliwy wybór koloru, łatwo się przedłuża.Wady — maksymalna długość taśmy w niektórych przypadkach może nie wystarczyć.Łączenie kolejnych „kawałków” jest zrobione dosyć dużym wtykiem, w porównaniu do taśm, które lutujemy i może spowodować problem z umieszczeniem całości w aluminiowym profilu na taśmy.Przekaźnik Shelly 2.5Zalety — duża liczba dostarczanych danych, a także różne sposoby integracji (API lub MQTT).Każdy z dwóch kanałów może być osobno konfigurowany pod względem rodzaju podłączonego klawisza (dzwonkowy, stanowy).Dla każdego z kanałów mamy też osobny zestaw informacji o aktualnym poborze prądu.Możemy skonfigurować stan przekaźnika po powrocie zasilania (dla mnie to ważne, aby pod nieobecność domowników nie okazało się, że w całym mieszkaniu mamy zapalone światła).No i ostatnie — mieści się w puszce, niestety część urządzeń nie przewiduje umieszczania w europejskich okrągłych puszkach.Wady — komunikacja Wi-Fi i wszystko, co z nią się wiąże.Z doświadczenia preferuje Zigbee, tym bardziej że urządzenia Zigbee na stałe podłączone do prądu z automatu przedłużają zasięg sieci.ESP32Mały, tani, ale potężny układ. Na standardowym wyposażeniu używanej przeze mnie wersji „devkit v1” znajduje się Wi-Fi oraz Bluetooth.Dzięki popularności układu łatwo można znaleźć projekty obudowy do samodzielnego wydrukowania na drukarce 3D.W moim Smart Home pracują dwa rodzaje takich układów:  Pierwszy z nich ma na pokładzie OpenMQTTGateway. Wykorzystuje go do odczytywania danych z czujników Bluetooth, jak czujnik w kwiatku (o czym dalej) czy zegarek z wyświetlaczem eInk.  Drugi z nich ma na pokładzie EspHome i podłączony odbiornik oraz nadajnik podczerwieni. Myślę, że podłączone elementy już dobrze reprezentują jego przeznaczenie. W dalszych planach mam też przetłumaczenie sygnałów podczerwieni z indukcji Electrolux Hob2Hood na okap firmy Globalo (sprzęt, jaki posiadam).Wyzwanie stanowi brak w okapie opcji „ustaw wentylator na 3” (taki sygnał nada indukcja). Jedynym sposobem osiągnięcia tego stanu jest klikanie w +/-.Komunikacja po BluetoothO tym sposobie integracji z pewnością można powiedzieć, że jego największą wadą jest mały zasięg.Wcześniej wspomniany układ Esp32 z OpenMQTTGateway może okazać się niezbędnym narzędziem, celem zebrania danych z różnych zakątków domu.Ciekawostką może być to, że sensory Bluetooth Low Energy, póki są niepołączone z konkretnym urządzeniem, nadają informacje o swoim stanie w taki sposób,że każdy może je odczytać. Stąd odbieram dane z zegarka Xiaomi sąsiadów…Urządzenia Bluetooth, z których korzystam:Xiaomi Flora flower monitorZalety — duża liczba danych, zarówno o glebie, jak i otoczeniu. Z danych otoczenia mamy np. temperaturę oraz jasność.Wady — to na pewno komunikacja bluetooth i to, z czym się wiąże.Zegarek z wyświetlaczem eInk (lywsd02mmc)Zalety — na pewno czytelny wyświetlacz i dodatkowe dane o wilgotności oraz temperaturze w pomieszczeniu.Wady — takie jak każdego urządzenia Bluetooth (m.in zasięg).Komunikacja po ZigBeeMój ulubiony sposób komunikacji, bo najmniej zawodny oraz łatwo zwiększa się zasięg.Dodatkowo są to urządzenia, które bardzo długo wytrzymują na baterii.Urządzeń o tym sposobie komunikacji mam najwięcej, wiec zbiorę je w kilku grupach.Przekaźniki oraz gniazdkaPozwoliłem sobie zebrać te wszystkie urządzenia w jedną grupę, bo ich głównym przeznaczeniem jest włączenie/wyłączenie zasilania. Różnią się jedynie wspieranymi funkcjami. Do których można zaliczyć wybór typu klawisza (w przypadku przekaźników), liczbę i rodzaj raportowanych danych. Wybrane informują o poborze energii, temperaturze, a także pozwalają ustawić stan po powrocie zasilania. Warto zwrócić też uwagę na to, aby mieściły się w europejskich puszkach (jeśli są to przekaźniki).Tak jak wcześniej już wspomniałem, zaletą tych urządzeń jest to, że większość z nich wzmacnia zasięg sieci ZigBee.Sterowniki taśm ledSterowniki taśm led, czyli urządzenia, które pozwalają nam na wybór koloru oraz jasności.W swoim Smart Home mam tylko jedno takie urządzenie, które działa prawie bez zastrzeżeń.Jedyną wadą jest to, że po powrocie zasilania jest domyślnie włączone i musiałem to „ograć” dodatkowym przekaźnikiem i automatyzacją.Z zalet — podłączamy dowolną taśmę LED oraz dowolny zasilacz (oczywiście dobieramy urządzenia w zależności od potrzebnej mocy i napięcia).W moim wypadku jest to taśma LED zasilona z 24V na dwóch końcach (dla równomiernego podświetlenia, w przypadku większych długości).PrzyciskiW przypadku przycisków podobnie jak i w przypadku przekaźników jest w czym przebierać.Szczególnie lubię te najmniejsze urządzenia z dwoma przyciskami, pochodzące z rodziny IKEA Tradfri. Są dosyć tanie, poza krótkim przyciśnięciem obsługują także te długie, więc możemy wykorzystać je nawet do ściemniania i rozjaśniania.W moim przypadku z ich pomocą dołożyłem obsługę głównego oświetlenia kuchennego w bardziej wygodnym miejscu — pod okap.Przyciski mają wbudowany magnes, więc nie było problemu z ich przymocowaniem w tym miejscu.Zanim odkryłem te przyciski, miałem kostkę Aqara Cube. Dosyć fajny gadżet z dużą liczbą zdarzeń.Przewrócenie o 90°, 180°, obrót ze wskazaniem kąta, podwójne uderzenie.Jednak jest dużo mniej intuicyjny od standardowych przycisków i trzeba wiedzieć, co kryje się pod każdą z akcji na kostce.Czujniki ruchuW moich automatyzacjach używam jedynie czujników ruchu od Xiami/Aqara.Mogę o nich powiedzieć, że są dosyć małe, dlatego łatwo je wpasować je w każde pomieszczenie.Mogą się jednak nie sprawdzić w domyślnej konfiguracji, zwłaszcza we wszystkich automatyzacjach, ponieważ raportują swój stan co około minutę-półtora.Na szczęście istnieją poradniki, jak można łatwo wprowadzić je w tryb testowy, gdzie będą raportować wykryty ruch co 5 sekund.Korzystam z takiego rozwiązania przy czujniku, który zapala mi światło w przedpokoju. Hack działa już od kilku miesięcy, bez widocznego wpływu na baterię.W kwestii modyfikacji mogę dodać, że pierwszy czujnik po takiej zmianie działał 2 dni, po czym z nieznanych przyczyn przestał.Być może przegrzałem go przy lutowaniu, a być może po prostu „nastał jego czas”. Link do modyfikacji: https://community.smartthings.com/t/making-xiaomi-motion-sensor-a-super-motion-sensor/139806Sensory otwartych okien/drzwiPodobnie jak w przypadku czujników ruchu, tak i tutaj używam czujników tylko jednego rodzaju. Szukałem jak najmniejszych, żeby nie rzucały się w oczy i po analizie kilku wybór padł na te od Aqara.Ciężko powiedzieć o nich coś więcej niż to, że dobrze spełniają swoją funkcję.Zasięg między magnesem a czujnikiem może się wahać, w zależności od tego jak ustawimy urządzenia, skierowane do siebie frontem czy bokiem.Mam urządzenia umieszczone w obydwu wariantach. Odległość między nimi zazwyczaj wynosi około 1 cm i nie zaobserwowałem jeszcze żadnych problemów.Głowice termostatyczneTemat głowic termostatycznych może być nieco bardziej zawiły, ponieważ mamy sporo wariantów do wyboru. Ja wybrałem Moes BRT-100-TRV. Przy wyborze kierowałem się liczbą wystawianych informacji oraz zapewnieniem producenta, że działają na baterii dłużej niż inne warianty.Szczegółowe informacje o wystawianych danych dostępne są na stronach projektu Zigbee2MQTT.Jedna z moich głowic raportuje stan baterii 91%, druga 83%. Obydwie działają od listopada, a w momencie sprawdzania tego stanu mieliśmy połowę stycznia.W kwestii baterii mogę dodać tylko, że ze względu na różnice w napięciu między bateriami a akumulatorami (1.2V vs 1.5V), konieczne może być używanie zwykłych baterii.Czujniki wycieku wodyKonstrukcyjnie dosyć proste urządzenia. Na ich spodzie znajdują się dwie elektrody, których zwarcie powoduje zmianę stanu czujnika.W przypadku czujników Xiaomi/Aqara elektrody możemy wykręcić, zmniejszając tym samym poziom, od którego reagują.Najstarszy z moich czujników ma już dwa lata, a ciągle raportuje stan baterii na 100%, więc bez obaw można umieszczać je w trudno dostępnych miejscach.Czujniki temperaturyJedne z moich pierwszych czujników, a jak wspominałem, zaczynałem z bramką od Xiaomi, stąd wybór też tego producenta.Dużą zaletą kwadratowych czujników temperatury Xiaomi jest to, że poza temperaturą oraz wilgotnością, raportują też ciśnienie atmosferyczne.Bezpieczeństwo i prywatnośćW mojej ocenie w całej zabawie w Smart Home warto zwrócić uwagę na prywatność oraz bezpieczeństwo.Nie wyobrażam sobie trzymać w sieci urządzeń, które zbierają dane i wysyłają je na azjatyckie, niesprawdzone serwery.W sieci można znaleźć wiele przykładów kamerek, które albo miały słabe zabezpieczenia, albo luki w bezpieczeństwie i pozwalały każdemu na zdalny dostęp.Dla mnie podstawowym wymaganiem przy tego typu rozwiązaniach jest praca w sieci lokalnej.Wszystkie systemy i narzędzia dla Smart Home w moim domu mają wycięty dostęp do internetu, a pozwalam na niego jedynie przy okresowych aktualizacjach.W przyszłości zastanawiam się nad poluźnieniem tej restrykcji dla wybranych serwerów i urządzeń, ponieważ koliduje to nieco z integracją od pralki LG oraz zmywarki Bosch.Aktualnie, jeśli mam potrzebę dostępu z zewnątrz, to zapewniam sobie ten dostęp sam. Zawsze za pomocą rozwiązań, które uważam za godne zaufania. Przykładem takiego dostępu jest VPN, który sam wystawiam oraz integracja HomeKit od iOS.Zdalny dostęp od Apple (przez HomeKit) dostałem tak naprawdę „out of the box”. W domu mam zawsze tablet od Apple i tak długo, jak jest naładowany, pozwala innym domownikom z urządzeniami tego samego producenta na zdalny dostęp,spinając całość przez chmurę Apple.To na tyle…Zabawa w SmartHome nie należy do najtańszych i najłatwiejszych. Mam jednak nadzieję, że lektura tego artykułu nieco przybliży Wam temat inteligentnego domu dla programisty.To już Wasza decyzja, czy chcecie zająć się całością sami, czy może skorzystać z pomocy specjalistycznych firm.Jeśli zdecydujecie się na wybór jakieś firmy, to ten artykuł nakieruje Was na możliwe opcje, tak aby nie żałować wydanych pieniędzy.",
"url": "/2022/02/15/smarthome.html",
"author": "Adrian Swarcewicz",
"authorUrl": "/authors/aswarcewicz.html",
"image": "aswarcewicz.jpg",
"highlight": "/assets/img/posts/2022-02-05-smarthome/smarthome.jpg",
"date": "15-02-2022",
"path": "pl/2022-01-11-smarthome"
}
,


"2022-01-25-transactions-html": {
"title": "Zarządzanie transakcjami w Java - jak to robić dobrze?",
"lang": "pl",
"tags": "transactions spring java transactional",
"content": "Czy ktokolwiek zamawiając w restauracji kawę i ciastka chciałby zapłacić za oba, a otrzymać tylko jedno? Czy do zaakceptowania byłby fakt, że po wysłaniu przelewu na pokaźną kwotę, uszczuplony zostałby jedynie nasz rachunek, a kwota nie zasiliłaby konta odbiorcy? Odpowiedzi na te pytania są oczywiste. Pewne rzeczy mają sens tylko wtedy, gdy wykonywane są kompleksowo. Podobnie jest z systemem zarządzania transakcjami w Javie, a podane przykłady świetnie ilustrują zależności, o jakich musimy pamiętać przy tym temacie.W tym wpisie przedstawię krótkie wprowadzanie, ponieważ temat transakcji w programowaniu jest na tyle obszerny, że warto go podzielić na 2 części. W części pierwszej skupię się na przybliżeniu samej definicji oraz zasady działania transakcyjności w systemach baz danych oraz wykorzystaniu adnotacji @Transactional w Springu. Druga część poświęcona będzie problemom jakie napotkaliśmy w codziennej pracy, co pozwoliło je zidentyfikować oraz jak zostały wyeliminowane.TransakcjaPewien zbiór operacji, które muszą zostać wykonane kompleksowo (wszystkie lub żadna), nazywamy właśnie transakcją. Odniesienie do operacji bankowej o takiej samej nazwie jest nieprzypadkowe, a wspomniany już przykład przelewu jest modelowym wzorem transakcji. Wykonując przelew w banku pieniądze muszą jedocześnie zostać odjęte z rachunku nadawcy, jak i dodane w tej samej kwocie do salda konta odbiorcy. Tylko wtedy transakcja kończy się sukcesem.W przypadku niepowodzenia, któregokolwiek z kroków, wykonane dotąd operacje muszą zostać cofnięte, a stan systemu przywrócony do chwili sprzed rozpoczęcia całej procedury.Transakcje cechują się pewnym zbiorem zasad, określanych mianem ACID. Jest to akronim 4 właściwości (konkretnie ich angielskich nazw), którymi charaktryzuje się transakcja:      A (ang. atomicity) – atomowość – transakcja może zostać wykonana w całości albo anulowana. Jeżeli w trakcie wykonywania kroku objętego transakcją wystąpi problem, to wykonane zmiany powinny zostać cofnięte (rollback). Jeżeli wszystkie zakończone zostałyby sukcesem, to transakcja jest akceptowana (commit).        C (ang. consistency) – spójność – mówiąc potocznie oznacza, że „baza jest w dobrym stanie”. Powracając do przykładu przelewu, spójność oznaczałaby, że konto, z którego wysyłane są środki, pozwala na dokonanie operacji. Po zakończeniu transakcji stan konta nie może być ujemny. Spójność jest zatem pewnym zestawem reguł i zasad, których należy przestrzegać.        I (ang. isolation) – izolacja – właściwość, która odseparowuje od siebie operacje wykonywane jednocześnie tak, aby te nie miały na siebie wpływu. Chodzi np. o sytuację, gdy odwołujemy się do tej samej tabeli w bazie. Można też powiedzieć, że izolacja zapewnia, że w przypadku wykonywania operacji w sposób równoległy, baza danych pozostanie w takim stanie, jakby operacje te zostały wykonane sekwencyjnie.        D (ang. durability) – trwałość – gwarancja, że zaakceptowana transakcja nie zniknie niespodziewanie z systemu np. w przypadku awarii. Trwałość uzyskuje się poprzez zastosowanie mechanizmu replikacji, odnotowywania operacji w dzienniku logów czy poprzez zapis danych na dysku.  Poziomy izolacjiW praktyce wiele transakcji wykonywanych jest równolegle przez System Zarządzania Bazą Danych. Oznacza to często równloegły dostęp aplikacji do tych samych danych w bazie. Warto zatem wspomnieć o sposobie ustawiania blokad w dostępie do danych. Sposób ustawiania blokad określany jest mianem poziomu izolacji transakcji i pomaga w zachowaniu integralności danych. Poziomy izolacji określają zasady równoległej realizacji kilku transakcji.System Zarządania Bazą Danych (SZBD) może np. zablokować wiersz tabeli, dopóki nie zostaną zatwierdzone aktualizacje. Efektem takiej blokady byłoby uniemożliwienie użytkownikowi uzyskania brudnego odczytu  (ang. dirty read). Odczyt tego typu polega na dostępie do zaktualizowanej wartości, która nie została jeszcze zatwierdzona, i której istnieje możliwość przywrócenia do wartości poprzedniej.Niepowtarzalny odczyt (ang. non-repeatable read) ma miejsce wtedy, gdy transakcja A pobiera wiersz, transakcja B go aktualizuje, a następnie transakcja A pobiera ten sam wiersz ponownie. W takim przypadku transakcja A pobiera dwa razy ten sam wiersz, ale widzi różne dane.Odczyt fantomowy (ang. phantom read) to sytuacja, gdy transakcja A pobiera zbiór wierszy będących w danym stanie. Transakcja B następnie wstawia lub aktualizuje wiersz do tabeli. Następnie transakcja A ponownie odczytuję zbiór wierszy widząc tym razem nowy lub zaktualizowany wiersz, określany jako fantom.Zgodnie ze standardem SQL-92 definiuje się 4 poziomy izolacji transakcji. Poniżej widzicie “efekty uboczne”, które są akceptowalne na danym poziomie, poczynając od najmniej restrykcyjnego:            Poziom izolacji      Brudne      Niepowtarzalne      Fantomowe                     odczyty      odczyty      odczyty                  READ_UNCOMMITTED      Dozwolone      Dozwolone      Dozwolone              READ_COMMITTED      Uniemożliwione      Dozwolone      Dozwolone              REPEATABLE_READ      Uniemożliwione      Uniemożliwione      Dozwolone              SERIALIZABLE      Uniemożliwione      Uniemożliwione      Uniemożliwione      Należy pamiętać, że zastosowanie wyższego poziomu izolacji wiążę się ze spadkiem wydajności aplikacji. Poziomy izolacji pozwalają zatem na zachowanie równowagi między wydajnością, a wymaganą izolacją.Projektując aplikację trzeba pamiętać także o tym, że różne SZBD dostarczają różną liczbę poziomów izolacji. Dla przykładu w MySQL mamy 4 poziomy izolacji, podczas gdy w bazie Oracle poziomy izolacji są tylko 3.Zarządzanie transakcjami w JDBCKażdy, kto korzysta z mechanizmu transakcji w Springu wie, że pomocna w tym jest adnotacja @Transactional. Dzięki niej możemy zaoszczędzić trochę czasu, bo sami nie musimy implementować całego mechanizmu, którym zarządza framework. Warto jednak wiedzieć co dzieje się „pod maską”. Zobaczmy zatem jak wygląda uproszczone zarządzanie transakcją w JDBC, na którym zawsze bazujemy.// nawiązanie połączenia z bazą danychConnection connection = dataSource.getConnection();try (connection) {\t// Po utworzenia połączenia z bazą, połaczenie to jest w trybie tzw. „automatycznego zatwierdzania”. Oznacza to, że każda pojedyncza instrukcja SQL jest traktowana jako jedna transakcja i jest automatycznie zatwierdzana zaraz po jej wykonaniu. Ustawienie flagi autoCommit na false pozwala na wykonanie więcej niż jednej instrukcji w ramach jednej transakcji.\tconnection.setAutoCommit(false);\t// ...\t// operacje wchodzące w skład transakcji\t// ...\t// zatwierdzenie transakcji\tconnection.commit(); } catch (SQLException e) {\t// wycofanie zmian w przypadku błędów \tconnection.rollback(); }Punkty zapisu (ang. save points)Oprócz operacji zatwierdzania transakcji (COMMIT) oraz jej wycofywania w przypadku niepowodzenia (ROLLBACK),  istotna jest jeszcze jedna funkcja całego mechanizmu – punkty zapisu. Są to miejsca, do których nastąpi wycofanie transakcji, w przypadku awarii czy niepowodzenia. Posługując się fragmentem kodu, zastosowanie punktów zapisu, mogłoby wyglądać tak:// ustawienie poziomu izolacjiconnection.setTransactionIsolation(Connection.TRANSACTION_READ_UNCOMMITED);// instrukcje wykonywane w ramach transakcji – część 1// …// ustawienie punktu zapisu Savepoint savepoint = connection.setSavepoint();// instrukcje wykonywane w ramach transakcji – część 2 // … // przywrócenie zmian do stanu z punktu zapisuconnection.rollback(savePoint);Transakcje w SpringuO ile JDBC pozwala na zagregowanie wybranych instrukcji, jako odrębnych transakcji poprzez ręczne ustawienie flagi setAutocommit(false), o tyle Spring daje kilka sposobów osiągnięcia tego samego w znacznie wygodniejszy sposób.Programowalne zarządzanie transakcjąPierwsza, ale raczej rzadko wykorzystywana metoda to użycie TransactionTemplate albo PlatformTransactionManager. Sposób jej użycia wygląda następująco:@Servicepublic class BookService {\t@Autowired\tprivate TransactionTemplate template;\tpublic Long addBook(Book book) {\t\tLong id = template.execute(status → {\t\t\t// dodaj książkę do bazy i zwróć wygenerowany identyfikator \t\t\t// … \t\t\treturn id;\t\t});\t}}Zalety:  nie trzeba zaprzątać sobie głowy otwieraniem i zamykaniem połączenia z bazą danych poprzez stosowanie bloku try-catch-finally. Zamiast tego wykorzystywany jest mechanizm callbacków (Transaction Callbacks).  nie trzeba przechwytywać wyjątków SQLException, ponieważ Spring konwertuje je na wyjątki RuntimeException.  lepsza integracja z ekosystemem Springa. TransactionTemplate wykorzystuje TransactionManager do konfiguracji połączenie z bazą danych. Ich implementacja wiążę się z koniecznością utworzenia odpowiednich beanów, jednak później nie musimy się już martwić o zarządzanie nimi.Deklaratywne zarządzanie transakcjąSpring daje też możliwość skorzystania z adnotacji @Transactional. Można ją umieścić zarówno nad definicją klasy, jak i publicznej metody. W pierwszym przypadku aplikacja zachowa się tak, jakby wszystkie publiczne metody wewnątrz klasy posiadały taką adnotację. Dzięki samej adnotacji, każda publiczna metoda, która zostanie oznaczona tą adnotacją, zostanie wykonana wewnątrz transakcji.Mając już wiedzę w jaki sposób wygląda zarządzanie transakcją w JDBC, zobaczmy jak łatwo przekształcić kod tak, aby wykorzystywał dobrodziejstwo frameworka Spring:Na początek spójrzmy ponownie na przykładowy kod z sekcji Programowalne zarządzanie transakcją i zobaczmy jak wyglądałoby dodanie nowej książki do bazy przy użyciu JDBC.Następnie przyjrzyjmy się użyciu adnotacji @Transactional w Springu:public class BookService {\t@Transactional\tpublic Long addBook(Book book) {\t\t// dodaj książkę do bazy i zwróć wygenerowany identyfikator \t\t// bookDao.save(book);\t\treturn id;\t}}Wygląda znacznie prościej, prawda? Aby skorzystać z adnotacji @Transactional, konieczne jest jedynie zdefiniowanie menadżera transakcji w konfiguracji Spring’a oraz dodanie adnotacji @EnableTransactionManagement (w Spring Boot nie jest to konieczne).@Configuration@EnableTransactionManagementpublic class MyConfig {    @Bean    public PlatformTransactionManager transactionManager() {        return transactionManager;    }}Jak działa adnotacja @TransactionalSpring pozwala na wykorzystanie usługi BookService w każdym innym beanie, który tego wymaga. Stosując adnotację @Transactional i odwołując się do metody, która jest nią opatrzona, Spring nie odwołuje się bezpośrednio do tej metody, ale tworzy tzw. proxy transakcyjne. Dzieje się to przy pomocy biblioteki Cglib i metody zwanej proxy-through-sublcassing. Zadaniem tego proxy jest zawsze:  otwieranie i zamykanie połączeń/transakcji z bazą danych  delegowanie zadania do rzeczywistej usługi, jak nasze BookServiceCałą procedurę można zobrazować prostym diagramem:Dodając adnotację @Transactional do naszej metody lub klasy, ale nie definiując ręcznie poziomu izolacji musimy zawsze pamiętać, że Spring ustawia ten poziom na domyślny. Co to oznacza? Otóż, w rezultacie, kiedy Spring tworzy transakcję, poziom izolacji będzie domyślną izolacją SZBD, z którego korzystamy. Niezwykle ważne jest zatem zachowanie ostrożności zarówno w momencie wyboru Systemu Zarządzania Bazą Danych, jak i jego późniejszej zmiany.Żeby samodzielnie określić poziom izolacji transakcji wystarczy wykorzystać atrybut adnotacji @Transational o nazwie isolation, np.:    @Transactional(isolation = Isolation.REPEATABLE_READ)    public void addBook() {//      ...    }Poziomy propagacji (ang. propagation levels)Spring pozwala na jeszcze jedną konfigurację mechanizmu zarządzania transakcjami. Daje bowiem możliwość ustawienia tzw. poziomów propagacji, które pozwalają niejako rozgraniczyć logikę biznesową w naszej aplikacji. Spring pozwala na wykorzystanie następujących rodzajów propagacji:  REQUIRED (domyślna) – Spring sprawdza czy istnieje aktywna transakcja, i jeżeli nie, tworzy ją. W przeciwnym razie logika biznesowa zostaje wykonana w ramach istniejącej transakcji.  SUPPORTS – Spring najpierw sprawdza czy istnieje aktywna transakcja. Jeżeli tak, wtedy jest ona wykorzystywana do wykonania logiki biznesowej, w przeciwnym razie logika wykonana jest poza transakcją.  MANDATORY – Podobnie, jak w przypadku SUPPORTS, Spring najpierw poszukuje aktywnej transakcji, i jeżeli ją znajdzie, to ją wykorzystuje. W przeciwnym razie rzucany jest wyjątek.  NEVER – Spring rzuca wyjątek w przypadku wykrycia aktywnej transakcji.  NOT_SUPPORTED – Jeżeli istenieje aktywna transakcja, Spring przerywa ją, a następnie logika biznesowa wykonywana jest poza transakcją.  REQUIRES_NEW – Podobnie, jak w przypadku NOT_SUPPORTED, Spring przerywa aktywną transakcję, lecz tym razem tworzy nową na potrzebę wykonania logiki biznesowej w odrębnej transakcji.  NESTED – Spring sprawdza czy istnieje aktywna transakcja. Jeżeli tak, to utworzony zostaje punkt zapisu. Oznacza to, że jeżeli wykonana następnie logika biznesowa zakończy się błędem, to stan systemu zostanie przywrócony do punktu zapisu. Jeżeli natomiast nie ma aktywnej transakcji, to system zachowa się tak, jak w przypadku REQUIRED.Wybór metody zarządzania transakcjami w springuMając do dyspozycji dwie metody zarządzania transakcjami - programową i deklaratywną warto wiedzieć, kiedy stosować każdą z nich.Programowe zarządzanie transakcjami:  pozwala na “zakodowanie” logiki transakcji między logiką biznesową  jest elastyczne, ale trudne w utrzymaniu z dużą ilością logiki biznesowej  preferowane, gdy stosowana jest względnie mała logika obsługi transakcjiDeklaratywne zarządzanie transakcjami:  pozwala na zarządzanie transakcjami poprzez konfigurację  oznacza oddzielenie logiki transakcji od logiki biznesowej  używa się adnotacji (lub XML) do zarządzania transakcjami  łatwe w utrzymaniu - boilerplate trzymany jest z dala od logiki biznesowej  preferowane podczas pracy z dużą ilością logiki obsługi transakcjiPułapkaNa zakończenie warto wspomnieć o częstym błędzie popełnianym głównie na początku przygody z transakcjami. Zdarza się, że adnotacją @Transactional oznacza się powiązane ze sobą metody w tej samej klasie oczekując, że te wykonane zostaną w ramach odrębnych transakcji. W takim przypadku należy spojrzeć ponownie na powyższy diagram. Spring tworzy proxy transakcyjne, ale w momencie, gdy znajdziemy się już w rzeczywistym beanie BookService i wywołamy inne metody w nim zaimplementowane, żadne nowe proxy nie będzie zaangażowane w te operacje. Innymi słowy, nie zostanie rozpoczęta żadna nowa transakcja.Spójrzmy na poniższy kod, który tworzy streszczenie wybranej książki, następnie zapisuje je w bazie i wysyła e-mail. Mimo oznaczenia obydwu metod adnotacją @Transactional, zgodnie z powyższym wyjaśnieniem, otwarta zostanie jedynie pojedyncza transakcja, nawet gdyby druga metoda jawnie wskazywała na konieczność utworzenia nowej transakcji poprzez ustawienie odpowiedniego poziomu propagacji.@Controllerpublic class BookService {    @Autowired    private BookService bookService;//  ...}@Servicepublic class BookService {    @Transactional    public void sendBookSummary() {        createBookSummary();        // zapisz streszczenie w bazie        // wyślij streszczenie mailem, itd.    }    @Transactional(propagation = Propagation.REQUIRES_NEW)    public void createBookSummary() {        // ...    }}Jak sobie z tym poradzić? Wystarczy np. odseparować logikę biznesową tworząc dodatkową warstwę i dedykowane serwisy.@Servicepublic class BookService {        private BookSummaryService bookSummaryService;    private MailService mailService;        public BookService(BookSummaryService bookSummaryService,                       MailService mailService) {        this.bookSummaryService = bookSummaryService;        this.mailservice = mailService;    }        public void sendBookSummary() {        bookSummaryService.createBookSummary();        // zapisz streszczenie w bazie        mailService.send();        // itd.    }}Mamy wtedy dedykowany serwis do tworzenia streszczenia książki:@Servicepublic class BookSummaryService {//  ...         @Transactional    public void createBookSummary() {        // ...    }}I np. serwis odpowiedzialny za wysyłkę streszczenia mailem:@Servicepublic class MailService {//  ...         @Transactional    public void send() {        // wyślij streszczenie mailem    }}PodsumowanieTeoretyczne wprowadzenie do systemu zarządzania transakcjami w Javie już za nami! Na początku dowiedzieliśmy się czym w ogóle są transakcje, dlaczego są ważne i co je cechuje. Następnie przedstawiony został mechanizm działania transakcji w JDBC po to, by na koniec zaprezentować uproszczenie implementacji tego mechanizmu w Springu. W kolejnym artykule z tej serii dowiemy się jakie problemy napotkaliśmy wykorzystując mechanizm zarządzania transakcjami, jak udało się go zlokalizować oraz na czym polegała poprawka.",
"url": "/2022/01/25/transactions.html",
"author": "Robert Mastalerek",
"authorUrl": "/authors/rmastalerek.html",
"image": "rmastalerek.webp",
"highlight": "/assets/img/posts/2022-01-25-transactions/transactions.jpg",
"date": "25-01-2022",
"path": "pl/2022-01-25-transactions"
}
,


"2022-01-17-http-idempotency-html": {
"title": "Czy wiesz, czym jest HTTP idempotency?",
"lang": "pl",
"tags": "http idempotency",
"content": "Właściwość pewnych operacji, która umożliwia ich wielokrotne stosowanie bez zmiany wyniku, nazywamy trudnym polskim słowem ;) - idempotentność. Częściej spotkaliście się pewnie z angielskim tłumaczeniem - idempotency i też będziemy go tutaj używać.Twórcy różnego rodzaju systemów rozproszonych, bardzo chętnie korzystają z idempotentnych rozwiązań. Kiedy nie mamy pewności czy jakaś konkretna operacja doszła do skutku, łatwiej jest przecież powtórzyć jej wykonanie, zamiast ją weryfikować.HTTP idempotency, czyli Idempotentność w metodach HTTPJeśli przy projektowaniu API stosujemy się do zasad REST, otrzymujemy HTTP idempotency dla GET, PUT, DELETE, HEAD, OPTIONS i TRACE. Tylko interfejsy POST nie będą idempotentne.            Metoda HTTP      Idempotentność                  OPTIONS      tak              GET      tak              HEAD      tak              PUT      tak              POST      nie              DELETE      tak              TRACE      tak      W jaki sposób powyższe metody HTTP stają się idempotentne i dlaczego POST nią nie jest? Sprawdźcie konkretne metody i ich możliwości.POSTMetoda POST służy zazwyczaj do tworzenia nowego zasobu na serwerze, chociaż nie jest to reguła. Kiedy wywołamy to samo żądanie POST N razy, otrzymamy N nowych zasobów na serwerze. Tak więc POST nie jest idempotentny.GET, HEAD, OPTIONS oraz TRACEMetody GET, HEAD, OPTIONS i TRACE nie zmieniają stanu zasobów na serwerze. Służą one wyłącznie do pobierania reprezentacji zasobów lub metadanych w danym momencie. W związku z tym, wywoływanie wielu żądań nie będzie miało żadnej operacji zapisu na serwerze, co sprawia, że GET, HEAD, OPTIONS i TRACE są idempotentne.PUTNajczęściej do aktualizacji stanu zasobów używa się metody PUT. (I tutaj, tak jak w przypadku POST, nie musi to być to regułą). Jeśli wywołamy metodę PUT N razy, pierwsze żądanie zaktualizuje zasób. Wówczas reszta żądań N-1 będzie po prostu nadpisywać ten sam stan zasobów, raz za razem, niczego nie zmieniając. Dlatego właśnie PUT jest idempotentny.DELETEGdy z kolei zawołamy N podobnych żądań DELETE, pierwsze żądanie usunie zasób, a odpowiedź będzie wynosić 200 (OK) lub 204 (Brak treści). Inne żądania N-1 zwrócą 404 (Nie znaleziono). Oczywiście odpowiedź różni się od pierwszego żądania, ale nie dochodzi do zmiany stanu żadnego zasobu po stronie serwera, ponieważ pierwotny zasób został już usunięty. Tak więc DELETE jest idempotentny.",
"url": "/2022/01/17/http-idempotency.html",
"author": "Filip Philavong",
"authorUrl": "/authors/fphilavong.html",
"image": "fphilavong.jpg",
"highlight": "/assets/img/posts/2021-12-14-http-idempotency/http-idempotency.jpg",
"date": "17-01-2022",
"path": "pl/2021-12-14-http-idempotency"
}
,


"2021-12-16-frontend-developer-html": {
"title": "Frontend Developer",
"lang": "pl",
"tags": "frontend pogadajmy",
"content": "Ostatnio ktoś mnie zapytał o naszą ofertę na Frontend Developera słowami “To co, robicie stronki?”.Czy Frontend Developer w Consdata robi stronki? #takżetego 😀Korzystając z okazji, chciałbym przybliżyć trochę projekty, którymi się zajmujemy i opowiedzieć, jak wygląda przeciętny sprint frontend/fullstack developera. Przełom roku to dla wielu moment postanowień i zmian - może poniższy opis to impuls, którego właśnie szukasz? 😉Frontend developer, czyli kto?Czym właściwie zajmuje się frontend developer w Consdata? Jeżeli obstawiałeś, że praca na froncie to głównie pisanie CSSków to muszę Cię rozczarować (albo uspokoić, różnie ludzie na to patrzą 😉). Wielu naszych developerów może przez całe sprinty nie dotykać HTMLa czy CSSów, a nadal być pewnym, że robią zadania na froncie.Jakieś 15 lat temu świat był prosty. Jak się chciało mieć aplikację web to zapewne była to statycznie generowana strona HTML+CSS podparta backendem i jakąś formą budowania HTMLa po stronie serwera. Wtedy większość z nas ignorowała cały obszar frontu, zostawiając to komuś kto jest “gdzieś” i zajmuje się takimi rzeczami. Dobrze było, jak ktoś od frontu był w ogóle na miejscu, równie często mogła być to osoba spoza firmy wskakująca na praktycznie gotowy fragment systemu. Powiedzmy, że “ktoś później wpadnie zrobić CSSki” 😉Kolejne kilka lat i okazało się, że świat nie kończy się na użytkownikach oglądających białe ekrany w oczekiwaniu na załadowanie kolejnej strony. Część z nas odkryła, że w przeglądarce da się wykonać kod i istnieje coś takiego jak JavaScript. Oczywiście na naukę nowych rzeczy szkoda nam było czasu, a JavaScript coś podobnie brzmi jak Java i radośnie wskoczyliśmy w GWT. Niby to fronty ale jednak w większości programiści Java nie specjalnie przejmowali się czy to Swing, czy GWT, w końcu autorzy postarali się o zbliżone API i wszystko było po staremu. Frontendowiec nadal był gdzieś obok, tylko miał trudniej. Trudniej uruchomić, trudniej ogarnąć strukturę, trudniej dopisać wstawkę w JSie. Nadal można było funkcjonować w modelu “ktoś później wpadnie zrobić CSSki” 😉Zapewne domyślasz się jaki jest efekt takiego podziału? Backend developerzy produkują coś, co nie jest do końca tym czym miało być, a frontend developerzy radzą sobie z tym jak mogą, osiągając jedynie fragment jakości potencjalnego rozwiązania. Brak komunikacji, zrozumienia potrzeb i ograniczeń “drugiej strony” wprost prowadzi do systemów niskiej jakości. Z obu stron “coś wyszło” i jakoś trzeba to zgrać w ramach QA 😉To co, do trzech razy sztuka? Za trzecim podejściem w końcu pogodziliśmy się, że z tematem należy się zmierzyć i nie taki diabeł straszny jak go malują. Obecnie aplikacje webowe mogą mieć znacznie więcej kodu niż wspierające je usługi. Od rozbudowanego backendu ze szczątkowym UI dochodzimy do miejsca, w którym to UI jest rozbudowany, a usługi są upraszczane i służą jedynie zaspokojeniu potrzeb frontu.Złożoność aplikacji doprowadziła nawet do dalszego rozróżniania specjalizacji!  programista (java/type/coffe/…)script, frontend developer,  programista ui,Tak, to te stanowiska potocznie nazywane frontendem i backendem frontendu. Dinozaury z serwerowymi korzeniami nie odpuszczają łatwo swoich nazw stanowisk 😉Skuteczny frontend developer łączy nie tylko te dwie funkcje, ale też potrafi sięgnąć głębiej i pracować z backendem. Takie holistyczne podejście pozwala mu swobodnie podejmować decyzje dążące do optymalnej realizacji oraz świadomie wybierać konkretne rozwiązania, znając ich konsekwencje i zalety. Dzięki temu swobodnie żongluje technikami backendu i frontendu, żeby osiągnąć zamierzony efekt i nigdy nie da sobie powiedzieć “tak musi być” czy “nie da się inaczej”. Mimo lekkiego tonu, warto jednak zapamiętać tę myśl, bo zdarza się, że znając tylko jedną stronę równania dążymy do rozwiązywania wszystkiego znanymi sobie narzędziami. Tak jak ten cieśla, który mając młotek wszędzie widzi gwoździe, tak i nam zdarzyło się wszędzie widzieć fabryki obiektów zamiast po prostu zmienić interfejs DTO 😉Odpowiedzialności Frontend developera w świecie Full-StackówCzyli muszę znać UI, architekturę aplikacji i jeszcze backend? Przecież mamy też full-stacków, to jaka jest różnica?Bycie full-stackiem oznacza bycie wszechstronnym i zdolnym do podjęcia wszystkich wyzwań stawianych przed zespołem (w uproszczeniu, bo taki wątek poboczny zdominowałby cały wpis, a już byłem gościem podcastu na ten temat 🙂).To znaczy, że w zależności od specjalizacji i zainteresowań, full-stack swobodnie porusza się we froncie w zakresie pracy, nazwijmy to, użytkowej. Umiejętność rozwoju zastanego kodu, realizacja tematów o znanym sposobie rozwiązania czy okazjonalne rozwiązywanie problemów z gatunku x-files, o ile szalenie ważna dla samodzielności zespołu, nadal nie zapewnia kompletu kompetencji.Frontend developer to osoba specjalizująca się w tworzeniu aplikacji web. Potrafi wyznaczyć długoterminową roadmapę rozwoju, określić kierunek technologiczny i dobrać najlepsze rozwiązania do napotkanych problemów. W gestii frontend developera jest dbanie o ciągły rozwój technologiczny, budowanie rozwiązań referencyjnych i wskazywanie innym sposobu pracy tam, gdzie kończą się znane przykłady i rozwiązania.To, czego możemy nauczyć się z krótkiej historii frontendu z początku artykułu to to, że silne rozdzielanie odpowiedzialności i stawianie jasnej granicy między frontend a backend developerami nie jest najlepszym pomysłem. Dlatego frontend-developer w Consdata w praktyce jest stanowiskiem zbieżnym z full-stackiem, jednak o nacisku położonym na część “ponad RESTem”. Tak jak od full-stacka oczekujemy, że poradzi sobie z typowymi tematami frontu, tak samo liczymy, że frontendowiec nie będzie się czuł zagubiony przy przeciętnych zagadnieniach backendu.Zrozumienie obu stron problemu jest kluczowe w zapobieganiu przerzucania się “u mnie działa” (ew. unikania sytuacji “szach mat frontendowcy” ;)).Ok, nie CSSki, to co?Padło już wiele okrągłych zdań próbujących opisowo wytłumaczyć kontekst i motywację, pora zatem na kilka reprezentatywnych przykładów typowych zadań.  Rozbudowa aplikacji o współdzielony pomiędzy komponentami stan:          integracja biblioteki NgRx z aplikacją Angular,      przygotowanie konfiguracji lokalnej i produkcyjnej,      referencyjna implementacja obsługi stanu w jednym z modułów funkcjonalnych,      wypracowanie utili i wytycznych określających zalecany sposób implementacji i dobre praktyki,      przedstawienie implementacji zespołom i wskazanie materiałów poszerzających wiedzę.        Wsparcie równoległej pracy wielu użytkowników nad pojedynczym elementem domeny:          zbudowanie mechanizmu wprowadzania zmian sterowanego zdarzeniami,      przygotowanie synchronizacji delt zmian klient-serwer,      podsystemu redukowania zdarzeń do projekcji bieżącego stanu po stronie aplikacji klienckiej,      propagowanie wiedzy i edukacja zespołu w zakresie wykorzystania rozwiązania.        Możliwość rozbudowy aplikacji rozszerzeniami funkcjonalnymi dostarczanymi przez klienta w runtime:          określenie api i specyfikacji osadzania webcomponentów,      implementacja ładowania i obsługi cyklu życia komponentów klienta,      rekomendacje i współpraca w budowaniu gateway dla webcomponentów z uwzlędnieniem modelu uprawnień,      przygotowanie przykładów komponentów w oparciu o LitElement,      eksponowanie elementów UI aplikacji nadrzędnej jako webcomponenty dostępne dla komponentów rozszerzeń.        Budowanie kreatora wieloetapowej akceptacji operacji asynchronicznych:          przygotowanie generycznego mechanizmu wieloetapowej akceptacji na wiele rąk,      referencyjna implementacja funkcjonalności potwierdzania paczek transakcji,      uogólnienie kodu na potrzeby przyszłych funkcjonalności i przygotowanie funkcjonalnych prototypów Storybook.        Komponent tworzenia wiadomości oferujący funkcjonalność formatowania treści:          opracowanie komponentu edytora WYSIWYG z przykładami w UI guideline,      wykorzystanie komponentu na ekranie tworzenia wiadomości,      integracja wysyłki z istniejącą usługą REST z uwzględnieniem praktyk UI/UX i optymistycznego potwierdzania operacji.      I tak dalej. To tylko kilka z przykładów kompetencji i umiejętności których szukamy u osób specjalizujących się w obszarze Frontend Developer.Czym można się zająć w Consdata?W Consdata, w zależności od zespołu, możesz pracować przy jednej lub kilku z naszych kluczowych aplikacji web. W kolejnych punktach przejrzymy razem co, i w jakiej skali, budujemy dla naszych klientów.W zależności od wybranego projektu, pojawiające się w nim wyzwania będą bliżej domeny lub stosowanej technologii.Projekty Consdata z uwzględnieniem charakterystyki nacisku na domenę lub technologięiBiznesiBiznes to złożony system bankowości elektronicznej do obsługi klienta korporacyjnego. Głównym wyznacznikiem projektu jest jego złożoność biznesowa.Mimo że dziedzina stanowi główne wyzwanie, to jednak nie brakuje w nim wyzwań technologicznych, takich jak np. wielomodułowa architektura zbudowana z myślą o niezależnych wdrożeniach obszarów funkcjonalnych, równoczesna praca wielu klientów nad pojedynczymi obiektami domeny czy optymalizacja z badaniami UX/UI i dbanie o czytelność złożonych biznesowo operacji.Jeżeli szukasz wyzwania w projekcie, który skupia się na domenie i jego złożoność leży w implementowanych funkcjonalnościach, to iBiznes jest właśnie dla Ciebie!iBiznes w liczbach to ~300k linii kodu TypeScript i ~30k linii SASS samego frontendu napisanego w Angularze!MailboxSkłaniasz się bardziej w stronę cutting edge technologii, o których tyle słyszysz na konferencjach? Twój wzrok powinien paść na projekty Mailbox. Nie da się mówić o Mailbox nie mówiąc o skali! Codzienna komunikacja z milionami klientów uczy nowego spojrzenia na nawet najprostsze problemy (zdziwiłbyś się ile daje optymalizacja aktualizacji licznika nieprzeczytanych wiadomości o, w praktyce, nieograniczonej liczbie użytkowników).Udział w projekcie Mailbox oferuje zrównoważoną dawkę złożoności biznesowej i wyzwań technologicznych.Dodatkowym atutem jest możliwość pochwalenia się znajomym i rodzinie co się ostatnio robiło, wystarczy, że są klientami jednego z naszych klientów, a że klienci duzi to i szansa na bycie ich klientem spora 😉~45k linii TypeScript z ~8k linii SASS stojących na froncie przed klastrem Kafki i Solara brzmi kusząco? Mailbox jest dla Ciebie!EximeePraca z domeną Cię nie kręci? Zawsze od tematów biznesowych bardziej kręciły Cię wyzwania czysto technologiczne?Może warto spojrzeć na nasz system obsługi wniosków i procesów? Eximee na każdym kroku projektowane jest jako framework, z którego nasi klienci korzystają budując własne rozwiązania biznesowe.W Eximee nigdy nie ma jednego słusznego rozwiązania problemu, wszystko jest warunkowe, “zależy” i “oczywiście, że można zrobić inaczej”. Podstawą pracy jest, z jednej stronie naginanie tego co założyli twórcy, a z drugiej projektowanie systemu podatnego na naginanie. Skala systemu leży w jego generyczności i rozszerzalności. Samo Eximee jest uruchomione w kilkunastu wariantach wdrożeniowych u różnych klientów, często w wielu kanałach równolegle!Wszystko to sprawia, że Eximee to głównie wyzwanie technologiczne i poligon nowych rozwiązań testowanych u klientów w publicznie dostępnych systemach!Ale to nie wszystko, w projekcie budujemy też narzędzia graficznego projektowania procesów i wniosków, obsługi backoffice i inne. Poznaj potrzeby systemu spełniającego oczekiwania od klienta, przez obsługę backoffice, po pracę programistów oraz projektantów tworzących biznesowe rozwiązania oparte o dostarczaną przez Ciebie platformę.Twoi znajomi pewnie już korzystali z systemu który tworzysz, zapewne nie raz 😉Być może Eximee z ~250k linii TypeScript i ~130k linii SASS frameworku to coś dla Ciebie.Consdata OSSGdy nabierzesz ochoty na eksperymenty, prototypu i ogólnie luźniejsze podejście, zawsze możesz zaplanować swój czas na rozwój lub budżet chapterowy na wsparcie jednego z naszych projektów open source skupionych na GitHub: https://github.com/orgs/Consdata/repositories.Tylko z nazwy warto wspomnieć o:  kouncil - ui do Kafki zbudowany na bazie doświadczeń naszych i naszych klientów,  sonarqube-companion - agregat naruszeń SonarQube rozumiejący strukturę zespołów i śledzący trendy analizy statycznej,  inne narzędzia jak logger, aplikacje intranetowe do newsletterów, rezerwacji parkingu, czy śledzenia feedbacku i zbierania ankiet.W Open Source nie ograniczamy się żadnymi wytycznymi, to nasze poligony. Firebase? Angular czy React? Node czy Java? Spring czy Quarkus? Nie ma znaczenia! Zrealizuj funkcjonalność i powiedz innym jak poszło.Dołączysz?Czujesz, że frontend to może być miejsce dla Ciebie? Zainteresował Cię któryś z naszych projektów? Czujesz, że podołasz? Chcesz uczestniczyć w wyznaczaniu architektury i projektowaniu aplikacji web z szerokiego portoflio?Sprawdź nasze oferty, u nas każdy znajdzie coś dla siebie 😉Prowadzimy obecnie rekrutację na stanowisko Frontend Developer. Poszukujemy osób biegłych w tworzeniu aplikacji web, które pomogą nam wyznaczyć dalszy kierunek rozwoju i rozpędzić toczące się już zmiany w naszych aplikacjach. Kluczowa jest dla nas wiedza z poziomu projektowania aplikacji, wierzymy, że braki w tworzeniu komponentów czy jedynie ogólne wyczucie “tych backendów” to tematy, nad którymi możemy już pracować razem.Jeśli tylko czujesz się na siłach, a w backendzie chociaż “wiesz co piszczy”, to nie czekaj dłużej, daj mi się zaprosić na rozmowę rekrutacyjną ;)https://consdata.com/pl/kariera/trwajace-rekrutacje/poznan/senior-frontend-fullstack-developer-angularPS. Jeśli masz jakiekolwiek pytania - nie wahaj się znaleźć mnie LinkedIn i pisać w dowolnej sprawie.",
"url": "/2021/12/16/frontend-developer.html",
"author": "Grzegorz Lipecki",
"authorUrl": "/authors/glipecki.html",
"image": "glipecki.jpg",
"highlight": "/assets/img/posts/2021-12-17-frontend-developer/frontend.webp",
"date": "16-12-2021",
"path": "pl/2021-12-17-frontend-developer"
}
,


"2021-12-07-json-vs-jsonb-postgresql-html": {
"title": "Czy wiesz czym różnią się json i jsonb w PostgreSQL?",
"lang": "pl",
"tags": "PostgreSQL",
"content": "Podstawowe różnicePodstawową różnicą pomiędzy json a jsonb jest sposób ich przechowywania. Typ json jest przechowywany jako tekst, natomiast typ jsonb w postaci binarnej. Typ, na jaki się zdecydujemy, json czy jsonb, ma wpływ na kilka czynników:  zapis danych w postaci jsonb trwa dłużej,  operacje wykonywane na typie jsonb trwają krócej, gdyż nie trzeba ich dodatkowo parsować.Ze sposobu przechowywania wynikają kolejne różnice. Zobaczmy poniższy przykład:SELECT &#39;{&quot;c&quot;:0, &quot;a&quot;:2,&quot;a&quot;:1}&#39;::json, &#39;{&quot;c&quot;:0, &quot;a&quot;:2,&quot;a&quot;:1}&#39;::jsonb;          json          |        jsonb------------------------+--------------------- {&quot;c&quot;:0,   &quot;a&quot;:2,&quot;a&quot;:1} | {&quot;a&quot;: 1, &quot;c&quot;: 0}  Json zachowuje formatowanie - dlatego w przykładzie widać spacje przed kluczem a, natomiast jsonb nie zachowuje formatowania - usuwa whitespace’y.  Json pozwala dodawać powielone klucze (w przykładzie widać, że klucz “a” występuje 2 razy), natomiast jsonb zachowuje tylko ostatnią wartość.  Json zachowuje kolejność kluczy, natomiast jsonb przechowuje posortowane alfabetycznie klucze.  Jsonb przechowuje obiekty w postaci binarnej, natomiast json przechowuje dane jako tekst.  Jsonb pozwala zakładać indexy na kluczach.Operatory wspierane tylko przez jsonbW celu zobrazowania dodatkowych operatorów dla typu jsonb, zacznijmy od utworzenia prostej tabeli, którą wykorzystamy w przykładachCREATE TABLE orders (    id serial NOT NULL PRIMARY KEY,    data jsonb NOT NULL);oraz zainicjujmy ją prostymi danymi w formacie json.INSERT INTO orders (data) VALUES (&#39;{ &quot;customer&quot;: &quot;Anna Kowalczyk&quot;, &quot;items&quot;: {&quot;product&quot;: &quot;Pieluchy&quot;,&quot;qty&quot;: 24} }&#39;),    (&#39;{ &quot;customer&quot;: &quot;Zofia Wiśniewska&quot;, &quot;items&quot;: {&quot;product&quot;: &quot;Zabawka samochód&quot;,&quot;qty&quot;: 1} }&#39;),    (&#39;{ &quot;customer&quot;: &quot;Wojciech Nowak&quot;, &quot;items&quot;: {&quot;product&quot;: &quot;Klocki LEGO&quot;,&quot;qty&quot;: &quot;1&quot;}, &quot;regular_customer&quot;: true}&#39;),    (&#39;{ &quot;customer&quot;: &quot;Jan Kowalski&quot;, &quot;items&quot;: {&quot;product&quot;: &quot;Piwo&quot;,&quot;qty&quot;: 6}}&#39;);Operator @&amp;gt; pozwala na sprawdzenie, czy json zawiera na tym poziomie pole wraz z podaną wartością.SELECT (data -&amp;gt; &#39;items&#39;) @&amp;gt; &#39;{&quot;qty&quot;: 6}&#39; from orders;  contains----------fffOperator ? pozwala na sprawdzenie, czy json posiada określone pole. Przydatne, jeśli jsony nie trzymają się restrykcyjnie określonego schematu i obiekty mogą posiadać różne zestawy pól.SELECT data ? &#39;regular_customer&#39; from orders;  field_exists----------fftfPoza tym typ jsonb wspiera kilka operatorów, które pozwalają na skuteczną modyfikację jsonów trzymanych w bazie.  || - Pozwala złączyć ze sobą dwa obiekty typu jsonb w jeden nowy obiekt typu jsonb, przykładowo dodać nowe wartości do istniejącej tablicy.  -  - Pozwala na usunięcie pary klucz-wartość lub też określonego elementu z tablicy, przykładowo ‘[“a”, “b”]’::jsonb - 1. Zwróci to tablicę zawierającą wyłącznie element “a”.  #- - Pozwala na usunięcie pola lub elementu znajdującego się pod określoną ścieżką.Kiedy stosować typ json, a kiedy jsonb?Jeżeli potrzebujemy jedynie zapisywać i odczytywać dane w formacie json i dalsze operacje na tych danych nie będą konieczne, to możemy zastosować typ json.Kiedy jednak wykonujemy wiele operacji na jsonie albo potrzebujemy indexu na kluczu, wtedy lepszym wyborem będzie zastosowanie typu jsonb.W dokumentacji PostgreSQL znajdziemy rekomendację używania typu jsonb. Pamiętajmy jednak o sytuacjach, w których typ json może się sprawdzić lepiej. Dobrym przykładem są systemy legacy, w których polegamy na kolejności zapisu danych.Przeczytaj więcej tutaj:  https://www.postgresql.org/docs/9.5/datatype-json.html  https://www.compose.com/articles/faster-operations-with-the-jsonb-data-type-in-postgresql/",
"url": "/2021/12/07/json-vs-jsonb-postgresql.html",
"author": "Katarzyna Kur",
"authorUrl": "/authors/kkur.html",
"image": "kkur.png",
"highlight": "/assets/img/posts/2021-11-23-json-vs-jsonb-postgresql/json.jpg",
"date": "07-12-2021",
"path": "pl/2021-11-23-json-vs-jsonb-postgresql"
}
,


"2021-11-16-shadow-dom-html": {
"title": "Czy wiesz, czym jest Shadow DOM?",
"lang": "pl",
"tags": "javascript",
"content": "Pewnie czytając o Web Componentach dane było Ci słyszeć o Shadow DOM. Pozwala on na przyczepienie się do istniejących elementów DOM (które stają się Shadow Hostem) i wyjście z nich z nowym drzewem DOM.Jak korzystać z Shadow DOMAby to zrobić, potrzebujemy utworzyć uchwyt do elementu z DOM, np.:var host = document.getElementById(&quot;host&quot;);a następnie przypiąć do niego Shadow Root, czyli miejsce, z którego będziemy mogli wyjść z Shadow DOMem:var root = host.attachShadow({&#39;mode&#39;: &#39;open&#39;});mode pozwala ustalić, czy z zewnątrz Shadow DOM będziemy mogli mieć do niego dostęp. Może on przyjąć wartość open lub closed.Na sam koniec do Shadow Root przypinamy kawałek HTMLa:root.innerHTML = &quot;jestem w shadow DOM!&quot;;PrzykładCałość można zobaczyć na przykładzie:  See the Pen   Shadow DOM by Porkite (@Porkite)  on CodePen.Przykład ten pokazuje również, że z Shadow DOM można korzystać jako z samodzielnego feature, bez udziału Web Componentu.Co nam to wszystko daje?Umożliwia nam to pisać Web Componenty (lub jak widać na przykładzie - niezależne dodatkowe struktury DOM), które ukrywają swoją strukturę, aby ułatwić czytanie html utworzonej strony.Dzięki nim możemy też enkapsulować stylizację: style dla naszej aplikacji nie będą wpływać na nasze Shadow DOM-y, a style zadeklarowane w ich wnętrzu nie zmienią wyglądu tego, co znajduje się na zewnątrz.",
"url": "/2021/11/16/shadow-dom.html",
"author": "Piotr Grobelny",
"authorUrl": "/authors/pgrobelny.html",
"image": "pgrobelny.webp",
"highlight": "/assets/img/posts/2021-09-01-shadow-dom/shadow.webp",
"date": "16-11-2021",
"path": "pl/2021-09-01-shadow-dom"
}
,


"2021-11-08-bucket-pattern-html": {
"title": "Czy wiesz, jak działa Bucket Pattern w MongoDB i czemu sprawdził się w implementacji IoT Boscha albo w apkach bankowych?",
"lang": "pl",
"tags": "mongoDB",
"content": "Największe korzyści z używania odpowiedniego wzorca grupowania danych, czyli Bucket Pattern w MongoDB, to m.in. zwiększenie wydajności indeksów czy uproszczenie zapytań. Przeczytaj jak to wszystko zrealizować.Jaki problem próbujemy rozwiązać?Zacznijmy od prostego przykładu - pomiarów czujnika wykonywanych bardzo często, np. co sekundę, każdego dnia. Można by je zapisywać następująco:{   sensor_id: 12345,   timestamp: ISODate(&quot;2019-01-31T10:00:00.000Z&quot;),   temperature: 40} {   sensor_id: 12345,   timestamp: ISODate(&quot;2019-01-31T10:01:00.000Z&quot;),   temperature: 40} {   sensor_id: 12345,   timestamp: ISODate(&quot;2019-01-31T10:02:00.000Z&quot;),   temperature: 41}Oczywiste jest, że dokumentów w takiej kolekcji będzie bardzo dużo i w związku z tym, problemem może być szybki dostęp do tych danych. W celu zwiększenia wydajności zdecydujemy się zastosować indeksy, najprawdopodobniej na polach sensor_id i timestamp. Wówczas sam rozmiar indeksów stanie się sporym wyzwaniem w kontekście niezbędnej do tego pamięci.Na czym polega Bucket Pattern?Wzorzec polega na odpowiednim pogrupowaniu danych. Pamiętając o naszym przykładzie z czujnikami, możemy pogrupować dane z jednego czujnika w interesującym nas przedziałach czasu, np. przedziałach 1-godzinnych. Przejdziemy wówczas z modelu danych odpowiadającemu relacyjnemu podejściu na model z zagnieżdżonymi dokumentami.Te same dane wyglądałyby następująco:{    sensor_id: 12345,    start_date: ISODate(&quot;2019-01-31T10:00:00.000Z&quot;),    end_date: ISODate(&quot;2019-01-31T10:59:59.000Z&quot;),    measurements: [       {       timestamp: ISODate(&quot;2019-01-31T10:00:00.000Z&quot;),       temperature: 40       },       {       timestamp: ISODate(&quot;2019-01-31T10:01:00.000Z&quot;),       temperature: 40       },       ...       {       timestamp: ISODate(&quot;2019-01-31T10:42:00.000Z&quot;),       temperature: 42       }    ],   transaction_count: 42,   sum_temperature: 2413}Dokumentów w kolekcji będzie mniej i zwiększymy wydajność zapytań. Wiedząc, jakie będzie zastosowanie danych, możemy również dodać do naszego “wiaderka” dodatkowe informacje. Czy faktycznie potrzebujemy do większości zapytań każdego pojedynczego pomiaru? Być może ciekawszą informacją będzie średnia temperatura z godziny w danym miejscu? Wówczas możemy zapisać takie zagregowane dane w Bucket Pattern i uprościć późniejsze zapytania.W naszym przykładzie, jeśli dojdzie nowy pomiar z czujnika w tym zakresie, zwiększymy liczbę transaction_count i sum_temperature. Zapytanie o średnią temperaturę w danym dniu lub godzinie, będzie wtedy nawet prostsze niż gdybyśmy korzystali z pojedynczych pomiarów.I na koniec jeszcze jedna wskazówka. Dobrym pomysłem może się okazać zarchiwizowanie części danych historycznych. Dane spływają wówczas na bieżąco i wiemy, że konkretny dokument nie będzie później modyfikowany, a dostęp do starych danych może być niezwykle rzadki.Praktyczne przypadki użyciaTwórcy Mongo chwalą się, że takie zastosowania to nie tylko teoria. Bosch korzysta z Bucket Pattern w MongoDB w aplikacji z branży automotive, zbierając dane z wielu czujników w pojeździe. Również niektóre banki skorzystały z tego wzorca, grupując transakcje.Korzyści, o których warto pamiętać  redukcja liczby dokumentów w kolekcji,  zwiększenie wydajności indeksów,  uproszczenie zapytań dotyczących zagregowanych danych.Przeczytaj więcej tutaj:  https://developer.mongodb.com/how-to/bucket-pattern/  https://www.mongodb.com/customers/bosch",
"url": "/2021/11/08/bucket-pattern.html",
"author": "Barbara Mitan",
"authorUrl": "/authors/bmitan.html",
"image": "bmitan.jpg",
"highlight": "/assets/img/posts/2021-09-16-bucket-pattern/bucket.jpeg",
"date": "08-11-2021",
"path": "pl/2021-09-16-bucket-pattern"
}
,


"2021-10-13-zarzadzanie-stanem-aplikacji-frontendowej-html": {
"title": "Zarządzanie stanem aplikacji frontendowej na przykładzie NgRx",
"lang": "pl",
"tags": "frontend redux state management zarządzanie stanem store reducer action effect angular react",
"content": "W miarę rozwijania złożonych aplikacji webowych ważnym i nieoczywistym zagadnieniem staje się projektowanie przepływu informacji pomiędzy komponentami. Często mamy do czynienia z wieloma źródłami danych. Mogą to być na przykład najróżniejsze zewnętrzne serwisy czy interakcje użytkownika z systemem. Nierzadko dane z tych źródeł potrzebne są w różnych obszarach aplikacji, wpływają na wiele aspektów jej działania czy wyglądu. W związku z powyższym muszą zostać rozpropagowane do miejsc, w których zostaną użyte. Można się jednak zastanowić, czy “przepychanie” parametrów po całej aplikacji jest szczytem naszych możliwości. W przypadku rozbudowanych projektów jest to dość żmudne zadanie, a w efekcie bardzo szybko może zabałaganić nasz kod, pomieszać orkiestrację z prezentacją, nie wspominając już o wprowadzeniu trudności w testowaniu logiki opartej o przekazywane dane.Jeden z pomysłów adresujących między innymi powyższe problemy wygląda następująco:  stwórzmy dostępne tylko do odczytu, pojedyncze źródło danych wejściowych,  wyraźnie rozgraniczmy logikę aplikacji od jej prezentacji,  sprawmy, żeby dane wynikały z historii pewnych zdarzeń zachodzących w aplikacji,  nie przechowujmy danych, które mogą zostać wyliczone na podstawie już przechowywanych - ewaluujmy je w locie.Te i kilka innych koncepcji zebrano i wymyślono w ten sposób architekturę zwaną Redux. Mając na uwadze problem, który chcemy rozwiązać oraz ogólny zarys rozwiązania, postaram się przedstawić jej składowe oraz implementację w bibliotece NgRx.Czy na pewno potrzebujesz zarządzania stanem aplikacji?Zanim zaczniemy, zwróćmy uwagę, że nie każda aplikacja jest tak rozbudowana i złożona, żeby potrzebować całego mechanizmu zarządzania stanem. Jeśli “przepychanie parametrów” odbywa się pomiędzy niewielką liczbą komponentów, to nie musimy wytaczać przysłowiowych armat, żeby ustrzelić komara. W ustaleniu, czy dobrze byłoby sobie pomóc zewnętrznym rozwiązaniem, pomaga zasada SHARI zaprezentowana, chociażby, w oficjalnej dokumentacji NgRx. Warto sobie odpowiedzieć, czy potrzebujemy stanu, który jest:  Shared - współdzielony pomiędzy wiele komponentów i serwisów.  Hydrated - trwały i z możliwością ponownego zasilenia z zewnętrznego źródła jak np. local storage.  Available - dostępny cały czas, niezależnie od sposobu nawigacji po aplikacji, np. podczas przechodzenia i cofania się w aplikacji prezentującej złożony wniosek.  Retrieved - zdolny do przechowywania danych pochodzących z zewnętrznych źródeł, co pozwala na przykład zapisać wynik żądania HTTP i nie wykonywać go po raz kolejny w celu otrzymania tych samych informacji.  Impacted - zdolny do zmieniania się pod wpływem akcji wykonywanych przez komponenty i serwisy.Architektura ReduxSpójrzmy zatem co tak naprawdę kryje się pod tym, być może na razie dość enigmatycznym, pojęciem “Redux”. Posłużę się tutaj obrazkiem zaczerpniętym z dokumentacji NgRx.Architektura Redux z lotu ptaka. Źródło: dokumentacja NgRxNa razie powyższy diagram może wydawać się nieco tajemniczy. Dla bardziej zaprawionych w bojach po stronie backendowej może się skojarzyć z CQRS. Niemniej już spieszę z wyjaśnieniami.  Architektura Redux zakłada istnienie globalnego, niemutowalnego bytu, przechowującego stan aplikacji - store. Fizycznie jest to obiekt w formacie JSON, którego struktura definiowana jest przez programistę.  Podstawowym mechanizmem komunikacji są akcje (ang. actions). Reprezentują one konkretne zdarzenia zachodzące w systemie i niosą ze sobą określone informacje.  Akcje mogą być przechwytywane przez reducery (ang. reducers). Reducerem nazywamy czystą funkcję (ang. pure function), która konsumuje akcję i, w zależności od jej przeznaczenia oraz zgodnie z logiką reducera, zastępuje store nowym, z uwzględnioną zmianą.  Dane ze store do komponentów trafiają poprzez selektory (ang. selectors). Selektory powinny otrzymywać wyłącznie dane potrzebne do działania komponentów, w których są używane.  W przypadkach, kiedy wywołanie akcji powinno pociągnąć za sobą dowolne działanie niezwiązane bezpośrednio z aplikacją (np. zapytanie do bazy danych, czy żądanie do zewnętrznej usługi) - do gry wchodzi middleware, które dalej, ze względu na nomenklaturę stosowaną w NgRx, nazywać będziemy efektami (ang. effects). Podobnie jak reducery, middleware potrafi reagować na konkretne akcje i wykonać przypisane im zadanie, jak również wywołać kolejne akcje.Implementacja Redux w NgRxPrzy pierwszym kontakcie wszystkie powyższe pojęcia i pomysły mogą wydawać się nieco skomplikowane i nadmiarowe, zatem na prostym przykładzie pokażę, jak wyglądają fragmenty aplikacji pisanej przy pomocy NgRx. Na końcu wpisu znajduje się link do repozytorium, w którym widać, jak wygląda cała aplikacja oraz jej konfiguracja. Również z tego powodu pomijam instrukcje instalacji biblioteki i konfiguracji środowiska.Załóżmy, że chcemy stworzyć prosty program pozwalający na zapisywanie sposobów na pokonanie nudy. Jednym z możliwych do podjęcia działań będzie wywołanie API, które na każde żądanie odpowie pomysłem na jakąś aktywność. Drugą opcją będzie dodanie własnej koncepcji poprzez prosty formularz. Zacznijmy zatem.StoreNa samym początku procesu, warto uzmysłowić sobie, co tak naprawdę chcemy zapisywać w naszym store, a potem powiedzieć to Typescriptowi tworząc interfejs. W naszej aplikacji będziemy przechowywali w nim jedynie listę czynności potencjalnie zabijających nudę, jednak nawet w tak prostym przypadku warto zwrócić uwagę na to, jaką strukturę planujemy nadać store’owi. Początkowo najprościej jest operować na płaskiej strukturze, w której wszystkie dane znajdują się na tym samym poziomie.Niestety przy rozwijaniu aplikacji takie podejście staje się bardzo nieefektywne, zarówno pod względem organizacji, jak i, z czasem, również wydajności. Zdecydowanie lepiej jest podzielić store na tak zwane “slice’y”, czyli wycinki danych, na przykład dzieląc store według funkcjonalności aplikacji. Zobaczmy takie podejście na przykładzie:export interface State {    activitiesState: ActivityItemsState;}export interface ActivityItemsState {    activities: ActivityItemModel[];}export interface ActivityItemModel {    name: string;    participants: number;}Przejdźmy po powyższych interfejsach po kolei. Pierwszy z nich mówi, że spodziewamy się mieć jeden wycinek danych (slice) typu ActivityItemsState, w którym będą przechowywane wszystkie informacje dotyczące aktywności dodanych do aplikacji. Innymi słowy jest to logicznie wydzielony fragment domeny. Następnie definiujemy, co tak naprawdę znajduje się w tym wycinku danych - jest to tablica obiektów typu ActivityItemModel, czyli informacji o różnych aktywnościach. Ostatni interfejs to już wyłącznie definicja modelu biznesowego - w naszej aplikacji będziemy mieli do czynienia z nazwą czynności oraz możliwą liczbą jej uczestników.Taki podział store zapewnia bardzo łatwą jego rozszerzalność, gdyż w przypadku dodania nowej funkcjonalności wystarczy dopisać nowy wycinek danych do store. W bardzo dużych aplikacjach, gdzie obiekt stanu aplikacji jest już dość potężny, to podejście ma jeszcze jedną zaletę - pozwala wykorzystać lazy loading, to znaczy wczytywać tylko konkretne wycinki store, potrzebne do aktualnie przetwarzanej części aplikacji.AkcjePotrzebujemy teraz opcji wywoływania akcji, żeby móc zgłaszać zajście pewnych faktów w systemie. W naszej aplikacji zakładamy możliwość wystąpienia dwóch zdarzeń - wystosowania żądania do API po pomysł na jakieś zajęcie oraz dodania aktywności do listy.export const activityAddedType = &#39;[activity] Activity added&#39;;export const activitiesRetrievedType = &#39;[activity] Activities retrieved&#39;;export const activityAdded = createAction(activityAddedType, props&amp;lt;ActivityItemModel&amp;gt;());export const activitiesRetrieved = createAction(activitiesRetrievedType);Każda akcja musi mieć zdefiniowany swój typ. NgRx używa do tego tzw. literal type, czyli po prostu używa typu string do identyfikacji akcji (u nas odpowiednio [activity] Activity added oraz [activity] Activities retrieved). W nazywaniu akcji mamy pełną dowolność. Dodatkowo w przypadku dodawania nowej aktywności do listy musimy ją przekazać w ciele akcji przy pomocy metody props&amp;lt;&amp;gt;(). Dzięki wykorzystaniu funkcji createAction z biblioteki NgRx tworzenie akcji, jak widać, jest bardzo proste.EfektJak wspomniałem, będziemy korzystać z zewnętrznego API. W tym celu napiszemy efekt, który będzie reagował na akcję [activity] Activities retrieved, następnie wystosowywał żądanie HTTP, a po otrzymaniu odpowiedzi - dodawał ją do listy.Pojawia się tutaj dość dużo zagadnień, zatem spójrzmy na ten fragment nieco inaczej. Zastanówmy się najpierw jak, używając dotychczas znanych nam narzędzi (na  przykład RxJs), moglibyśmy napisać logikę wywołującą żądanie na każde kliknięcie przycisku i przekazać je dalej. Jeśli założymy, że zdarzenie kliknięcia pojawia się w Observable click$, a strumień, na który ma trafić odpowiedź został nazwany response$, wówczas kod mógłby wyglądać tak:this.response$ = this.click$.pipe(    switchMap(() =&amp;gt; this.http.get&amp;lt;ActivityItemModel&amp;gt;(&#39;https://www.boredapi.com/api/activity&#39;)      .pipe(        catchError(() =&amp;gt; EMPTY)      )));Jeśli na tym etapie potrzebujesz chwili przerwy na zrozumienie co dzieje się w tym kodzie, to polecam wpis o RxJs dostępny na łamach naszego bloga.Wracając do naszego efektu - w zasadzie większość mamy już napisaną! Pozostaje nam tylko kilka rzeczy:  efekt jest tak naprawdę serwisem, jakie znamy z codziennego pisania w Angularze, tworzymy zatem klasę ActivityEffect i dekorujemy ją przy pomocy @Injectable().  nie powinniśmy reagować bezpośrednio na zdarzenie kliknięcia przycisku, a na zgłoszone wcześniej akcję typu activitiesRetrievedType. Mamy do dyspozycji serwis Actions, który możemy traktować jak swoistą szynę, na którą trafiają wywołane akcje. Wstrzykujemy go więc przez konstruktor do efektu. Chcąc reagować wyłącznie na określony typ akcji korzystamy z operatora ofType.  wynik żądania w naszym wypadku nie powinien trafiać do żadnego strumienia, tylko spowodować wywołanie akcji activityAdded. Wystarczy dodać  mapowanie w operatorze pipe().Cały efekt wygląda wówczas tak:@Injectable()export class ActivityEffect {  constructor(private actions$: Actions, private http: HttpClient) {  }  getActivity$ = createEffect(() =&amp;gt; this.actions$.pipe(    ofType(activitiesRetrievedType),    switchMap(() =&amp;gt; this.http.get&amp;lt;ActivityItemModel&amp;gt;(&#39;https://www.boredapi.com/api/activity&#39;)      .pipe(        map(response =&amp;gt; (activityAdded(response))),        catchError(() =&amp;gt; EMPTY)      ))  ));}Podsumowując: w wyniku działania tego efektu każde wywołanie akcji [activity] Activities retrieved przy poprawnej odpowiedzi z API wywoła akcję [activity] Activity added z otrzymaną czynnością. Można oczywiście tworzyć efekty obsługujące inne operacje, nie trzeba ograniczać się do żądań HTTP.ReducerW reducerach zazwyczaj znajduje się najwięcej logiki ze wszystkich komponentów NgRx. To tutaj trzeba zdecydować jak dodawać do store kolejne dane. Warto pamiętać, że reducery to czyste funkcje, które przyjmują jako parametr akcję i obecny stan aplikacji, a następnie zwracają nowy stan, dzięki czemu zachowujemy jego niemutowalność. W naszym przypadku wygląda to tak:const initialState: ActivityItemsState = {activities: []};export const activityReducer = createReducer(    initialState,    on(addActivity, (state, payload) =&amp;gt; ({        ...state,        activities: state.activities.concat({activity: payload.activity, participants: payload.participants} as ActivityItemModel)    })));W pierwszej linii definiujemy, w jaki sposób ma zostać zainicjowany store (lub - jak ma to miejsce powyżej - jego wycinek). W naszym przypadku będzie to pusta tablica. Następnie wywołujemy funkcję createReducer, która w pierwszym parametrze przyjmuje wspomniany stan początkowy, a w drugim (i każdym kolejnym) odpowiednie handlery. W tym przypadku reducer reaguje na akcję [activity] Activity added poprzez dodanie do dotychczasowej tablicy activities nowego elementu i zwrócenie całości jako wynik wywołania funkcji.Reducer jest częścią całej architektury, którą można w bardzo łatwy sposób przetestować i upewnić się, że działa poprawnie. W tym przypadku możemy napisać taki test:describe(&#39;Activity reducer test&#39;, () =&amp;gt; {  function createState(activities: ActivityItemModel[]): ActivityItemsState {    return {activities: [...activities]};  }  it(&#39;should add new activity&#39;, () =&amp;gt; {    // given    const state = createState([]);    const newActivityName = &#39;New activity&#39;;    const newActivityParticipants = 2;    const action = activityAdded({activity: newActivityName, participants: newActivityParticipants});    // when    const newState = activityReducer(state, action);    // then    expect(newState.activities.length).toBe(1);    expect(newState.activities[0].activity).toEqual(newActivityName);    expect(newState.activities[0].participants).toEqual(newActivityParticipants);  });});Test przebiega następująco:  w sekcji given tworzymy sztuczny, pusty store oraz akcję activityAdded,  w sekcji when wywołujemy interesujący nas reducer i przypisujemy wynik do zmiennej newState  w sekcji then sprawdzamy czy w store znajduje się jeden, dodany przez nas element i czy jego właściwości zgadzają sie z tym, co przekazaliśmy w akcji.Jeśli wszystkie trzy asercje zostaną spełnione, wówczas wiemy, że mechanizm dodawania nowej aktywności do store działa.SelektorOstatnią częścią układanki w przepływie są selektory. Podobnie jak reducery są one czystymi funkcjami, których zadaniem jest obserwowanie wycinków store i dostarczanie informacji o ich zmianach do komponentów. Warto tu wspomnieć, że są one dobrym miejscem na to, by dane te odpowiednio przygotować, tak, by po trafieniu do komponentu mogły być „wygodnie” użyte. Dzięki temu komponent może całkowicie abstrahować od struktury store. W naszej aplikacji selektor zdefiniowany jest następująco:const getActivitiesFeatureState = createFeatureSelector&amp;lt;ActivityItemsState&amp;gt;(&#39;activitiesState&#39;);export const getActivities = createSelector(getActivitiesFeatureState, state =&amp;gt; state.activities);Najpierw, przy pomocy funkcji createFeatureSelector tworzymy tzw. feature selector pozwalający na wyciągnięcie pojedynczego wycinka (slice) danych, nazwanego przez nas activitiesState. Następnie używamy go tworząc właściwy selektor przy pomocy funkcji createSelector i wskazując go w pierwszym jej argumencie. Drugim parametrem jest funkcja wskazująca, które dane chcemy otrzymać w wyniku działania selektora. W naszym przypadku jest to tablica czynności, o nazwie activities. Tak zbudowany selektor jest gotowy do użycia w komponencie.Na początku tego punktu wspomniałem, że selektory są czystymi funkcjami, co między innymi oznacza, że przy zachowaniu tego samego stanu i dla tych samych parametrów wielu wywołań zawsze zwrócą ten sam wynik. Ta właściwość została wykorzystana w mechanizmie nazywanym „memoization” (zapamiętywanie). Dzięki niemu NgRx zapamiętuje, z jakimi argumentami ostatnio wywoływany był dany selektor. Jeśli nie uległy one zmianie, wówczas zwraca wynik poprzedniego wywołania selektora, nie wykonując logiki pobierania danych ze store.Spięcie całościMamy już wszystkie potrzebne części logiki NgRx, z których chcemy skorzystać. Pozostaje już tylko dowiedzieć się, jak ich użyć. Pokażę sytuację, w której użytkownik klika przycisk odpowiadający za pobranie przykładowej aktywności z API, przechodząc jednocześnie przez odpowiednie miejsca w kodzie. W trakcie czytania zachęcam do spoglądania na zamieszczony wcześniej w poście diagram przepływu.  Zacznijmy od komponentu wywołującego początkową akcję typu [activity] Activities retrieved. Budowa klasy tego komponentu może wyglądać następująco (pomijam template):    export class ActivityApiComponent { constructor(private store: Store&amp;lt;State&amp;gt;) { } getActivity(): void {     this.store.dispatch(getActivity()); }}        Jak widać w konstruktorze, wstrzykujemy obiekt Store, na którym wykonujemy metodę dispatch podając w jej argumencie typ akcji. Na tym kończy się odpowiedzialność komponentu.    Wysłaną akcję przechwytuje efekt, który, zgodnie z kodem zaprezentowanym wcześniej, wysyła żądanie do API, a otrzymawszy odpowiedź przekazuje ją jako argument nowej akcji [activity] Activity added.  Akcja typu [activity] Activity added jest przechwytywana przez reducer, który wyłuskuje z niej przekazaną z API odpowiedź i dodaje do store.  W ostatniej kolejności do gry wchodzi selektor, który wykrywa zmianę store wynikającą z działania reducera. Użycie selektora w komponencie może wyglądać następująco:export class ActivityListComponent implements OnInit {    activities$: Observable&amp;lt;ActivityItemModel[]&amp;gt;;    constructor(private store: Store&amp;lt;State&amp;gt;) {    }    ngOnInit(): void {        this.activities$ = this.store.select(getActivities);    }}W metodzie ngOnInit wskazujemy, że chcemy reagować na zmiany store przy pomocy selektora getActivities. Zmiennej activities$ możemy następnie użyć w ciele komponentu przy pomocy async pipe, na przykład:&amp;lt;div *ngFor=&quot;let activity of activities$ | async&quot;&amp;gt;    &amp;lt;!-- wnętrze komponentu listy --&amp;gt;&amp;lt;/div&amp;gt;Tym samym cały proces dobiega końca, a kolejne kliknięcie przycisku wywoła go od nowa.Inne podejściaWarto wspomnieć, że tak, jak dla Angulara istnieje biblioteka NgRx, tak również dla pozostałych frameworków z tzw. wielkiej trójcy znajdziemy implementacje architektury Redux:  Redux dla Reacta  Vuex dla VueNależy również zaznaczyć, że Redux nie jest jedynym sposobem na zarządzanie stanem. Można tutaj wymienić takie alternatywy, jak:  MobX  Cycle.js  Flux - warto wiedzieć, że Flux był protoplastą Reduxa  ApolloPodsumowanieŁatwo zauważyć, że na pierwszy ogień NgRx, czy szerzej - Redux - potrafią nieco przytłoczyć ilością kodu, którą trzeba napisać, by nawet drobne funkcjonalności działały. Jest to jeden z największych zarzutów wobec tego rozwiązania, więc jeśli takie były Twoje odczucia podczas czytania tego artykułu - gratuluję krytycznego myślenia! Zauważmy jednak, że po przebrnięciu przez początkowe trudności zostajemy z aplikacją, którą bardzo łatwo możemy przetestować, rozszerzać i której działanie jest jasno zdefiniowane. Chcę też zwrócić uwagę, że przytaczane tutaj przykłady były trywialne, w związku z czym stosunek tzw. boilerplate kodu do faktycznej logiki jest duży. W przypadku złożonych aplikacji, w których sens użycia bibliotek takich jak NgRx jest znacznie większy, narzut ten staje się dużo bardziej akceptowalny.Mam nadzieję, że tym artykułem zainspirowałem nieco do zainteresowania się tematem zarządzania stanem aplikacji frontendowych. Serdecznie zapraszam do rozpoznawania tematu we własnym zakresie, gdyż przedstawiłem tu zaledwie namiastkę możliwości, jakie zapewnia Redux i NgRx.Linki  Repozytorium z projektem  Dokumentacja NgRx",
"url": "/2021/10/13/zarzadzanie-stanem-aplikacji-frontendowej.html",
"author": "Marcin Chrapkowicz",
"authorUrl": "/authors/mchrapkowicz.html",
"image": "mchrapkowicz.jpg",
"highlight": "/assets/img/posts/2021-09-31-zarzadzanie-stanem-aplikacji-frontendowej/ngrx.jpeg",
"date": "13-10-2021",
"path": "pl/2021-09-31-zarzadzanie-stanem-aplikacji-frontendowej"
}
,


"2021-09-13-kafka-retry-dlq-html": {
"title": "Niezawodne dostarczanie zdarzeń w Apache Kafka oparte o ponawianie i DLQ",
"lang": "pl",
"tags": "kouncil programming kafka event sourcing tool dlq",
"content": "W każdym dostatecznie złożonym systemie informatycznym dochodzimy w pewnym momencie do miejsca, w którym musimy sobie odpowiedzieć na pytanie: a co jeśli coś pójdzie nie tak. Jeśli mamy szczęście, to może się okazać, że rozwiązania, które wybraliśmy, dostarczają nam gotowe narzędzia do radzenia sobie w sytuacjach wyjątkowych. Może też się okazać, że nie mieliśmy tyle szczęścia i wybraliśmy Kafkę…W niniejszym wpisie znajdziesz odpowiedź na to, dlaczego w Kafce nie ma DLQ (ang. Dead Letter Queue) oraz jak sobie poradzić w sytuacji, gdy potrzebujesz takiego mechanizmu w swoim systemie.Dlaczego w Kafce nie ma DLQ?Zacznijmy zatem od odpowiedzi na to pytanie. Większość popularnych systemów kolejkowych takich jak RabbitMQ czy ActiveMQ ma wbudowane systemy odpowiedzialne za niezawodne dostarczanie komunikatów. Dlaczego zatem Kafka nie oferuje takowego? Odpowiedź na to pytanie ściśle związana jest z jednym z rozwiązań architektonicznych leżących u podstaw działania Kafki: głupi broker i sprytny konsument (ang. dumb broker / smart consumer). Wzorzec ten sprowadza się do tego, że ciężar logiki związanej z obsługą odczytów przenoszony jest na konsumenta. Konsekwencją takiego podejścia jest brak gotowego rozwiązania mogącego wspomóc konsumenta w przypadku wystąpienia problemu podczas przetwarzania komunikatu. Broker jest zainteresowany tylko jedną informacją: pozycją, na której konsument zakończył przetwarzanie (ang. committed offset). Oczywiście zawsze można rzec, że w tej sytuacji należy dobrać odpowiednie narzędzie do problemu i zastosować system kolejkowy mający takie wsparcie. Nie zawsze jednak mamy nieograniczoną swobodę wprowadzania wielu rozwiązań w jednym systemie. Jeśli tak jak ja wybraliście Kafkę jako silnik rejestrujący zdarzenia, to w przypadku wystąpienia opisywanego problemu musicie poradzić sobie sami i odpowiednio go oprogramować.Jak sobie radzić z błędami?Wyobraźmy sobie sytuację, w której elementem procesu obsługi zdarzenia jest komunikacja z zewnętrznym systemem. Musimy podjąć decyzję, jak ma zachować się konsument w momencie, gdy zewnętrzny system odpowiada w inny sposób, niż się spodziewaliśmy albo, co gorsza — w ogóle nie odpowiada. Jest wiele strategii obsługi takiej sytuacji. Ja na potrzeby tego artykułu wybrałem cztery.Brak obsługiBardzo popularna i często stosowana strategia obsługi sytuacji wyjątkowych, to brak reakcji. Może to potwierdzić każdy programista. Na powyższym rysunku prostokąty oznaczają kolejne wiadomości w topiku. Gdy konsument napotka problem z przetwarzaniem komunikatu o offsecie 4, ignoruje go i przechodzi do następnego. I mimo że takie podejście wydaje się niezbyt rozsądnym rozwiązaniem, to istnieją sytuacje, gdy utrata części komunikatów nie niesie za sobą ryzyka. Za przykład można podać wszelkie rozwiązania przechowujące i analizujące zachowanie użytkowników w aplikacji. Ponieważ zadaniem takiego systemu jest zbieranie danych statystycznych, utrata pojedynczych zdarzeń nie wpłynie znacząco na wyniki. Ważne jest jednak, żeby dysponować skutecznym monitoringiem, który wychwyci sytuację, w której utrata komunikatów przekracza pewien arbitralnie ustalony poziom.Nieskończone ponawianie w miejscuGdy nie możemy pozwolić sobie na utratę komunikatów, najprostszym podejściem jest ponawianie do skutku. Oczywistą konsekwencją jest tzw. zatrzymanie świata. Dopóki błąd nie zostanie poprawiony albo zewnętrzny system udrożniony — żaden kolejny komunikat nie zostanie przetworzony. Takie rozwiązanie jest konieczne w przypadku, gdy chcemy zachować kolejność przetwarzania zdarzeń systemie. W ten scenariusz tym bardziej wpisuje się potrzeba stałego monitoringu.Skończone ponawianie w miejscu z topikiem błędówOmawiane do tej pory strategie zachowują kolejność przetwarzania zdarzeń. Jest to niezwykle istotne w sytuacji, gdy zdarzenia są od siebie zależne i spójność naszego systemu opiera się na kolejności przetwarzania. Nie zawsze jednak zdarzenia mają taką właściwość i brak konieczności zachowania kolejności otwiera przed nami nowe możliwości. Wyobraźmy sobie, co się stanie, gdy nieco poluzujemy wymaganie bezwzględnego zachowania kolejności. Załóżmy, że próbujemy przez jakiś czas ponawiać, ponieważ statystyka i doświadczenie podpowiada nam, że 99% problemów z przetwarzaniem komunikatów jest chwilowych i samoczynnie ustępuje po pewnym czasie. Dodatkowo komunikaty, których nie udało się przetworzyć, kopiujemy na oddzielny topic traktowany jako DLQ. Dzięki temu mamy od razu wyłuskane problematyczne wiadomości i możemy uruchomić na nich osobną grupę konsumentów. Problematyczny komunikat numer 4 zatrzymuje na chwilę przetwarzania, po czym kopiowany jest na topic wiadomości zepsutych eventsDLQ. Z kolei komunikat numer 7 tylko przez chwilę jest ponawiany i po poprawnym przetworzeniu, przetwarzanie jest wznawiane.Krótkie wyjaśnienie, dlaczego komunikaty są kopiowane a nie przenoszone. Odpowiedź jest bardzo prosta — nie mogą być przenoszone. Wynika to z kolejnego fundamentu architektonicznego Kafki, czyli niezmienności topików (ang. topics immutability). Niezależnie jaka była przyczyna błędu, komunikat na zawsze pozostanie utrwalony. Istnieją sposoby na radzenie sobie z tym tematem i wrócimy do tego później.Skończone ponawianie na wydzielonym topikuDochodzimy niniejszym do naszego ostatecznego rozwiązania. Skoro mamy osobny topic dla zepsutych wiadomości to może warto wprowadzić kolejny, na którym odbywa się ponawianie. W tym modelu jeszcze bardziej luzujemy konieczność zachowania kolejności, ale dostajemy w zamian możliwość bezprzerwowego przetwarzania głównego topiku. W konsekwencji nie zatrzymujemy świata, a wiadomości kaskadowo kopiowane są najpierw na topic wiadomości ponawianych, a w przypadku niepowodzenia — topic DLQ. Teoretycznie powinniśmy nazwać go DLT, ale zostańmy przy akronimie DLQ, jako że jest on dobrze kojarzony z tego rodzaju technikami.W systemie, na podstawie którego powstał ten wpis, występują wszystkie cztery opisywane warianty postępowania w sytuacji awaryjnej. Wyzwanie polega na dopasowaniu odpowiedniej metody do natury danych przetwarzanych w topiku. Warto też zaznaczyć, że należy uczyć się od największych i wówczas dwa ostatnie modele są mocno inspirowane sposobem, w jaki Kafkę w swoich systemach używa Uber.Śledzenie przebiegu eventuJakiekolwiek rozwiązanie wybierzemy, pewne jest jedno, potrzebujemy narzędzia, które pozwoli nam śledzić i podglądać, jak zachowuję się eventy na topikach. Kouncil (demo), którego rozwijamy od jakiegoś czasu, szczególnie wpasowuje się w sytuację, gdy wybrana została strategia z topikiem retry oraz dlq. Korzystając z widoku track i mając zapewniony identyfikator korelującym, możemy szybko zweryfikować ścieżkę przetwarzania eventu. Wiemy na przykład, że zdarzenie o identyfikatorze h57z2z zostało poprawnie przetworzone, czyli przeszło przez topiku event-in oraz events-out, co widać na załączonym zrzucie ekranu.Może tak się zdarzyć, że dostaniemy zgłoszenie dotyczące niedostarczenia komunikatu o identyfikatorze oCvD19i. Szybki rzut oka pozwala potwierdzić, że event najpierw trafił na topic do ponawiania, a ostatecznie wylądował w dlq.Więcej na temat śledzenia przebiegu eventów można przeczytać w artykule Marcina Mergo Event Tracking, czyli jak znaleźć igłę w stogu sianaDLQ a grupy konsumentówPozostaje jeszcze jeden istotny aspekt, czyli jak wdrożyć to rozwiązanie w sytuacji wielu grup konsumentów. Na pierwszy rzut oka mogłoby się wydawać, że podobnie jak w RabbitMQ, topic ponawiający i DLQ są ściśle powiązane z głównym topikiem. Nic bardziej mylnego. Koncepcja grup konsumentów działających w Kafce na tym samym topiku, ale mających inną implementację powoduje, że mechanizm ponawiający musi być powiązany z konkretną grupą. W szczególności różne grupy mogą mieć inną logikę ponawianie i obsługi błędów. Sytuacja jeszcze bardziej się komplikuje, gdy grupy konsumentów są w jakiś sposób od siebie zależne. Jedna grupa może bazować na fakcie, że inna grupa poprawnie przetworzyła dany komunikat. Trzeba wtedy bardzo rozważnie dostosować mechanizmy ponawianie i obsługi błędów tak, aby zachować spójność przetwarzania komunikatów.Sprzątanie.Na zakończenie pozostaje jeszcze rozwiązanie problemu związanego z redundancją danych wynikającą z faktu kopiowania wiadomości pomiędzy topikami. W przypadku głównego topika mamy sytuację, że każda wiadomość, która ostatecznie trafiła do DLQ, uznawana jest za uszkodzoną. Jeśli w naszej aplikacji jest możliwość ponownego przetworzenia strumienia wiadomości, to musimy jakoś obsłużyć tę sytuację. Istnieją co najmniej dwa rozwiązania:  rejestr uszkodzonych wiadomości — może być budowany automatycznie na podstawie wiadomości trafiających do DLQ. Składają się na niego offsety wiadomości z głównego topiku. Podczas ponownego przetworzenia konsument, wiedząc o rejestrze, pomija wszystkie oznaczone w nim wiadomości,  kompaktowanie — napisałem wcześniej, że nie można zmieniać i usuwać wiadomości w topiku. Jest od tej reguły wyjątek — mechanizm kompaktowania topiku. W największym skrócie działa to w ten sposób, że broker uruchamia cyklicznie zadanie, które przegląda topik, zbiera wiadomości o tym samym kluczu i pozostawia tylko tą najnowszą. Trik polega na tym, żeby wstawić do strumienia wiadomość o tym samym kluczu co uszkodzona, ale o pustej treści. Konsument musi wcześniej być przygotowany na obsługę takich wiadomości.Można obie techniki stosować jednocześnie, należy jednak pamiętać, że offsety skompaktowanych wiadomości znikną bezpowrotnie z topiku.Topic retry zawiera wiadomości, które po przetworzeniu nie mają żadnej wartości, więc w tym przypadku wystarczy skonfigurować retencję, czyli czas życia wiadomości. Trzeba tylko pamiętać, żeby retencja nie była krótsza, niż najdłuższy możliwy czas przetwarzania pojedynczej wiadomości.Topic DLQ powinien zawierać wiadomości dopóki, dopóty nie zostaną zdiagnozowane a system — poprawiony. Jako że ten czas nie jest łatwy do ustalenia, to nie wchodzi w rachubę retencja. Stąd też trik z kluczami opartymi na datach. Jeśli uznajemy, że incydenty z określonego dnia zostały rozwiązane, to wprowadzamy do DLQ pusty komunikat z kluczem takim jak dzień, i przy najbliższej sesji kompaktowania — wszystkie wiadomości zostaną usunięte z DLQ.PodsumowanieW ten oto sposób dobrnęliśmy do końca. Liczę, że udało mi się zaprezentować na tym prostym przykładzie, że iteracyjne podejście do problemu potrafi doprowadzić nas do ciekawych i skutecznych rozwiązań.",
"url": "/2021/09/13/kafka-retry-dlq.html",
"author": "Jacek Grobelny",
"authorUrl": "/authors/jgrobelny.html",
"image": "jgrobelny.jpg",
"highlight": "/assets/img/posts/2021-09-13-kafka-retry-dlq/kafka-retry-dlq4.png",
"date": "13-09-2021",
"path": "pl/2021-09-13-kafka-retry-dlq"
}
,


"2021-09-08-kouncil-event-tracking-kafka-html": {
"title": "Event Tracking, czyli jak znaleźć igłę w stogu siana",
"lang": "pl",
"tags": "kouncil kafka event tracking",
"content": "Systemy zbudowane na bazie Apache Kafka korzystają na ogół z więcej niż jednego topika. Często też eventy w ramach pojedynczego procesu pokonują drogę pomiędzy różnymi topikami - w niektórych przypadkach ogranicza się to jedynie do przepływu pomiędzy topikami w niezmienionej formie, ale równie często rekordy na kolejnych topikach ulegają modyfikacji, np. dodawane są dane, które będą potrzebne w kolejnych krokach procesu.Event tracking pozwala na prześledzenie oraz wizualizację drogi danego eventu, czy też procesu, przez topiki na Kafce.Weźmy za przykład system do wysyłania notyfikacji do użytkowników. Zanim dane powiadomienie będzie gotowe do wysyłki, potencjalnie będzie musiało przejść przez kilka topików, gdzie uzupełnione zostaną np. treść notyfikacji, czy kanał, którym powiadomienie zostanie przesłane.Przykładowy przepływ na KafceJak wobec tego namierzyć drogę konkretnego procesu pośród milionów innych?Gdzieś już to widziałemW istocie, problem nie jest nowy i sięga co najmniej mikroserwisów. Jeśli w obsługę danego żądania HTTP  zaangażowany jest więcej niż jeden mikroserwis, to potrzebujemy łatwego sposobu na prześledzenie logów związanych z obsługą tego żądania, niezależnie od tego, ile mikroserwisów w tym procesie uczestniczyło. Rozwiązanie tego problemu jest dobrze znane - kiedy żądanie pojawia się w systemie, np. kiedy trafia na API Gateway, wystarczy wygenerować losowy ciąg znaków, umieścić go w nagłówku HTTP, a następnie zadbać o przekazywanie tego nagłówka pomiędzy mikroserwisami, oraz wstrzyknięcie jego wartości do kontekstu logowania (MDC).Śledzenie logów pomiędzy mikroserwisamiDzięki temu, wszystkie logi związane z danym żądaniem będą powiązane wygenerowanym na początku identyfikatorem. Wystarczy wyszukać ten identyfikator w jednym z popularnych agregatorów logów, np. Splunk (zakładając oczywiście, że tego rodzaju narzędzie jest dostępne) i w rezultacie otrzymamy logi powiązane z szukanym żądaniem.Ale co to ma wspólnego z Kafką?Nagłówki na ratunekOkazuje się, że doświadczenia z korelacji logów ze świata mikroserwisów możemy wykorzystać do event trackingu na Kafce. Z rekordami na Kafce, podobnie jak z wiadomościami HTTP, mogą być związane nagłówki, a więc metadane w postaci klucz-wartość.Wykorzystując znany z mikroserwisów wzorzec, kiedy rekord pojawia się pierwszy raz w systemie, generujemy unikalny identyfikator oraz umieszczamy go w nagłówku. Następnie, analogicznie jak w przypadku mikroserwisów, kiedy event wędruje z jednego topika na inny, przekazujemy również powiązany z nim nagłówek korelacji. Idealnie, jeśli wartość tego nagłówka wstrzykujemy również do kontekstu logowania.Przekazywanie nagłówka pomiędzy topikami na KafceNie da się jednak nie zauważyć, że samo przekazywanie nagłówków pomiędzy topikami nie stanowi nawet połowy sukcesu. Prawdziwy problem leży w znalezieniu rekordów z tą konkretną wartością nagłówka pośród setek topików i miliardów rekordów.Jak więc się za to zabrać? Trzeba zacząć od zadania sobie trzech pytań:  Co? - czego tak naprawdę szukamy? Jakich nagłówków, z jakimi wartościami?  Gdzie? - które topiki weźmiemy pod uwagę? przeszukiwanie wszystkich topików często będzie nadmiarowe, na ogół można ograniczyć się jedynie do tych związanych z pewną funkcjonalnością lub przepływem.  Kiedy? - w przypadku kiedy na Kafce znajdują się miliardy rekordów, przeszukanie ich wszystkich zajęłoby wieki. W wielu jednak przypadkach procesy potrzebują kilku sekund lub maksymalnie kilku minut aby w pełni przejść przez Kafkę, co oznacza, że na ogół można zawęzić zakres wyszukiwania do 5, 15 minut lub np. godziny.Co oczywiste, im mniejszy zakres poszukiwań, tym sprawniej ono pójdzie.Jak zatem wziąć te trzy pytania i faktycznie prześledzić drogę procesu przez Kafkę? Out of the box, nie istnieją na Kafce mechanizmy, które pozwoliłby to prosto zrealizować.Event Tracking w praktyceTak się jednak składa, że stworzone przez nas narzędzie, Kouncil, oferuje dokładnie taką funkcjonalność, rozszerzoną o dodatkowe opcje oraz optymalizacje. Zaglądając do zakładki Track znajdziemy filtry pozwalające wprost odpowiedzieć na powyższe trzy pytania.  Po pierwsze, możemy określić jaki nagłówek z jaką zawartością jest dla nas interesujący. Możemy też wskazać sposób matchowania szukanej wartości nagłówka z faktyczną - nie musi to być dopasowanie 1 do 1. Do wyboru są takie operatory jak równa się, nie równa się, zawiera się, nie zawiera się, a nawet wyrażenia regularne.  Po drugie, istnieje możliwość określenia zbioru przeszukiwanych topików - w przypadku, kiedy na klastrze znajdują się setki lub tysiące topików nie ma potrzeby ręcznie wyszukiwać ich na liście - komponent wyboru topika obsługuje filtrowanie.  Po trzecie, mamy możliwość podania przedziału czasowego, który nas interesuje.Filtry zakładki TrackJednak każdorazowe wypełnianie tych pól ręcznie w końcu okaże się być co najmniej niewygodne.Wróćmy więc do przykładu wysyłki powiadomień z początku tego artykułu. Przyjmijmy, że istnieją na klastrze topiki odpowiedzialne za przetwarzanie wysyłki powiadomień: notification-input, gdzie powiadomienia zaczynają swoje życie, oraz inne, przez które eventy przepływają w zależności od przypadku użycia. Tymi topikami mogą być notification-content, -channel, -template, -delivery itp., na których eventy mogą być uzupełniane o potrzebne dane. Na przykład na topic notification-channel wpadną eventy, dla których trzeba wyznaczyć kanał powiadomienia.Zajrzyjmy do topika notification-input:Topic notification-inputZałóżmy, że chcemy prześledzić drogę rekordu o kluczu vFeYAx. Wystarczy, że klikniemy w rekord, aby podejrzeć jego szczegóły, a następnie na interesujący nas nagłówek.Wybór nagłówkaZostaniemy automatycznie przeniesieni na zakładkę Event Trackingu, gdzie pola filtra będą już za nas wypełnione na podstawie wybranego rekordu! Jedyne co musimy zrobić, to dodać interesujące nas topiki, gdyż domyślnie wybrany będzie jedynie ten, na którym znajduje się event, którego drogę chcemy prześledzić.Automatycznie wypełniony filtr Event TrackinguTeraz pozostało nam już tylko kliknąć Track events i obserwować jak w czasie rzeczywistym Kouncil odnajduje rekordy o podanym nagłówku. Sortując wyniki po czasie widzimy nie tylko kiedy oraz przez jakie topiki przeszedł dany proces, ale możemy też podejrzeć rekord na każdym etapie procesu.Wynik Event TrackinguTaki sposób analizy przepływu procesów na Kafce pozwala zaoszczędzić sporo czasu, zwłaszcza w bardziej zawiłych procesach, kiedy rekordy mogą powtórnie trafić na topic, na którym znajdowały się wcześniej lub podczas analizy błędnych sytuacji, kiedy proces mógł trafić na topiki służące do obsługi błędów. Wyszukanie analogicznych informacji w logach aplikacji, choć możliwe, z reguły okazuje się być bardziej czasochłonne, szczególnie, jeśli konsumentami danego procesu są moduły należące do różnych systemów i nie istnieje prosta możliwość zbiorczego przeszukania ich logów.Co ważne, Event Tracking możemy zacząć od dowolnego miejsca w procesie - może to być jego początek, tak jak w przypadku topika notification-input, jednak nic nie stało na przeszkodzie, żeby analizę rozpocząć od samego końca przepływu - topika notification-status - efekt byłby dokładnie ten sam!KouncilKouncil jest darmowy, a jego źródła wraz z instrukcją uruchomienia można znaleźć na naszym githubie. Proces jego uruchomienia jest trywialny i sprowadza się do pojedynczej komendy docker run, w ramach której trzeba jedynie wskazać namiar na którykolwiek z węzłów klastra Kafki. Jedyne czego Kouncil wymaga do obsługi Event Trackingu to zapewnienie istnienia nagłówków w wiadomościach, a resztę, związaną z przeszukiwaniem topików, bierze już na siebie.",
"url": "/2021/09/08/kouncil-event-tracking-kafka.html",
"author": "Marcin Mergo",
"authorUrl": "/authors/mmergo.html",
"image": "mmergo.webp",
"highlight": "/assets/img/posts/2021-09-08-kouncil-event-tracking-kafka/route.jpeg",
"date": "08-09-2021",
"path": "pl/2021-09-08-kouncil-event-tracking-kafka"
}
,


"2021-08-30-kouncil-introduction-html": {
"title": "Kouncil - nowoczesny frontend do Kafki",
"lang": "pl",
"tags": "kouncil programming kafka event sourcing tool",
"content": "Niespełna dwa lata temu w artykule Kafka Companion pisałem o narzędziu, które stworzyliśmy rozwijając system oparty o event sourcing na Kafce. Kafka Companiona już nie ma, ale jest to dobra informacja, gdyż jego miejsce zajął Kouncil. Nowa wersja narzędzia oferuje wszystkie funkcje, które posiadał poprzednik i jednocześnie wprowadza nowe. Oprócz rzucającego się w oczy całkowitego redesignu aplikacji pojawiły się nowe funkcjonalności, które w szczegółach zostaną opisane w kolejnych wpisach na tym blogu.Motywacja opisana w poprzednim artykule nie uległa zmianie. Nadal uważamy, że żaden z dostępnych darmowych interfejsów graficznych do Kafki nie spełnia naszych oczekiwań. A trzeba przyznać, że kilka ich powstało. Przez ostatnie lata pracy z Kafką wypracowaliśmy szereg wzorców i dobrych praktyk, które Kouncil pozwala nam nadzorować. Przedstawię teraz poszczególne funkcjonalności, kładąc szczególny nacisk na to, co zmieniło się w stosunku do poprzednika.Podgląd kondycji klastraEkran pozwala podejrzeć listę węzłów w klastrze. Został rozbudowany o podstawowe statystyki maszyny, na której węzeł jest osadzony. Co więcej, po wybraniu elementu z listy, istnieje możliwość przeglądu wartości wszystkich parametrów konfiguracyjnych. Warto też w tym miejscu zwrócić uwagę na możliwość obsługi wielu klastrów, których przełączanie odbywa się w prawym górnym rogu.Podgląd i dodawanie wiadomości do topikuTabelaryczna prezentacja wiadomości w topiku jest tym, od czego zaczęliśmy budować narzędzie w pierwszej kolejności. Nic więc dziwnego, że nadal duży nacisk położony jest na funkcjonalność i użyteczność tego widoku. Pojawiły się tutaj możliwości wyczekiwane przez wielu użytkowników Kouncila, czyli:  stronicowanie,  możliwość przejścia do dowolnego offsetu,  obsługa natywnych nagłówków wiadomości.Podgląd stanu grup konsumentówTen ekran był funkcjonalnie kompletny w poprzedniej wersji, więc niewiele się tutaj zmieniło, poza bardziej czytelną prezentacją tempa, w którym odbywa się konsumpcja komunikatów.Śledzenie wiadomościZupełnie nowa funkcjonalność, która zdecydowanie wyróżnia nas na tle konkurencji. Więcej o motywacji oraz możliwościach tego ekranu będzie można przeczytać w kolejnym wpisie.PodsumowanieBardzo mi miło zaprezentować efekt naszej intensywnej pracy. Kouncil jest nadal darmowy i dostępny na naszym githubie. A jeżeli zrzuty ekranu nie są wystarczająco zachęcające, to na koniec pozostawiłem jeszcze jedną niespodziankę. Przygotowaliśmy demo narzędzia osadzone w infrastrukturze GCP. I tak jak poprzednio, zachęcam do pobrania, testowania i zgłaszania uwag.",
"url": "/2021/08/30/kouncil-introduction.html",
"author": "Jacek Grobelny",
"authorUrl": "/authors/jgrobelny.html",
"image": "jgrobelny.jpg",
"highlight": "/assets/img/posts/2021-08-30-kouncil-introduction/kouncil_dashboard.png",
"date": "30-08-2021",
"path": "pl/2021-08-30-kouncil-introduction"
}
,


"2021-08-20-stun-server-html": {
"title": "Czy wiesz, że WebRTC korzysta z serwera STUN, aby umożliwić połączenie P2P?",
"lang": "pl",
"tags": "webrtc stun p2p",
"content": "Czy wiesz że WebRTC korzysta z serwera STUN, aby umożliwić połączenie P2P, a w ostateczności z serwera TURN aby przepuścić ruch przez niego i umożliwić połączenie?Rzadko kiedy zwykły użytkownik posiada na swoim komputerze publiczne IP, przez co przekazanie informacji drugiej osobie “moje ip to 192.168.0.X” nie umożliwi połączenia P2P.Nieudane połączenie P2Pźródło: MDN - dostęp: 2020-08-20Dopiero wykorzystanie serwera STUN do określenia publicznego IP oraz ewentualnych ograniczeń może, w większości przypadków, pomóc przy zestawieniu połączenia P2P.Wykorzystanie serwera STUNźródło: MDN - dostęp: 2020-08-20Dzięki temu użytkownik A i B mogą przesłać do siebie adresy przez serwer pośredniczący (tzw. signaling) i ustanowić połączenie P2P.Niestety w niektórych przypadkach samo wykorzystanie STUN nie umożliwia połączenia P2P (np. kiedy użytkownik jest za symetrycznym NATem).Wtedy możliwe jest wykorzystanie serwera TURN, który będzie działał jak proxy dla przesyłanych pakietów.Niestety wiąże się to z większym wykorzystaniem zasobów oraz łącza sieciowego przy przesyłaniu pakietów przez dostawcę usługi WebRTC.Wykorzystanie serwera TURNźródło: MDN - dostęp: 2020-08-20W tym wariancie połączenie bezpośrednie nie było możliwe, ale przesyłanie pakietów przez TURN pozwala użytkownikom nadal się połączyć z przez WebRTC.Połączenie w tym wypadku nadal będzie bezpieczne, ponieważ pakiety są szyfrowane i TURN nie ma możliwości ich odczytania, służy tylko jako pośrednik.Darmowym serwerem STUN/TURN jest coTURN (https://github.com/coturn/coturn). A jeśli chcesz zobaczyć, pod jakim adresem Cię widać z zewnątrz możesz wykorzystać to narzędzie online: https://webrtc.github.io/samples/src/content/peerconnection/trickle-ice/.Źródłahttps://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API/Protocolshttps://blog.ivrpowers.com/post/technologies/what-is-stun-turn-server/https://www.html5rocks.com/en/tutorials/webrtc/infrastructure/",
"url": "/2021/08/20/stun-server.html",
"author": "Jakub Goszczurny",
"authorUrl": "/authors/jgoszczurny.html",
"image": "jgoszczurny.jpg",
"highlight": "/assets/img/posts/2021-08-20-stun-server/handshake.jpeg",
"date": "20-08-2021",
"path": "pl/2021-08-20-stun-server"
}
,


"2021-08-02-sredniki-w-js-html": {
"title": "Czy wiesz co się dzieje, kiedy nie stawiasz średników w JS?",
"lang": "pl",
"tags": "javascript",
"content": "Każdemu zdarza się czasem zapomnieć postawić średnika na końcu linijki. Powinny to wyłapywać reguły zawarte w narzędziach takich jak TSLint, ESList, lecz czasami mimo ich pomocy zapominamy o tym. I co wtedy? Niby nic się nie dzieje, a kod dalej działa według naszej myśli, tylko dlaczego z jakiegoś powodu wszyscy każą stawiać te średniki?Automatic Semicolon InsertionJavaScript nie wymaga od nas stawiania średników. Posiada automat (Automatic Semicolon Insertion, w skrócie ASI), który interpretuje nasz kod i wstawia je za nas. Opiera się w skrócie na kilku zasadach: kiedy kończymy linijkę używając “}”, kiedy jest to koniec pliku, kiedy w linijce użyjemy któregoś z słów kluczowych: return, break, throw, continue oraz kiedy nowa linijka powoduje błąd w połączeniu z następną.Nie powinniśmy jednak opierać w pełni na tych zasadach, bo łatwo można popełnić błąd, który ASI zinterpretuje w zaskakujący sposób:const test = 12[&#39;c&#39;,&#39;d&#39;].forEach((letter) =&amp;gt; console.log(letter))// Cannot read property &#39;forEach&#39; of undefinedDzieje się tak, ponieważ JS spróbował połączyć ze sobą 12 i [‘c’,’d’].Podobny przykład:var a = 1var b = a(a+b).toString()// Uncaught TypeError: a is not a functionCo powinniśmy w takim razie robić? Nie opierać się na ASI ponieważ, nie jesteśmy w stanie całkowicie zdać się na niego. Stawiajmy wszędzie średniki, żeby uniknąć niekonsekwencji w naszym kodzie.Po więcej informacji odsyłam do dokumentacji.",
"url": "/2021/08/02/sredniki-w-js.html",
"author": "Piotr Grobelny",
"authorUrl": "/authors/pgrobelny.html",
"image": "pgrobelny.webp",
"highlight": "/assets/img/posts/2021-08-02-sredniki-w-js/semicolon.jpg",
"date": "02-08-2021",
"path": "pl/2021-08-02-sredniki-w-js"
}
,


"2021-07-16-zarzadzanie-dostepem-przy-uzyciu-access-control-list-html": {
"title": "Zarządzanie dostępem przy użyciu ACL (Access Control List)",
"lang": "pl",
"tags": "acl access control list spring security acl access control list authorization",
"content": "W świecie programistów Java, od wielu lat prym wiedzie Spring Framework, który błyskawicznie dostosowuje się do panujących trendów. Trudno sobie wyobrazić programistę Java, szczególnie aplikacji internetowych, który nie znałby tego projektu, lub precyzyjniej, zbioru projektów. Jednym z popularnych i bardzo dojrzałych elementów ekosystemu, jest Spring Security, który dostarcza gotowe rozwiązania dla różnych zawiłych zagadnień w zakresie bezpieczeństwa.Kilka lat temu, kiedy zaczynałem przygodę z programowaniem, moim pierwszym komercyjnym projektem do wykonania, było zaimplementowanie aplikacji internetowej, która miała służyć do zarządzania zadaniami. Zadania te były przypisane do konkretnych użytkowników, którzy nie mogli sobie ich nawzajem przeglądać. Problem wydawał się być powszechny. Po krótkim poszukiwaniu, natknąłem się na koncepcję znaną jako Access Control List oraz jego realizacją w Spring Security ACL, która jest rozszerzeniem Spring Security. To było to, czego szukałem!W tym artykule wyjaśnię, dlaczego Spring Security jest niewystarczający do zrealizowania wspomnianego wymagania, i dlaczego potrzebujemy rozszerzenia Spring Security ACL. Dodatkowo przedstawię fragmenty kodu, które są istotne w naszym projekcie.Pełny kod jest dostępny pod adresem https://github.com/pawelwalaszek/spring-security-acl.Czym właściwie jest Access Control List?Krótka definicja brzmi:Access Control List (ACL) jest listą uprawnień skojarzonych z obiektem.W naszym przypadku obiektem jest zadanie. Natomiast lista uprawnień jest przechowywana w specjalnych strukturach tabelarycznych znajdujących się w bazie danych, w której są zdefiniowane relacje między obiektem, a użytkownikiem.Dlaczego Spring Security jest niewystarczający?Spring Security pozwala określić dostęp na poziomie żądania HTTP lub wywołania metody.Przykład:@PreAuthorize(&quot;hasRole(&#39;TASK&#39;)&quot;)public List&amp;lt;Task&amp;gt; getTasksWithoutAcl() {    return taskRepository.findAll();}W powyższym przykładzie użytkownik z rolą TASK otrzyma pełną listę obiektów Task. Nie jest to zgodne z naszym wymaganiem, ponieważ chcemy, aby użytkownik otrzymał wyselekcjonowaną listę obiektów Task, dokładniej, listę obiektów do których został przypisany.Spring Security ACL pozwala określić dostęp na poziomie obiektów.Przykład:@PreAuthorize(&quot;hasRole(&#39;TASK&#39;)&quot;)@PostFilter(&quot;hasPermission(filterObject, &#39;READ&#39;)&quot;)public List&amp;lt;Task&amp;gt; getTasksWithAcl() {    return taskRepository.findAll();}W powyższym przykładzie użytkownik z rolą TASK otrzyma listę obiektów Task, ale tylko tych, do których otrzymał uprawnienie odczytu.@PreAuthorize – sprawdza, czy użytkownik posiada rolę TASK, a w przypadku jej braku generuje wyjątek, który tworzy odpowiedź HTTP ze statusem 403.@PostFilter – usuwa obiekty z kolekcji, do których użytkownik nie ma uprawnień.Kombinacja adnotacji @PreAuthorize i @PostFilter jest bardzo wygodna, ale żeby taka była, należy odpowiednio przygotować konfigurację w naszej aplikacji zarówno dla Spring Security jak i dla Spring Security ACL.Konfiguracjaa) Konfiguracja dla Spring Security:@Configuration@EnableWebSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter {    @Override    public void configure(WebSecurity webSecurity) throws Exception {        webSecurity.ignoring().antMatchers(&quot;/h2-console/**&quot;);    }    @Override    protected void configure(HttpSecurity http) throws Exception {        http.csrf().disable()            .authorizeRequests()            .anyRequest().authenticated()            .and()            .formLogin()            .and()            .httpBasic();    }    @Override    protected void configure(AuthenticationManagerBuilder auth) throws Exception {        auth.inMemoryAuthentication()                .withUser(&quot;admin&quot;).password(passwordEncoder().encode(&quot;admin&quot;)).roles(&quot;ADMINISTRATION&quot;)                .and()                .withUser(&quot;user1&quot;).password(passwordEncoder().encode(&quot;user1&quot;)).roles(&quot;TASK&quot;)                .and()                .withUser(&quot;user2&quot;).password(passwordEncoder().encode(&quot;user2&quot;)).roles(&quot;TASK&quot;);    }    @Bean    public PasswordEncoder passwordEncoder() {        return new BCryptPasswordEncoder();    }}b) Konfiguracja dla Spring Security ACL. W tym punkcie dokładniej przyjrzyjmy się konfiguracji.Pierwszym istotnym krokiem jest dodanie klasy DefaultMethodSecurityExpressionHandler, która jest wzbogacona o obsługę wyrażeń ACL.W tym miejscu dochodzi do załadowania przydzielonych uprawnień oraz skonfrontowanie ich z zabezpieczonymi obiektami. Na potrzeby wspomnianejklasy należy dodać klasę JdbcMutableAclService, która wchodzi w interakcję z bazą danych. To w tej klasie są zdefiniowane zapytania SQL.Dodatkowa klasa BasicLookupStrategy określa strategię, która optymalizuje zapytania. W naszym przypadku optymalizacja jest wykonanaprzy założeniu, że użyta baza danych jest zgodna z ANSI SQL.@Autowiredprivate MethodSecurityExpressionHandler defaultMethodSecurityExpressionHandler;@Overrideprotected MethodSecurityExpressionHandler createExpressionHandler() {    return defaultMethodSecurityExpressionHandler;}@Beanpublic MethodSecurityExpressionHandler defaultMethodSecurityExpressionHandler(DataSource dataSource) {    DefaultMethodSecurityExpressionHandler expressionHandler = new DefaultMethodSecurityExpressionHandler();    AclPermissionEvaluator permissionEvaluator = new AclPermissionEvaluator(aclService(dataSource));    expressionHandler.setPermissionEvaluator(permissionEvaluator);    return expressionHandler;}@Beanpublic JdbcMutableAclService aclService(DataSource dataSource) {    return new JdbcMutableAclService(dataSource, lookupStrategy(dataSource), aclCache());}@Beanpublic LookupStrategy lookupStrategy(DataSource dataSource) {    return new BasicLookupStrategy(dataSource, aclCache(), aclAuthorizationStrategy(), new ConsoleAuditLogger());}Klasa AclAuthorizationStrategyImpl definiuję strategię, która określa, w jakich warunkach jest przydzielane uprawnienie.Jeśli jesteśmy właścicielem obiektu lub mamy uprawnienie administracyjne, wtedy otrzymujemy dostęp do obiektu.@Beanpublic AclAuthorizationStrategy aclAuthorizationStrategy() {    return new AclAuthorizationStrategyImpl(new SimpleGrantedAuthority(&quot;ROLE_TASK&quot;));}Klasa DefaultPermissionGrantingStrategy definuję dodatkową strategię, która określa, czy jest przydzielane uprawnienie. Jeśli nie jesteśmywłaścicielem obiektu i nie mamy uprawnienia administracyjnego, ale posiadamy wpisy w bazie definiujące uprawnienie do obiektu,to otrzymujemy dostęp do obiektu.@Beanpublic PermissionGrantingStrategy permissionGrantingStrategy() {    return new DefaultPermissionGrantingStrategy(new ConsoleAuditLogger());}W repozytorium, w konfiguracji ACL znajdują się dodatkowe klasy odpowiedzialne za cache, który zmniejsza ilość zapytań do bazy danych.c) Struktura schematu dla poszczególnych baz danych jest dostępna w kodach źródłowych Spring Security tutaj. W naszym przypadku DDL jest dla bazy danych H2:-- App schemasCREATE TABLE IF NOT EXISTS tasks (  id bigint(20) NOT NULL AUTO_INCREMENT,  chapter varchar(100) NOT NULL,  title varchar(100) NOT NULL,  description varchar(1000),  creation_date TIMESTAMP WITH TIME ZONE NOT NULL,  PRIMARY KEY (id));-- ACL schemasCREATE TABLE IF NOT EXISTS acl_sid (  id bigint(20) NOT NULL AUTO_INCREMENT,  principal tinyint(1) NOT NULL,  sid varchar(100) NOT NULL,  PRIMARY KEY (id),  UNIQUE KEY unique_uk_1 (sid,principal));CREATE TABLE IF NOT EXISTS acl_class (  id bigint(20) NOT NULL AUTO_INCREMENT,  class varchar(255) NOT NULL,  PRIMARY KEY (id),  UNIQUE KEY unique_uk_2 (class));CREATE TABLE IF NOT EXISTS acl_entry (  id bigint(20) NOT NULL AUTO_INCREMENT,  acl_object_identity bigint(20) NOT NULL,  ace_order int(11) NOT NULL,  sid bigint(20) NOT NULL,  mask int(11) NOT NULL,  granting tinyint(1) NOT NULL,  audit_success tinyint(1) NOT NULL,  audit_failure tinyint(1) NOT NULL,  PRIMARY KEY (id),  UNIQUE KEY unique_uk_4 (acl_object_identity,ace_order));CREATE TABLE IF NOT EXISTS acl_object_identity (  id bigint(20) NOT NULL AUTO_INCREMENT,  object_id_class bigint(20) NOT NULL,  object_id_identity bigint(20) NOT NULL,  parent_object bigint(20) DEFAULT NULL,  owner_sid bigint(20) DEFAULT NULL,  entries_inheriting tinyint(1) NOT NULL,  PRIMARY KEY (id),  UNIQUE KEY unique_uk_3 (object_id_class,object_id_identity));ALTER TABLE acl_entryADD FOREIGN KEY (acl_object_identity) REFERENCES acl_object_identity(id);ALTER TABLE acl_entryADD FOREIGN KEY (sid) REFERENCES acl_sid(id);ALTER TABLE acl_object_identityADD FOREIGN KEY (parent_object) REFERENCES acl_object_identity (id);ALTER TABLE acl_object_identityADD FOREIGN KEY (object_id_class) REFERENCES acl_class (id);ALTER TABLE acl_object_identityADD FOREIGN KEY (owner_sid) REFERENCES acl_sid (id);Przykładowe daneUtworzony schemat należy wypełnić odpowiednimi danymi. W naszej aplikacji mamy dwóch predefiniowanych użytkowników user1 i user2 z taką samą rolą TASK. Dla tych użytkowników utworzymy 8 zadań i odpowiednio przypiszemy ich do poszczególnych zadań. Użytkownik user1 będzie mieć prawo odczytu do zadań z kategorii Security, natomiast użytkownik user2 będzie mieć prawo odczytu do pozostałych zadań. W naszym przykładzie ograniczamy się jedynie do prawa odczytu, jednak ACL umożliwia nadawanie uprawnień dla odczytu, zapisu, tworzenia oraz usuwania.Warto podkreślić, że model aplikacji nie jest bezpośrednio powiązany z tabelami, w których są trzymane informacje o uprawnieniach. Pośrednim powiązaniem między modelem aplikacji, a zdefiniowanymi uprawnieniami, są adnotacje.W naszej przykładowej aplikacji dodajemy dane bezpośrednio do bazy danych za pomocą DML.-- Kilka przykładowych zadańINSERT INTO tasks (id, chapter, title, description, creation_date) VALUES (1, &#39;Security&#39;, &#39;tytuł 1&#39;, &#39;opis zadania 1&#39;, current_timestamp);INSERT INTO tasks (id, chapter, title, description, creation_date) VALUES (2, &#39;Cloud&#39;, &#39;tytuł 2&#39;, &#39;opis zadania 2&#39;, current_timestamp);INSERT INTO tasks (id, chapter, title, description, creation_date) VALUES (3, &#39;Security&#39;, &#39;tytuł 3&#39;, &#39;opis zadania 3&#39;, current_timestamp);INSERT INTO tasks (id, chapter, title, description, creation_date) VALUES (4, &#39;Cloud&#39;, &#39;tytuł 4&#39;, &#39;opis zadania 4&#39;, current_timestamp);INSERT INTO tasks (id, chapter, title, description, creation_date) VALUES (5, &#39;Frontend&#39;, &#39;tytuł 5&#39;, &#39;opis zadania 5&#39;, current_timestamp);INSERT INTO tasks (id, chapter, title, description, creation_date) VALUES (6, &#39;Security&#39;, &#39;tytuł 6&#39;, &#39;opis zadania 6&#39;, current_timestamp);INSERT INTO tasks (id, chapter, title, description, creation_date) VALUES (7, &#39;Cloud&#39;, &#39;tytuł 7&#39;, &#39;opis zadania 7&#39;, current_timestamp);INSERT INTO tasks (id, chapter, title, description, creation_date) VALUES (8, &#39;Storage&#39;, &#39;tytuł 8&#39;, &#39;opis zadania 8&#39;, current_timestamp);-- ACL - przepisanie uprawnieńINSERT INTO acl_sid (id, principal, sid) VALUES(1, 0, &#39;ROLE_TASK&#39;),(2, 1, &#39;admin&#39;),(3, 1, &#39;user1&#39;),(4, 1, &#39;user2&#39;);INSERT INTO acl_class (id, class) VALUES(1, &#39;com.consdata.task.model.Task&#39;);INSERT INTO acl_object_identity (id, object_id_class, object_id_identity, parent_object, owner_sid, entries_inheriting) VALUES(1, 1, 1, NULL, 2, 0),(2, 1, 3, NULL, 2, 0),(3, 1, 6, NULL, 2, 0),(4, 1, 2, NULL, 2, 0),(5, 1, 4, NULL, 2, 0),(6, 1, 5, NULL, 2, 0),(7, 1, 7, NULL, 2, 0),(8, 1, 8, NULL, 2, 0);INSERT INTO acl_entry (id, acl_object_identity, ace_order, sid, mask, granting, audit_success, audit_failure) VALUES(1, 1, 1, 3, 1, 1, 1, 0),(2, 2, 1, 3, 1, 1, 1, 0),(3, 3, 1, 3, 1, 1, 1, 0),(4, 4, 1, 4, 1, 1, 1, 0),(5, 5, 1, 4, 1, 1, 1, 0),(6, 6, 1, 4, 1, 1, 1, 0),(7, 7, 1, 4, 1, 1, 1, 0),(8, 8, 1, 4, 1, 1, 1, 0);Opis poszczególnych tabel został przedstawiony tutaj.Możemy również programowo dodawać uprawnienia do użytkowników. Przykład klasy, która to realizuje:@RequiredArgsConstructor@Servicepublic class PermissionService {    private final MutableAclService aclService;    public void addPermission(String username, Class&amp;lt;?&amp;gt; type, Long id, Permission permission) {        ObjectIdentity objectIdentity = new ObjectIdentityImpl(type, id);        Sid sid = new PrincipalSid(username);        MutableAcl acl;        try {            acl = (MutableAcl) aclService.readAclById(objectIdentity);        } catch (NotFoundException exception) {            acl = aclService.createAcl(objectIdentity);        }        acl.insertAce(acl.getEntries().size(), permission, sid, true);        aclService.updateAcl(acl);    }}Access Control List w akcjiOmówiliśmy już wszystkie niezbędne kwestie. Jesteśmy gotowi uruchomić aplikację i ją przetestować. Pełny kod znajduje się pod adresem https://github.com/pawelwalaszek/spring-security-acl.mvn spring-boot:runUwaga! Dla każdego użytkownika należy zalogować się w osobnym trybie incognito, gdyż pozwoli nam uniknąć problemów z cachowaniem danych dostępowych w przeglądarce internetowej.Wejście pod adres:http://localhost:8080/tasks/list-with-acli zalogowanie się jako user1 z hasłem user1 spowoduje wyświetlenie zadań tylko z kategorii Security. Natomiast dla użytkownika user2 z hasłem user2 zostaną wyświetlone zadania z pozostałych kategorii.Spróbujmy przypisać użytkownikowi user1 uprawnienie do odczytywania zadania z identyfikatorem nr 8. Dla tej sytuacji został przygotowany endpoint, który potrafi zrealizować taką operację.curl -X PUT -u admin:admin http://localhost:8080/permissions/READ/tasks/8/users/user1/addPonowne wejście pod adres:http://localhost:8080/tasks/list-with-acli zalogowanie się jako user1 z hasłem user1 spowoduje wyświetlenie zadań z kategorii Security oraz jedno zadanie z identyfikatorem nr 8, czyli zadanie z kategorii Storage.Wejście pod adres:http://localhost:8080/tasks/list-without-acldowolnym użytkownikiem spowoduje wyświetlenie wszystkich zadań, gdyż dla tego adresu został określony dostęp na poziomie wywołania metody, w tym przypadku, dla użytkowników z rolą TASK.PodsumowanieCzy potrzebujmy Spring Security ACL? To zależy od wymagań:  Tak, jeśli potrzebujemy określać dostęp na poziomie obiektów.  Nie, jeśli potrzebujemy określać dostęp na poziomie żądania HTTP lub wywołania metody.Tym artykułem chciałbym zwrócić uwagę na obecność gotowej implementacji Access Control List oraz jaki konkretny problem rozwiązuje. Warto skorzystać z gotowych i dojrzałych rozwiązań, takich jak, Spring Security ACL, gdyż pozwoli nam zaoszczędzić sporo czasu oraz uniknąć potencjalnych błędów podczas tworzenia własnej implementacji.",
"url": "/2021/07/16/zarzadzanie-dostepem-przy-uzyciu-access-control-list.html",
"author": "Paweł Walaszek",
"authorUrl": "/authors/pwalaszek.html",
"image": "pwalaszek.jpg",
"highlight": "/assets/img/posts/2021-07-16-zarzadzanie-dostepem-przy-uzyciu-access-control-list/acl.png",
"date": "16-07-2021",
"path": "pl/2021-07-16-zarzadzanie-dostepem-przy-uzyciu-access-control-list"
}
,


"2021-07-08-metaprogramowanie-w-javie-html": {
"title": "Metaprogramowanie w Javie - @Target",
"lang": "pl",
"tags": "java",
"content": "W tym artykule dowiemy się jak używać adnotacji @Target przy tworzeniu własnej adnotacji.@Target określa, w których miejscach konstruowana przez nas adnotacja może zostać użyta. Gdy zadeklarujemy, w jakich miejscach kodu jest możliwe użycie adnotacji, błędne jej umiejscowienie nie pozwoli na skompilowanie kodu.Podstawowa składnia adnotacjiAdnotacje tworzy się używając deklaracji interface ze znakiem @ na początku. Taka adnotacja nic nie robi, lecz program nie wyrzuci błędu przy kompilacji.public @interface MyAnnotation {}Powyższa adnotacja nie ma zdefiniowanego miejsca, w którym możemy jej użyć. Aby to zrobić, należy dodać adnotację @Target, wraz z parametrami.Parametry do adnotacji @Target możemy przekazać na 4 różne sposoby. W trzecim i czwartym przykładzie możliwe jest zdefiniowanie jednej lub większej liczby typów:@Target(ElementType.FIELD)@Target(value=ElementType.FIELD)@Target({ElementType.FIELD, ElementType.CONSTRUCTOR})@Target(value={ElementType.FIELD, ElementType.CONSTRUCTOR})Gdzie możemy użyć adnotacji, w zależności od przekazanego typu ElementType?  ElementType.ANNOTATION_TYPE - adnotacja do tworzenia adnotacji 🙂. Na przykład wymienione w tytule @Target() jest taką adnotacją. Inny przykład to @Retention() opisujące cykl życia adnotacji. Obie są oznaczone adnotacją ElementType.ANNOTATION_TYPE.@Target({ ElementType.ANNOTATION_TYPE })public @interface MyAnnotation {}@MyAnnotation@interface NewAnnotation {}  ElementType.CONSTRUCTOR - przy deklaracji konstruktora.@Target({ ElementType.CONSTRUCTOR })public @interface MyAnnotation {} public class NewClass {         @MyAnnotation    public NewClass(){}}  ElementType.FIELD - przy deklaracji pola w klasie.@Target({ ElementType.FIELD })public @interface MyAnnotation {} public class NewClass {         @MyAnnotation    private String text;}  ElementType.LOCAL_VARIABLE - przy deklaracji lokalnej zmiennej (np. w funkcji). Nie mylić z deklaracją FIELD.@Target({ ElementType.LOCAL_VARIABLE })public @interface MyAnnotation {} public class NewClass {         public void newFunction(){        @MyAnnotation        String text = &quot;text&quot;;    }}  ElementType.METHOD - przy deklaracji metody/funkcji.@Target({ ElementType.METHOD })public @interface MyAnnotation {} public class NewClass {     @MyAnnotation    public void newFunction(){}}  ElementType.PACKAGE - przy deklaracji pakietu. Możliwe jedynie do dodania w pliku package-info.java!@Target({ ElementType.PACKAGE })public @interface MyAnnotation {} @MyAnnotationpackage mypackage; // tylko w pliku package-info.java  ElementType.PARAMETER - przy deklaracji parametrów metody/funkcji.@Target({ ElementType.PARAMETER })public @interface MyAnnotation {} public class NewClass {    public void newFunction(@MyAnnotation int a, @MyAnnotation String b){}}  ElementType.TYPE - przy deklaracji klasy, interfejsu, enuma.@Target({ ElementType.TYPE })public @interface MyAnnotation {} @MyAnnotationpublic class NewClass {}  ElementType.TYPE_USE - od Javy 8. Wszędzie tam gdzie użyty jest typ. “As of the Java SE 8 release, annotations can also be applied to any type use. This means that annotations can be used anywhere you use a type. A few examples of where types are used are class instance creation expressions (new), casts, implements clauses, and throws clauses”.@Target({ ElementType.TYPE_USE })public @interface MyAnnotation {} public class NewClass extends @MyAnnotation Object{    List&amp;lt; @MojaAdnotacja String&amp;gt; myList;    int a = (@MyAnnotation int) 4.21;    Exception e = new @MyAnnotation Exception();}  ElementType.TYPE_PARAMETER- od Javy 8. Przy definiowaniu typów generycznych.@Target({ ElementType.TYPE_PARAMETER })public @interface MyAnnotation {}public class NewClass&amp;lt; @MyAnnotation T&amp;gt;{    &amp;lt;@MyAnnotation U&amp;gt; void myMethod(){    }}W przypadku gdy nie zadeklarujemy, w którym miejscu możemy użyć adnotacji, możemy jej użyć wszędzie oprócz TYPE_USE i TYPE_PARAMETER.public @interface MyAnnotation {} @MyAnnotation@interface NewAnnotation {} @MyAnnotationpublic class NewClass {     @MyAnnotation    String newField;     @MyAnnotation    public NewClass(){    }     @MyAnnotation    public void newFunction(@MyAnnotation String parametr){        @MyAnnotation String myLocalVariable;    }}Źródła:  https://www.programmersought.com/article/7951304416/  https://www.samouczekprogramisty.pl/adnotacje-w-jezyku-java/  https://bykowski.pl/adnotacje-w-jezyku-java-2/  https://docs.oracle.com/javase/tutorial/java/annotations/basics.html  https://docs.oracle.com/javase/tutorial/java/annotations/type_annotations.html",
"url": "/2021/07/08/metaprogramowanie-w-javie.html",
"author": "Dominik Surdyk",
"authorUrl": "/authors/dsurdyk.html",
"image": "dsurdyk.webp",
"highlight": "/assets/img/posts/2021-07-06-metaprogramowanie-w-javie/metaprogramowanie.jpeg",
"date": "08-07-2021",
"path": "pl/2021-07-08-metaprogramowanie-w-javie"
}
,


"2021-05-31-slowo-kluczowe-volatile-html": {
"title": "Volatile nie należy się bać",
"lang": "pl",
"tags": "java",
"content": "Słowo kluczowe volatile wydaje się jednym z najrzadziej stosowanych, ale też najbardziej tajemniczych i najsłabiej poznanych słów kluczowych w Javie. Do czego więc służy, i czy jest się czego bać?Na początek należy zauważyć, że volatile ma zastosowanie jedynie w przypadku zmiennych. Sam typ zmiennej, oraz to, czy jest to typ prosty czy złożony, nie ma znaczenia. Przykładowe użycie słowa volatile wygląda następująco:private volatile int myInt = 0;Volatile ma zastosowanie w przypadku aplikacji wielowątkowych, i związane jest z optymalizacjami, które wykonuje zarówno procesor, jak i JVM podczas zmiany wartości zmiennych, z których korzysta więcej niż jeden wątek. Brzmi skomplikowanie? Wyobraźmy sobie sytuację, w której wątki A oraz B mają dostęp do zmiennej foo. W przypadku kiedy wątek A zmieni wartości tej zmiennej, zmiana ta niekoniecznie będzie od razu spropagowana do wątku B! W skrajnym przypadku może nie zostać spropagowana nigdy. Oznacza to, że dwa wątki, odczytujące pozornie tą samą zmienną, mogą widzieć dwie rozbieżne wartości. Tyle teorii, kod jest ciekawszy.Żeby lepiej zobrazować problem, pochylmy się nad poniższym kawałkiem kodu:public class VolatileExample {     private static int counter = 0;     public static void main(String[] args) {        ThreadSupplier producer = () -&amp;gt; {            while (true) {                System.out.println(format(&quot;Ustawiam licznik na %s&quot;, ++counter));                Thread.sleep(1000);            }        };         ThreadSupplier consumer = () -&amp;gt; {            int localCounter = counter;            while (true) {                if (localCounter != counter) {                    System.out.println(format(&quot;Licznik zmienił wartość na: %s&quot;, localCounter = counter));                }            }        };         CompletableFuture.allOf(supplyAsync(producer), supplyAsync(consumer)).join();    }}Co robi ta klasa? Uruchamia dwa wątki: odpowiednio wątek producenta, podbijający raz na sekundę licznik, oraz wątek konsumenta, który w pętli sprawdza czy licznik zmienił wartość, a jeśli tak - wypisuje jego wartość.Jakiego wyjścia na pierwszy rzut oka można by się spodziewać po odpaleniu powyższego kodu? Prawdopodobnie wielu z nas spodziewać się będzie efektu mniej więcej jak poniżej - i wydaje się to zupełnie rozsądne:Ustawiam licznik na 1Licznik zmienił wartość na: 1Ustawiam licznik na 2Licznik zmienił wartość na: 2Ustawiam licznik na 3Licznik zmienił wartość na: 3Ustawiam licznik na 4Licznik zmienił wartość na: 4Ustawiam licznik na 5Licznik zmienił wartość na: 5Tymczasem jednak, po uruchomieniu tego kodu najprawdopodobniej zobaczycie takie wyjście:Ustawiam licznik na 1Ustawiam licznik na 2Ustawiam licznik na 3Ustawiam licznik na 4Ustawiam licznik na 5...Ustawiam licznik na 1000Co się dzieje z wątkiem konsumenta, a w szczególności, dlaczego nie podchwytuje zmian licznika? Okazuje się, że oba te wątki - producent oraz konsument - posiadają własną kopię zmiennej counter. Kiedy jeden z wątków zmienia jej wartość, to JVM oraz procesor decydują, kiedy przepropagować jej wartość do pozostałych wątków. W imię optymalizacji taka propagacja może nie nastąpić nigdy. Jak więc naprawić nasz program? Bardzo prosto, wystarczy do definicji zmiennej counter dodać słowo kluczowe volatile, które poinformuje wszystkie zainteresowane mechanizmy, że z tej zmiennej korzysta więcej niż jeden wątek, i wszelkie zmiany jej wartości należy natychmiast propagować do tych wątków:private static volatile int counter = 0;Po takiej zmianie i ponownym uruchomieniu programu ujrzymy wyjście, którego się oryginalnie spodziewaliśmy.Jak natomiast ma się to do aplikacji, które rozwijamy na co dzień? Dobrym przykładem mogą być np. zmienne trzymane w sesji użytkownika, czytane i modyfikowane przez potencjalnie wiele wątków - w skrajnym przypadku może dojść do sytuacji, w której różne wątki korzystające z takiego sesyjnego obiektu będą widziały rozbieżne jego wartości, co z kolei może doprowadzić do najróżniejszych anomalii oraz błędów (szczególnie, jeśli jeden z tych wątków wykonuje przetwarzanie obciążające procesor - z punktu widzenia procesora będzie to kandydat do wykonania optymalizacji polegającej na niepropagowaniu do tego wątku nowej wartości takiej zmiennej).Podsumowując, warto pamiętać o volatile rozwijając wielowątkowe aplikacje, w których różne wątki korzystają ze wspólnych zmiennych.PS. uważny czytelnik zauważy, że w gołej Javie nie istnieje taki interfejs funkcyjny jak ThreadSupplier - utworzyłem go na potrzeby czytelności przykładu, aby nie zaciemniać kodu obsługą wyjątku z Thread.sleep. Pełny kod źródłowy tego przykładu znajduje się poniżej — polecam lekturę wszystkim, którzy chcieliby się dowiedzieć jak poradzić sobie z wyjątkami rzucanymi w lambdach bez użycia zewnętrznych bibliotek.package com.consdata.webdev;import java.util.concurrent.CompletableFuture;import java.util.function.Supplier;import static java.lang.String.format;import static java.util.concurrent.CompletableFuture.supplyAsync;public class VolatileExample {    private static volatile int counter = 0;    public static void main(String[] args) {        ThreadSupplier producer = () -&amp;gt; {            while (true) {                System.out.println(format(&quot;Ustawiam licznik na %s&quot;, ++counter));                Thread.sleep(1000);            }        };        ThreadSupplier consumer = () -&amp;gt; {            int localCounter = counter;            while (true) {                if (localCounter != counter) {                    System.out.println(format(&quot;Licznik zmienił wartość na: %s&quot;, localCounter = counter));                }            }        };        CompletableFuture.allOf(supplyAsync(producer), supplyAsync(consumer)).join();    }}/** * Gdyby ktoś się zastanawiał dlaczego interfejs funkcyjny z dwoma metodami w ogóle działa: * W przypadku interfejsów funkcyjnych pod uwagę brane są jedynie nieabstrakcyjne metody interfejsu. * Metoda interfejsu posiadająca domyślną implementację NIE JEST traktowana jako abstrakcyjna. * * Na podobnej zasadzie działa np. Consumer * https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/function/Consumer.html */@FunctionalInterfaceinterface ThreadSupplier&amp;lt;T&amp;gt; extends Supplier&amp;lt;T&amp;gt; {    default T get() {        try {            return getThrows();        } catch (InterruptedException e) {            // W tym miejscu należy zrobić coś sensownego z wyjątkiem - minimum zalogować.            throw new RuntimeException(e);        }    }    T getThrows() throws InterruptedException;}",
"url": "/2021/05/31/slowo-kluczowe-volatile.html",
"author": "Marcin Mergo",
"authorUrl": "/authors/mmergo.html",
"image": "mmergo.webp",
"highlight": "/assets/img/posts/2021-05-31-slowo-kluczowe-volatile/volatile.jpeg",
"date": "31-05-2021",
"path": "pl/2021-05-31-slowo-kluczowe-volatile"
}
,


"2021-04-30-java-darmowa-czy-nie-update-html": {
"title": "Java darmowa, czy nie? - aktualizacja",
"lang": "pl",
"tags": "java",
"content": "Microsoft dołącza do dystrybutorów JDK. To dobry moment żeby sprawdzić, jak rozwinęła się sytuacja z wydawaniem darmowych dystrybucji od czasu, kiedy Oracle zmienił tryb licencjonowania. Pisaliśmy o tym dwa lata temu w artykule Java darmowa, czy nie?OpenJDK pozostaje miejscem, w którym toczą się prace rozwojowe nad poszczególnymi wersjami Java SE. Szczegółowe plany dla wersji LTS (Long Term Support) można znaleźć na wiki:  JDK 8  JDK 11Kolejną wersją LTS będzie wersja 17, której pierwsze wydanie planowane jest na wrzesień 2021 roku.Zgodnie z tym, co pisaliśmy 2 lata temu, Oracle pozostał przy wydawaniu jedynie referencyjnych wersji z każdej, głównej wersji OpenJDK: https://jdk.java.net/. Oczywiście poza wersjami referencyjnymi, Oracle wydaje regularnie komercyjne wersje oparte również na OpenJDK.AdoptOpenJDK -&amp;gt; Eclipse AdoptiumAdoptOpenJDK został głównym dostawcą darmowej dystrybucji Javy. Zgodnie z raportem zamieszczonym na stronie https://snyk.io/blog/jvm-ecosystem-report-2020/ udział OracleJDK w rynku na przestrzeni lat 2018 - 2019, czyli w momencie zakończenia wydawania darmowej wersji, zmalał o 36%. Większa część tych 36% użytkowników zmigrowała się właśnie do wersji wydawanych przez AdoptOpenJDK. Wg raportu z 2020 roku udział AdoptOpenJDK na rynku używanych dystrybucji wyniósł 24%.W samym AdoptOpenJDK zachodzą natomiast dość istotne zmiany. Społeczność AdoptOpenJDK przechodzi z London Java Community pod skrzydła Eclipse Foundation. Informacje dot. motywacji, jak i samego procesu można przeczytać na blogu AdoptOpenJDK. Nazwa AdoptOpenJDK zostanie zastąpiona przez Eclipse Adoptium.Microsoft Build of OpenJDK6 kwietnia Microsoft opublikował informację o dołączeniu do dystrybutorów darmowej wersji JDK. Binaria, na razie w wersji preview, można pobrać ze strony https://www.microsoft.com/openjdk. Wersja Microsoftu oparta jest na źródłach OpenJDK i budowana przy użyciu skryptów Eclipse Adoptium.Co ciekawe, Microsoft deklaruje, że do końca roku dystrybucja oparta na OpenJDK zostanie domyślną dystrybucją używaną w serwisach dostępnych za pośrednictwem Azure’a. Pozostałe dystrybucje używane przez Microsoft będą również sukcesywnie zastępowane przez wersje pochodzące z OpenJDK.Polityka wydawania nowych wersji, podobnie jak w przypadku AdoptOpenJDK, zakłada kwartalny cykl wydawniczy. Microsoft planuje wydawać wersję JDK 11 do 2024 roku. Do końca bieżącego roku planują również wydać wersję JDK 17.Przydatne linki  https://snyk.io/blog/jvm-ecosystem-report-2020/  https://blog.adoptopenjdk.net/2020/06/adoptopenjdk-to-join-the-eclipse-foundation/  https://devblogs.microsoft.com/java/announcing-preview-of-microsoft-build-of-openjdk/  https://www.microsoft.com/openjdk  Java darmowa, czy nie?",
"url": "/2021/04/30/java-darmowa-czy-nie-update.html",
"author": "Jakub Wilczewski",
"authorUrl": "/authors/jwilczewski.html",
"image": "jwilczewski.jpg",
"highlight": "/assets/img/posts/2021-04-25-java-darmowa-czy-nie-update/java-darmowa.png",
"date": "30-04-2021",
"path": "pl/2021-04-26-java-darmowa-czy-nie-update"
}
,


"2021-04-16-webpack-externals-vs-bundled-html": {
"title": "Micro frontends: czy współdzielić zależności?",
"lang": "pl",
"tags": "frontend angular webpack micro-frontends",
"content": "tl;dr To współdzielić zależności czy nie? Nie zawsze! Jeżeli nie natrafiłeś na problemy z rozmiarem bundli, to nie rozwiązuj teoretycznych problemów. Zanim zabierzesz się za optymalizację - oceń, co jest problemem, określ, co chcesz osiągnąć i zastanów się, jakimi krokami możesz tam dotrzeć. Bieżące narzędzia stosują zaawansowane mechanizmy optymalizacji, które mogą zachwiać Twoje intuicyjne rozumienie problemu ;-)Zarysujmy prosty przykład biznesowo sensownej aplikacji, która posłuży nam do zobrazowania omawianego problemu. Załóżmy, że budujemy system wewnętrznej komunikacji z klientami. Każdy klient ma dostęp do panelu poczty, z którego może przeglądać listę dotychczasowych wątków, prowadzić konwersację oraz przeglądać szczegóły dowolnego wątku i wysłać nową wiadomość, żeby rozpocząć nowy wątek.Poniższy diagram przedstawia przykładowy system zbudowany w podejściu micro frontends. System składa się z czterech niezależnie rozwijanych i osadzanych webcomponentów:  mailbox-webcomponent - Aplikacja hosta odpowiedzialna za uwierzytelnianie użytkownika, layout oraz routing komponentów funkcjonalnych.  thread-webcomponent - Aplikacja prezentująca szczegóły konkretnego wątku.  threads-webcomponent - Aplikacja prezentująca listę dostępnych wątków.  new-message-webcomponent - Aplikacja pozwalająca wysłać nową wiadomość.Poszczególne aplikacje są wczytywane lazy, gdy są potrzebne, a konkretna nawigacja i konfiguracja jest realizowana na poziomie aplikacji hosta. Wszystkie aplikacje są zbudowane z wykorzystaniem Angular + Angular Elements oraz korzystają z NgRx i rx.js.Angular jest ciężki, a Ty masz 4 kopie?! Szalony!Naturalnym pierwszych odruchem jest zwątpienie (żeby nie powiedzieć panika) w sensowność takiego podejścia. Przecież to jest właściwie homogeniczny system i niepotrzebnie dostarczamy te same zależności czterokrotnie!Sprawdźmy, ile ważą zależności i oceńmy potencjalny zysk?            Aplikacja      bundled      vendor.js      angular+rx                  mailbox-webcomponent      505K      434K      294K              new-message-webcomponent      285K      248K      212K              thread-webcomponent      280K      248K      212K              threads-webcomponent      558K      378K      235K      Podsumowując rozmiar zależności Angular+Rx.js w chunku vendor możemy zaryzykować, że do zaoszczędzenia jest nawet 659K!Oczywiście, powinna do tego dojść jeszcze kompresja serwer-przeglądarka, ale rozmiar wynikowy będzie malał proporcjonalnie do wartości w tabeli.Inne pytanie, czemu angular czasem zajmuje więcej, a czasem mniej? Wrócimy do tego w dalszej części wpisu.Czy możemy coś z tym zrobić? Oczywiście! Z pomocą przychodzi webpack i konfiguracja externals (https://webpack.js.org/configuration/externals/#externals). Bez większych trudności możemy określić nie tylko, które moduły trafiają do którego bundle (main, polyfills czy vendor), ale wprost wskazać, że część zależności w ogóle ma nie trafić do wynikowych skryptów i obiecać webpackowi: “jakoś to będzie, nie martw się, przyjdą z zewnątrz”.Konfiguracja współdzielenia zależności pomiędzy aplikacjamiCo właściwie robimy?  W osadzanych aplikacjach oznaczamy zależności jako zewnętrzne  W aplikacji hosta eksponujemy jego zależności jako globalneW komponentach threads, thread i new-message oznaczamy zależności jako zewnętrzne:{    // ....,    externals: {        &#39;rxjs&#39;: &#39;rxjs&#39;,        &#39;rxjs/operators&#39;: &#39;rxjs.operators&#39;,        &#39;@angular/core&#39;: &#39;ng.core&#39;,        &#39;@angular/common&#39;: &#39;ng.common&#39;,        &#39;@angular/common/http&#39;: &#39;ng.common.http&#39;,        &#39;@angular/compiler&#39;: &#39;ng.compiler&#39;,        &#39;@angular/platform-browser&#39;: &#39;ng.platformBrowser&#39;,        &#39;@angular/platform-browser-dynamic&#39;: &#39;ng.platformBrowserDynamic&#39;,        &#39;@angular/router&#39;: &#39;ng.router&#39;,        &#39;@angular/forms&#39;: &#39;ng.forms&#39;,        &#39;@angular/elements&#39;: &#39;ng.elements&#39;    }}W aplikacji hosta dodajemy wymagane skrypty do skryptów globalnie dołączanych do aplikacji.Korzyści ze współdzielenia zależnościZobaczmy zatem, ile faktycznie ugraliśmy wyrzucając Angular i rx.js poza aplikacje.            Aplikacja      main.js (bundled)      main.js (externals)      diff      scripts                  mailbox-webcomponent      505K      227K      278K      1.2M              new-message-webcomponent      285K      101K      184K      -              thread-webcomponent      280K      95K      185K      -              threads-webcomponent      558K      357K      201K      -      Okazuje się, że finalnie straciliśmy na całej operacji! Na poszczególnych aplikacjach zaoszczędziliśmy 848K, ale w hoście dołożyliśmy 1.2M skryptów z zależnościami. Mieliśmy oszczędzić 659K, a dołożyliśmy ~350K. W dodatku, w najgorszym możliwym miejscu, bo podczas ładowania pierwszego, początkowego bundla (wcześniej było to rozłożone na bundle ładowane lazy w razie potrzeby).Co się właściwie stało?Bundler, Tree Shaking, minification, AoT i inniNa początek zastanówmy się, z czego wynikają różnice w wielkości dołączanych bibliotek pomiędzy aplikacjami? W tym celu porównajmy, co trafia do wynikowego kodu przed i po zastosowaniu optymalizacji na zależnościach i bundle.Poniższe obrazy przedstawiają wizualizację analizy wynikowego bundle webpacka:  vendor.js - reprezentuje aplikację bez optymalizacji,  vendor.43b31…js - reprezentuje aplikację z włączonymi optymalizacjami,  na różowo oznaczone są rozmiary zależności widziane przez webpack, gdzie stat size to rozmiar początkowy, a gzipped size to rozmiar w wynikowym bundle.Dla uproszczenia, rozmiary części zależności umieścimy w tabeli:            Zależność      stat (unoptimized)      gzip (unoptimized)      stat (optimized)      gzip (optimized)      gzip (diff)                  @angular/core/core.js      1.26M      280K      1.26M      36K      244K              @angular/common      215K      48K      159K      6K      42K              @angular/forms      262K      46K      222K      7K      39K              @angular/common/http      87K      20K      82K      5K      15K              @angular/platform-browser      85K      20K      75K      4K      16K              @angular/elements      24K      6K      24K      2K      4K              date-fns      516K      100K      187K      10K      90K              rxjs      245K      44K      92K      9K      35K      Porównując wartości unoptimized i optimized łatwo wychwycimy różnicę w rozmiarach zależności w wynikowej aplikacji. Już te kilka przykładów pokazuje różnicę ~500K w skompresowanym kodzie!Bundler (np. webpack) w trakcie budowania aplikacji ma okazję skorzystać z wielu technik optymalizacji, takich jak Tree Shaking, importy, AoT compilation, minification, itp., które pozwalają mu:  zbierać tylko faktycznie używane elementy zależności,  minifikować nazwy od strony zależności i jej klienta,  optymalizować wykonania kodu,  itp.Możliwe optymalizacje dobrze widać na przykładzie biblioteki rxjs gdzie większość jej operatorów została pominięta, a sam rozmiar biblioteki został zredukowany o ~80%!Sam proces optymalizacji kodu i jego zależności to zupełnie osobny, rozbudowany temat, któremu nie możemy przyjrzeć się po prostu “przy okazji”. Jednak na nasze potrzeby sama świadomość procesu optymalizacji i jej wpływu na wynikowy rozmiar jest wystarczająca.Rozmiar zależności w bundle, a rozmiar z npmBudując konkretną aplikację z konkretnymi zależnościami proces budowania może wykonać ciężką pracę i wyznaczyć minimalny działający zakres kodu. Dodatkowo, taki kod może zostać dalej optymalizowany, bo mamy możliwość modyfikowania zarówno samej zależności, jak i kodu, który z niej korzysta. Możemy nawet przyjąć, z częściowym zaufaniem, że wynikowa aplikacja nie zawiera ani kawałka kodu, który nie jest przez nią faktycznie wykorzystywany w runtime.Zupełnie inaczej sytuacja wygląda, gdy zależności dostarczamy w runtime. Na etapie buildu nie możemy ani wnioskować, które fragmenty kodu będą potrzebne, ani wprowadzać optymalizacji w źródle. Nie znając potrzeb aplikacji klienckich, ani nie mogąc wpływać na sposób wykorzystania dostarczanych zależności, musimy je dostarczyć as-is w całości.W naszym przykładzie, różnica między aplikacją przed i po wprowadzeniu externals wynika wprost z tego, że ta druga musi dostarczyć masę kodu, którego nikt nie potrzebuje. W efekcie, realizacja wprost zaprzeczyła założeniom eksperymentu :)To kiedy współdzielić zależności?Zawsze, gdy jest ku temu przesłanka! ;) Jest kilka dodatkowych czynników sugerujących wprowadzenie współdzielenia zależności:  nie masz kontroli nad aplikacjami klienckimi,  wykorzystywane zależności nie poddają się łatwo optymalizacji,  zależności nie poddają się łatwo tree shaking,  gdy liczba aplikacji sprawia, że oszczędność będzie większa niż narzut,  gdy stosowana zależność nie lubi być w wielu instancjach w DOM (np. różnego rodzaju globalne zależności).To, co chcieliśmy pokazać, to że współdzielenie nie jest oczywistą rzeczą, którą “należy zawsze robić”. Za każdym razem należy przeanalizować obecne problemy i metody ich rozwiązania. Nie warto wprowadzać rozwiązań problemów, których się nie ma ;)",
"url": "/2021/04/16/webpack-externals-vs-bundled.html",
"author": "Grzegorz Lipecki",
"authorUrl": "/authors/glipecki.html",
"image": "glipecki.jpg",
"highlight": "/assets/img/posts/2021-04-16-webpack-externals-vs-bundled/scale.jpeg",
"date": "16-04-2021",
"path": "pl/2021-04-16-webpack-externals-vs-bundled"
}
,


"2021-04-14-wspoldzielona-biblioteka-w-jenkins-pipeline-html": {
"title": "Współdzielona biblioteka w Jenkins pipeline",
"lang": "pl",
"tags": "jenkins devops pipeline shared library",
"content": "W tym artykule zapoznamy się z mechanizmem bibliotek współdzielonych (Pipeline: Shared Groovy Libraries), które stanowiązależność do szeroko stosowanej wtyczki jenkinsowej pipelines (https://www.jenkins.io/doc/book/pipeline/).Każdy z nas podczas modelowania procesów CI/CD spotkał się z pewnymi podobieństwami pomiędzy projektami. Niewątpliwiejednym z takich procesów może być release projektu. W naszej firmie prawie każdy projekt budowany jest przy użyciumavena. Tym samym release takich projektów jest procesem bardzo zunifikowanym. W związku z tym jest to idealnykandydat, by zamknąć go w bibliotece, ustawić na półkę i używać, gdy zajdzie taka potrzeba.Zakładając, że każdy projekt w procesie wydawniczym (pipeline) ma krok pod tytułem “release”, wówczas wystarczy, że zbudujemy współdzieloną bibliotekę, którą użyjemy w kroku bez zagłębiania się w sposób działania.Biblioteka ma zapewnić nam:  podbicie wersje w POMie do stabilnej,  upload artefaktów (binarki) do repozytorium,  ustalenie nowej wersji developerskiej + commit do głównej gałęzi.Tworzenie biblioteki w Jenkins pipelineW nowo utworzonym repozytorium kodu tworzymy strukturę katalogów. W naszym przykładzie git@git.consdata/consdata-shared-lib(projekt)+- src|  +- com.consdata.shared.library|   +- VersionBumper.groovy  # klasa(pomocnicza) generuje wersje artefactu+- vars|  +- release.groovy # Definicja zmiennej ‘release’ dostępna z pipelinuW ten sposób stworzyliśmy zmienną globalną o nazwie release. Aby można było ją wywołać bezpośrednio po nazwie wewnątrz Jenkins pipeline, definiujemy funkcję call.//vars/release.groovyimport com.consdata.shared.library.VersionBumperdef call(String branchName, String gitCredentialId, String versionToBump) {    def currentVersion = readMavenPom().getVersion()    def bumper = new VersionBumper(currentVersion)    releaseVersion = currentVersion.minus(&quot;-SNAPSHOT&quot;);    snapshotVersion = bumper.bump(versionToBump)    sh &quot;mvn clean versions:set -DnewVersion=\\&quot;$releaseVersion\\&quot; -DgenerateBackupPoms=false&quot;    sshagent(credentials: [gitCredentialId]) {        sh(&quot;git commit -a -m &#39;Release $releaseVersion&#39;&quot;)    }    releaseShaCommit = sh(script: &quot;git rev-parse HEAD&quot;, returnStdout: true).trim()    echo &quot;SHA Commit $releaseShaCommit&quot;    sh &quot;mvn clean versions:set -DnewVersion=\\&quot;$snapshotVersion\\&quot;-SNAPSHOT -DgenerateBackupPoms=false&quot;    sshagent(credentials: [gitCredentialId]) {        sh(&quot;git commit -a -m &#39;Snapshot $snapshotVersion&#39;-SNAPSHOT&quot;)        sh(&quot;git push origin HEAD:$branchName&quot;)        sh(&quot;git checkout $releaseShaCommit&quot;)    }    sh &quot;mvn clean deploy&quot;    sshagent(credentials: [gitCredentialId]) {        sh(&quot;git tag $releaseVersion&quot;)        sh(&quot;git push --tags&quot;)    }}W kodzie powyżej wykorzystaliśmy klasę VersionBumper, która dostarcza nam funkcjonalność wyliczania nowej wersji. Napotrzeby tego artykułu, implementacja została pominięta.Definiowanie bibliotekiTak przygotowaną bibliotekę musimy dodać do Jenkinsa. Ponieważ biblioteka będzie stosowana globalnie na poziomie każdegoprojektu. Tym samym zdefiniujemy ją na najwyższym poziomie.Zarządzaj Jenkinsem → Skonfiguruj system → Global Pipeline Libraries (Uwaga: Należy zweryfikować, czy pluginJenkins pipeline: Shared Groovy Libraries - https://plugins.jenkins.io/workflow-cps-global-lib jest aktywny.)Użycie w pipelinePrzechodzimy do ostatniego etapu wykorzystania biblioteki wewnątrz pliku Jenkinsfile.@Library([&#39;consdata-shared-lib&#39;]) _ //Adnotacja określająca nazwę bibliotekipipeline {    agent any    stages {        stage(&#39;Release&#39;) {            steps {                release(&quot;master&quot;, env.GIT_CREDENTIAL_ID, &quot;MINOR&quot;)                //nazwa &#39;release&#39; wynika z konwencji,                //jest to nazwa pliku w repozytorium biblioteki /vars/release.groovy            }        }    }}PodsumowanieW artykule zastosowaliśmy tylko niewielki wycinek mechanizmu współdzielonych bibliotek i wykorzystaliśmy go w Jenkins pipeline. Szerszy kontekst dostępny jestbezpośrednio w dokumentacji.",
"url": "/2021/04/14/wspoldzielona-biblioteka-w-jenkins-pipeline.html",
"author": "Dawid Kubiak",
"authorUrl": "/authors/dkubiak.html",
"image": "dkubiak.webp",
"highlight": "/assets/img/posts/2021-04-14-wspoldzielona-biblioteka-w-jenkins-pipeline/jenkins-shard-pipeline.jpeg",
"date": "14-04-2021",
"path": "pl/2021-04-14-wspoldzielona-biblioteka-w-jenkins-pipeline"
}
,


"2021-03-31-rest-api-dobre-praktyki-html": {
"title": "Standard HTTP i REST - dobre i złe praktyki",
"lang": "pl",
"tags": "http rest rest api",
"content": "HTTP i REST - czy zawsze dobrze je stosujemy?Jakie są częste błędy przy doborze metod HTTP?  Kiedy trzymać się standardu, a kiedy może to być problemem? O czym pamiętać projektując swoje REST API? Choć ani standard HTTP, ani REST nie są nowością w świecie IT, to pewne twierdzenia ich dotyczące są błędnie powielane przez wiele osób. Czasem znów okazuje się, że deweloperzy pracujący dla różnych firm podchodzą do tego samego problemu w inny sposób. Zapraszam na podróż w znane, ale wciąż jeszcze nie zawsze przestrzegane, standardy HTTP i REST.Kiedy POST, a kiedy PUT?POST do tworzenia, a PUT do modyfikacji? NiekoniecznieCzęsto można spotkać się z prostym (lecz błędnym) wyjaśnieniem, kiedy używać metody POST, a kiedy PUT. Twierdzi się, że kiedy tworzymy nowy zasób — POST, kiedy modyfikujemy istniejący — PUT. Jest to jednak znaczne uproszczenie i pierwszy mit, z którym chcę się rozprawić.Zacznijmy od tego, że PUT również może tworzyć zasób, więc nie jest to wcale różnica między PUT i POST wynikająca ze standardu HTTP. Co więcej, POST ma o wiele szersze zastosowanie niż działanie na zasobach. Może np. być użyty do przekazania danych z formularza lub innych danych, wiadomości, które zostaną przetworzone przez serwer praktycznie w dowolny sposób. Niekoniecznie celem jest utworzenie (lub też modyfikacja) czegokolwiek, efektem może być np. wysłanie wiadomości, rozpoczęcie procesu. W wyniku żądania może, lecz nie musi, powstać zasób identyfikowany przez URI. Jeśli powstanie, należy zwrócić kod odpowiedzi 201 (Created). Jeśli nie powstanie taki zasób, należy zwrócić kod 200 (OK) lub 204 (No Content). Natomiast metoda PUT odnosi się do konkretnego zasobu, który w jej wyniku może zostać zastąpiony reprezentacją zawartą w przekazanych danych (wówczas zwracamy 200 (OK) lub 204 (No Content)). Jeśli taki zasób nie istnieje, w zależności od naszej decyzji możemy go utworzyć, zwracając 201 (Created) lub odpowiedzieć kodem błędu, jeśli nie chcemy wspierać takiej obsługi. GET wykonany tuż po przetworzeniu metody PUT zwróci nam dokładnie tę samą reprezentację, która była przekazana w metodzie PUT. Warto jednak podkreślić, że PUT może mieć wpływ na inne powiązane zasoby.Na razie więc widzimy, że choć POST i PUT mają ze sobą dużo wspólnego, POST może mieć szersze zastosowanie i może w dowolny sposób przetwarzać przekazane dane. Natomiast PUT odnosi się do konkretnego zasobu określonego przez jego identyfikator i powinien być obsłużony przez zastąpienie obecnej reprezentacji zasobu reprezentacją przekazaną w żądaniu. Nie tłumaczy to jednak głównej różnicy między POST a PUT, którą możemy zawrzeć w haśle „idempotentność”.IdempotentnośćTermin ten, wywodzący się z matematyki, oznacza tutaj, że operacja idempotentna wykonana wielokrotnie nie doprowadzi do błędu, a jej wynik będzie za każdym razem taki sam. PUT jest idempotentny, natomiast POST nie. Przy okazji warto zauważyć, że dwie inne popularne metody HTTP — GET i DELETE — są również idempotentne.Parokrotne wywołanie metody PUT spełnia te zasady (o ile nasza implementacja jest zgodna ze standardem).PUT /messages/1234{  &quot;title&quot;: &quot;Hello!&quot;,  &quot;text&quot;: &quot;Do you know that PUT is indempotent?&quot;}POST bez ciała (body)?Biorąc pod uwagę to, że metoda POST ma bardzo szerokie zastosowanie i nie jest idempotentna, w skrajnym przypadku możemy nie chcieć przekazywać żadnych danych. Nasze żądanie może być np. jedynie wyzwalaczem (trigger) jakiegoś procesu i nie potrzebujemy przekazywać nic więcej. W innym przypadku możemy przekazać potrzebne informacje w nagłówkach HTTP, a ponieważ reprezentacja to połączenie nagłówków z danymi, to reprezentacja interpretowana przez serwer nie będzie wcale pusta. Nie będzie to sprzeczne ze standardem HTTP. Należy jedynie pamiętać o poprawnej interpretacji w zależności od technologii, z której korzystamy, zarówno po stronie serwera, jak i klienta. Jeśli np. standardowo posługujemy się formatem application/json i tu też posłużymy się takimi nagłówkami, puste żądanie może być uznane za niepoprawne w przeciwieństwie do przekazania {}. Problem może wystąpić również na innym poziomie infrastruktury, przykładowo tak spreparowane żądanie może zostać zablokowane na WAF-ie (Web application firewall), jeśli została ustalona reguła, którą będzie ono łamać.GET z dużą liczbą parametrówOpis problemuStandard nie określa ograniczenia na rozmiar URI, jednakże poszczególne implementacje już je mają (zazwyczaj mieszczący się między 2 KB, a 8 KB). Co ważne, w zależności od stosowanych technologii musimy mieć nad nimi kontrolę od strony serwerowej. Z kolei same przeglądarki mają różne ograniczenia. Bezpiecznie jest więc trzymać się dolnej granicy leżącej poniżej 2048 znaków (możemy też spotkać się z zaleceniami, by utrzymywać rozmiar URI poniżej 2000 znaków). Czy w ogóle tak długie URLe są potrzebne, czytelne i wskazane? Odpowiadając na pierwszą wątpliwość, mogą być dosyć częste przy wyszukiwaniach po wielu kryteriach, dodatkowo z paginacją i sortowaniem. Niewątpliwie czytelność parametrów w URI jest dużo mniejsza niż przy przekazaniu parametrów w ciele żądania w formacie json czy XML. Wyszukiwanie nie zmienia stanu zasobów, czyli powinniśmy skorzystać i zazwyczaj korzystamy z GETa do pobierania danych. Metoda ta jest idempotentna, możemy korzystać z gotowych mechanizmów zapewniających nam cache’owanie lub powtórne wywołanie GETa w przypadku błędu sieciowego. I jak najbardziej wydaje się odpowiednia, skoro chcemy pobrać dane.Przykładowy GET:GET /locations/?country=France,Italy,Greece,......,Brazil&amp;amp;minimum_population=250000&amp;amp;type=city,village&amp;amp;max_height=1300&amp;amp;min_area=100000&amp;amp;.......Czasami parametrów nie chcemy przekazywać w ten sposób jeszcze z innego powodu. Serwery często logują całą treść zapytania, co może od razu wykluczać pewne zastosowania, np. przesyłanie wrażliwych danych. Dodatkowo taki url będzie dostępny w historii, może zostać udostępniony przez nieświadomego użytkownika komuś innemu, będzie widoczny przez dostawcę usług internetowych. Taka podatność pozwoli atakującym uzyskać dostęp do wrażliwych danych.Sednem problemu jest jednak to, że standard zakłada, że parametry będą przekazane w URL-u, a nie w ciele metody. Co więc w przypadku, gdy naszych parametrów jest za dużo, żeby trzymać się tego standardu lub gdy są to dane wrażliwe?Rozwiązań jest kilka i każde ma swoje wady i zalety.GET i parametry przekazywanie nie w URI, a w ciele żądaniaPierwszy pomysł rozwiązania tego problemu to przekazanie parametrów w ciele metody GET. Każda metoda HTTP może mieć ciało, ale w przypadku GETa pojawia się kilka wątpliwości.  Nie jest to zgodne z założeniami i praktyką. Co prawda standard pozwala na to, żeby dowolna metoda HTTP miała ciało, ale przez wiele lat i w wielu implementacjach założono, że ciało metody GET będzie ignorowane przez serwer. Znajdziemy się więc w dosyć nietypowej sytuacji, w której, chociaż nie jesteśmy niezgodni ze standardem, to jednak ów standard pomija kwestię, jak powinno być traktowane ciało metody GET. W związku z tym również konkretne implementacje nie będą wspierać tego rozwiązania.  Wiele implementacji po stronie frontu i serwera nie wspiera GET-a z ciałem. Część bibliotek HTTP (np. JavaScript) nie wspiera wysyłania żądań GET z ciałem. Różne serwery i proxy mogą ignorować dane przekazane w ten sposób.  Tracimy łatwe wsparcie dla cache’owania, ponieważ działa ono na podstawie URL-a, a nie ciała metody.  Rozwiązanie może stworzyć problemy związane z konfiguracją blokującą takie żądania na WAF-ie (Web application firewall), jeśli osoby odpowiedzialne za konfigurację WAF-a uznały, że żądania GET nie powinny mieć ciała.GET /locations{  &quot;country&quot;: &quot;France,Italy,Greece,......,Brazil&quot;,  &quot;minimum_population&quot;: 250000,  &quot;type&quot;: &quot;city, village&quot;,  &quot;max_height&quot;: 1300,  &quot;min_area&quot;: 100000,  ....}{  &quot;locations&quot;: [    {&quot;id&quot;: &quot;2443&quot;, &quot;name&quot;: &quot;Rome&quot;},    {&quot;id&quot;: &quot;6789&quot;, &quot;name&quot;: &quot;Paris&quot;},    ...  ]}Czy takie rozwiązania są spotykane w praktyce? Warto wiedzieć, że twórcy Elasticsearch są zwolennikami stosowania GETa w ten sposób, argumentując to tutaj. W swoim Search API umożliwiają zarówno skorzystanie z metody GET, jak i z metody POST, którą omówimy za chwilę.POSTDrugi pomysł, będący obejściem na problemy poprzedniego rozwiązania, to skorzystanie z innej metody HTTP — POST. Jest to dosyć pragmatyczne podejście, przy którym jednak tracimy zalety, które dawał nam GET z parametrami w URLu, czyli:  łatwe wsparcie dla cache’owania. Co prawda standard RFC 7234 zakłada wsparcie dla metod GET, HEAD i POST, ale większość implementacji wspiera tylko metody GET i HEAD.  wsparcie dla ponawiania żądania przez kliencką bibliotekę HTTP przy błędzie sieciowym.  zgodność ze standardem. Można też spierać się o to, jak bardzo to rozwiązanie jest nieeleganckie lub czy nie wprowadzi osób korzystających z naszego API w błąd.POST /locations/search{  &quot;country&quot;: &quot;France,Italy,Greece,......,Brazil&quot;,  &quot;minimum_population&quot;: 2500000,  &quot;type&quot;: &quot;city, village&quot;,  &quot;max_height&quot;: 1300,  &quot;min_area&quot;: 100000,  ....}{  &quot;locations&quot;: [    {&quot;id&quot;: &quot;2443&quot;, &quot;name&quot;: &quot;Rome&quot;},    {&quot;id&quot;: &quot;6789&quot;, &quot;name&quot;: &quot;Paris&quot;},    ...  ]}Czy takie rozwiązanie są spotykane w praktyce? Tak, np. zdecydowali się na to twórcy Dropboxa, argumentując swoją decyzję tutaj.GET i POSTTrzeci pomysł to próba połączenia dwóch poprzednich rozwiązań w taki sposób, żebyśmy byli zgodni ze standardem i intuicyjnym rozumieniem metod (czyli do pobierania danych ma służyć GET, a nie POST). Najpierw tworzymy zapytanie wyszukiwania, przekazując w ciele POST-a parametry wyszukiwania. W odpowiedzi otrzymujemy id naszego wyszukiwania. Korzystamy z GETa i ze zwróconego wcześniej id, aby otrzymać nasz wynik.  Możemy odczuć, że zaczynamy wprowadzać nadmiernie skomplikowany mechanizm, a przecież chcemy jedynie otrzymać wynik wyszukiwania. W tym celu konieczne będą aż dwa wywołania usług i cały mechanizm na serwerze zapisujący gdzieś te wyniki wyszukiwania.  Można w tej koncepcji wprowadzić cache’owanie.  Jesteśmy zgodni ze standardem.POST /locations/search{  &quot;country&quot;: &quot;France,Italy,Greece,......,Brazil&quot;,  &quot;minimum_population&quot;: 2500000,  &quot;type&quot;: &quot;city, village&quot;,  &quot;max_height&quot;: 1300,  &quot;min_area&quot;: 100000,  ....}{  &quot;id&quot;: &quot;1297612456456&quot;}GET /locations/search?id=1297612456456{  &quot;locations&quot;: [    {&quot;id&quot;: &quot;2443&quot;, &quot;name&quot;: &quot;Rome&quot;},    {&quot;id&quot;: &quot;6789&quot;, &quot;name&quot;: &quot;Paris&quot;},    ...  ]}Na pewno nie jest to popularne rozwiązanie, jest najbardziej kosztowne i ryzykowne, ale zyskujemy zgodność ze standardem.Jak poprawnie odpowiadać na żądaniaRozróżnianie między niewłaściwą ścieżką, nieistniejącym zasobem, brakiem uprawnień…Częstym błędem w odpowiedzi na żądania jest zwracanie nieprawidłowego (niezgodnego ze standardem) kodu błędu.  Należy pamiętać, że odpowiedź musi być zrozumiała i łatwa w interpretacji dla użytkownika naszego API, a z drugiej strony nie można zapomnieć o kwestiach związanych z bezpieczeństwem.Zacznijmy od przykładu.GET /films/1234Response: 404Co będzie oznaczała odpowiedź z kodem błędu 404? Możemy podejrzewać np. brak takiego zasobu, albo np. że zasób istnieje, ale nie mamy uprawnień do niego. Inną nasuwającą się opcją jest to, że ścieżka jest niewłaściwa. Dlatego część osób uważa, że należy zwracać inny kod błędu w każdym z tych przypadków.Załóżmy więc, że będziemy zwracać 403, jeśli zasób istnieje, a użytkownik nie ma do niego dostępu. W ten sposób można łatwo poprzez enumerację i przekazywanie kolejnych id, dowiedzieć się, ile jest zasobów danego typu i poznać ich identyfikatory.GET /films/1Response: 403GET /films/2Response: 404...Dlatego też w zależności od tego, czy bezpieczne jest ujawnianie takich informacji klientowi, możemy zwracać albo oba kody błędu - 403 i 404 dla rozróżnienia obu przypadków, albo samo 404, gdy chcemy postawić na jedną możliwość. Jeśli wystawiamy serwisy wewnętrznie, to o wiele przydatniejsze będzie skorzystanie z kodu 403. Warto pamiętać, że kod 404 będzie odpowiedni w sytuacji, gdy nie chcemy ujawnić informacji, dlaczego nie zwracamy zasobu.Przejdźmy do innego przykładu. Czy 404 oznacza, że zasób nie istnieje, czy że ścieżka jest niewłaściwa? Trudno powiedzieć, jeśli zwrócimy jedynie kod błędu 404.GET /firms/100Response: 404GET /films/100Response: 404W tym przypadku zalecane jest pozostanie przy kodzie 404, ale dodanie do odpowiedzi przyczyny błędu. Inny sposób — z dwoma różnymi kodami błędu, jest niezgodny ze standardem.GET /firms/100Response: 404Invalid path: /firmsGET /films/100Response: 404No film with id: 100Szczegóły błęduJeśli wystawiamy puliczne API, z którego korzystać będzie wielu deweloperów, możemy chcieć podzielić się bardzo dokładnym szczegółem błędu. W innym przypadku możemy ze względów bezpieczeństwa nie przekazywać w wersji produkcyjnej tak szczegółowych informacji na temat błędu. W ogólności poza obowiązkowym kodem błędu można zaproponować dodatkowe pola, przedstawione w poniższym przykładzie.Response: 400{  &quot;errorCode&quot; : &quot;12355&quot;,  &quot;developerMessage&quot; : &quot;Message for developers of your API&quot;,  &quot;userMessage&quot; : &quot;Message for end user&quot;,  &quot;moreInfo&quot; : &quot;http://www.myapi.com/help/errors/12355&quot;}Poprawne odpowiedziPoza kodem odpowiedzi często zwracamy dane — warto trzymać się następujących zasad:  nie zwracamy kluczy i wartości związanych z wewnętrzną serwerową reprezentacją i implementacją,  nie zwracamy wartości jako klucze.Przykład zgodny z zasadami{  &quot;locations&quot;: [    {&quot;id&quot;: &quot;2443&quot;, &quot;name&quot;: &quot;Rome&quot;},    {&quot;id&quot;: &quot;6789&quot;, &quot;name&quot;: &quot;Paris&quot;}  ]}Przykład łamiący pierwszą zasadę{  &quot;locations&quot;: [    {&quot;id&quot;: &quot;2443&quot;, &quot;name&quot;: &quot;Rome&quot;, &quot;internalId&quot;: &quot;345345&quot;, &quot;node&quot;: &quot;1&quot;},    {&quot;id&quot;: &quot;6789&quot;, &quot;name&quot;: &quot;Paris&quot;, &quot;internalId&quot;: &quot;235435345&quot;, &quot;node&quot;: 2}  ]}Przykład łamiący drugą zasadę{  &quot;locations&quot;: [    {&quot;2443&quot;: &quot;Rome&quot;},    {&quot;6789&quot;: &quot;Paris&quot;}  ]}Metoda PATCHPATCH a PUTMetoda PATCH została wprowadzona do standardu w RFC 5789 w 2010 roku, nie był to jednak wówczas nowy pomysł. Koncepcja opiera się na tym, że w przeciwieństwie do metody PUT, nie przekażemy całego zasobu, a jedynie zmiany. W związku z tym metoda nie jest idempotentna w przeciwieństwie do metody PUT.Załóżmy, że chcemy zaktualizować liczbę ludności miasta, przechowywaną w polu  population i dodać wartość, która dotychczas nie była zdefiniowana, dla gęstości zaludnienia density. Wówczas oczywiście PUT z pełnymi danymi tylko po to, żeby zaktualizować dwie wartości z wielu, nie ma sensu.PUT /locations/6789{  &quot;name&quot; : &quot;Paris&quot;,  &quot;country&quot;: &quot;France&quot;,  &quot;population&quot;: 2206488,  &quot;density&quot;: 20000,  &quot;type&quot;: &quot;city&quot;,  &quot;height_a&quot;: 1300,  &quot;min_area&quot;: 105.4,  &quot;time_zone&quot;: &quot;UTC+01:00&quot;,  &quot;amsl&quot;: 35,  ....}Zazwyczaj czytając o metodzie PATCH, zetkniemy się z następującym przykładem, który jednak nie jest zgodny ze standardem.PATCH /locations/6789{  &quot;population&quot;: 2206488,  &quot;density&quot;: 20000}Na postawie dokumentu RFC 7396 można zaproponować drobną modyfikację tego przykładu.PATCH /locations/6789Content-Type: application/merge-patch+json{  &quot;population&quot;: 2206488,  &quot;density&quot;: 20000}Innym podejściem, opisanym w RFC 6902 jest zapis stosujący operacje. Taki obiekt składa się z jednego operatora, określającego, jaką akcję należy wykonać. Są to add, remove, replace, move, copy, test. W żądaniu możemy przekazać kilka operacji. W przypadku metody PATCH serwer musi zaaplikować nasze żądanie atomowo, czyli mamy pewność, że albo wszystkie pola zostaną zmodyfikowane w żądany sposób, albo żadne (w przypadku błędu). PATCH /locations/6789Content-Type: application/json-patch+json   [     { &quot;op&quot;: &quot;test&quot;, &quot;path&quot;: &quot;/population&quot;, &quot;value&quot;: 2175601 },     { &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/population&quot;, &quot;value&quot;: 2206488 },     { &quot;op&quot;, &quot;add&quot;, &quot;path&quot;: &quot;/density&quot;, &quot;value&quot;: 20000},   ]Czy w praktyce PATCH jest często stosowany? Czy faktycznie deweloperzy stosują PUT tylko zgodnie z zasadą, że należy przekazać cały zasób? W wielu przypadkach stosowanie jest obejście w postaci użycia POSTa.Przykład z blokowaniem na WAFChcąc jednak być spójni ze standardem i stosując metodę PATCH w odpowiednim przypadku, możemy zorientować się, że żądania PATCH trafiające na nasz serwer są po drodze wycinane — blokowane przez WAF. WAF, którego celem jest zapewnienie bezpieczeństwa, jest konfigurowany zazwyczaj przez inny zespół, czasem z innej firmy lub przez klienta, dla którego dostarczamy rozwiązanie. Dlatego tego typu problemy są dosyć częste. Konfiguracja WAF jest oparta na blokowaniu niedozwolonych żądań i odpowiedzi lub akceptowaniu tylko dozwolonych.Czy w opisanym przypadku blokowanie żądań, które korzystają z metody PATCH, było uzasadnione? Zapewne wynikało z tego, że konfiguracja była zawężona najbardziej, jak to było możliwe. W tym przypadku można oczywiście zmienić konfigurację WAF, ale nierzadko w tego typu sytuacjach w praktyce dostosowujemy się do infrastruktury klienta, któremu dostarczamy oprogramowanie i decydujemy się na zmianę np. na PUT. W ten sposób nie jesteśmy już zgodni ze standardem. Widać tu wyraźnie różnicę między sytuacją, gdy wystawiamy API dla potencjalnie wielu klientów i wówczas trzymanie się standardów ułatwi innym zrozumienie naszego API, a sytuacją, gdy tworzymy rozwiązanie dla jednego klienta, które działa na jego infrastrukturze, a my staramy się być elastyczni. W drugim przypadku jakość naszego API będzie dużo niższa. Nie można jednak zapominać, że zazwyczaj reguły na WAFie służą poprawie bezpieczeństwa i zabezpieczeniem przed znanymi i dobrze opisanymi atakami. Dlatego często to właśnie brak trzymania się standardów doprowadzi do problemów na WAF-ie.PodsumowanieKrótka podróż po polecanych i odradzanych praktykach w obszarze REST i HTTP dobiegła końca, a jest to jedynie wycinek tych zagadnień. Te konkretne przypadki to próba rozprawienia się z często spotykanymi błędami. Ale najważniejszy być może wniosek to: uczmy się standardów, by albo je stosować, albo — w uzasadnionych przypadkach — świadomie łamać.",
"url": "/2021/03/31/rest-api-dobre-praktyki.html",
"author": "Barbara Mitan",
"authorUrl": "/authors/bmitan.html",
"image": "bmitan.jpg",
"highlight": "/assets/img/posts/2021-03-31-rest-api-dobre-praktyki/RESTApi.jpeg",
"date": "31-03-2021",
"path": "pl/2021-03-31-rest-api-dobre-praktyki"
}
,


"2021-03-03-przyklad-migracji-do-chmury-html": {
"title": "Migracja aplikacji do chmury Google Cloud Platform w praktyce",
"lang": "pl",
"tags": "cloud serverless gcp googlecloud cloud migration google cloud platform",
"content": "W poprzednim wpisie pt. “Migracja do chmury - czyli od czego zacząć?”przedstawiłem etapy procesu migracji aplikacji do chmury, a także opisałem poszczególne strategie migracji.W tym wpisie chciałbym przedstawić migrację do chmury Google Cloud Platform w praktyce. Na warsztat wezmę działającą aplikację demo, która w żaden sposób nie jest przystosowana do uruchomienia w chmurze, przedstawię architekturę docelową przy wykorzystaniu każdej z opisywanych wcześniej strategii oraz zaimplementuję jedną z nich.Słowem wstępuKody źródłowe aplikacji demo są dostępne na GitHubie:  aplikacja źródłowa (przed migracją)  aplikacja docelowa (po migracji)Dla przypomnienia, 4 etapy procesu migracji do chmury, opisane w poprzednim wpisie:SzacowanieNa etapie szacowania powinniśmy dokonać analizy istniejącego systemu/infrastruktury. Na potrzeby wpisu przygotowałem aplikację demo, która nie jest uruchomiona produkcyjnie, stąd trudno byłoby oszacować wykorzystywane zasoby. Nie chcąc wróżyć z fusów, na etapie szacowania przedstawię jedynie architekturę aplikacji oraz jej działanie.Zadaniem aplikacji jest przyjęcie danych od użytkownika za pomocą formularza, przetworzenie ich, utrwalenie w bazie danych oraz umożliwienie użytkownikowi odczytu tych danych. System składa się z kilku elementów:  aplikacji frontendowej - napisanej w Angularze (TypeScript), której zadaniem jest przyjmowanie danych od użytkownika oraz ich prezentacja;  aplikacji backendowej - napisanej w Javie (Spring Boot), której zadaniem jest przyjęcie danych z aplikacji frontendowej i przesłanie ich do kolejki oraz odczytanie z bazy danych;  kolejki - opartej o RabbitMQ, której celem jest optymalizacja zapisu;  bazy danych - w postaci nierelacyjnej bazy MongoDB, która przechowuje przesłane dane.Wykorzystałem kolejkę, aby dane przesyłane przez użytkowników były od razu przyjmowane, a użytkownik otrzymywał potwierdzenie przyjęcia ich przez system. Przetwarzanie danych może być w teorii czasochłonnym procesem, dlatego dzięki wykorzystaniu kolejki, użytkownik nie musi czekać na przetworzenie danych, aby uzyskać odpowiedź.Dane pobierane są z kolejki przez aplikację backendową, następnie przetwarzane (w przykładowej aplikacji pominąłem element przetwarzania - możemy wyobrazić sobie tutaj jakiś złożony i czasochłonny proces) i utrwalane w bazie danych. Odczyt danych z bazy wykonywany jest również przez aplikację backendową, za pośrednictwem sterownika bazy danych, a następnie dane te są przesyłane do aplikacji frontendowej w formacie JSON.Infrastruktura systemu źródłowego może być oparta o maszyny fizyczne, wirtualne, kontenery (Docker, Kubernetes, OpenShift) lub o dowolne inne rozwiązanie. Na potrzeby wpisu wykorzystałem Docker Compose, co ułatwia uruchomienie całej aplikacji lokalnie.Zakładając znajomość podstawowych usług oferowanych przez Google Cloud Platform, przejdźmy do kolejnego etapu migracji.PlanowaniePlanując architekturę systemu w chmurze, pierwszą czynnością, jaką należy wykonać, jest wybór strategii migracji. Od niej zależeć będzie to, jak będzie wyglądał system docelowy oraz jakie narzędzia i usługi zostaną wykorzystane.Wybierając strategię migracji, przyjąłem kilka założeń:  nie ma żadnych ograniczeń budżetowych oraz czasowych na wykonanie migracji;  system musi być gotowy obsłużyć każdy ruch użytkowników (automatyczne skalowanie);  utrzymanie infrastruktury powinno wymagać jak najmniejszego udziału człowieka (automatyzacja);  koszt utrzymania powinien być możliwie jak najniższy, szczególnie w przypadku braku ruchu użytkowników.Zanim podejmiemy ostateczną decyzją, zastanówmy się, jaka będzie architektura systemu docelowego, w zależności od wykorzystanej strategii. Zalety oraz wady każdej ze strategii opisałem już w poprzednim wpisie, a podane tutaj rozwiązania mają jedynie charakter przykładowy. To jak zaplanujemy architekturę docelową, zależy od wielu czynników, a strategie mają jedynie pomóc nam w podjęciu decyzji.Strategia “Lift and shift”Migracja strategią “Lift and shift” polega na wykorzystaniu maszyn wirtualnych (Compute Engine) na których uruchomione zostaną poszczególne elementy systemu. Wyjątek stanowi aplikacja frontendowa, której pliki statyczne będące wynikiem zbudowania aplikacji, umieszczone zostaną w buckecie (Cloud Storage).Plusy:Proces migracji w tym wypadku jest nieskomplikowany, wymaga od nas minimalnej znajomości platformy i zajmie stosunkowo niewiele czasu. W przypadku, gdy infrastruktura źródłowa oparta jest o VMware vSphere, chmurę AWS lub Azure, możemy zautomatyzować cały proces, wykorzystując do tego celu narzędzie Migrate for Compute Engine.Minusy:Nie wykorzystujemy praktycznie wcale potencjału chmury, odpowiedzialność za system stoi w większości po naszej stronie, a kolejka oraz baza danych nie jest skalowana automatycznie. Musimy również z góry określić typ maszyn Compute Engine oraz skonfigurować ich skalowanie (managed instance groups).Strategia “Improve and move”Migracja strategią “Improve and move” polega na zastąpieniu bazy danych oraz kolejki usługami SaaS. Są to elementy, które najłatwiej jest zastąpić, przy jak najmniejszym nakładzie pracy. Bazę danych zastępuje w tym przypadku odpowiadająca jej usługa Cloud Firestore, a kolejkę usługa Cloud Pub/Sub.Plusy:Eliminujemy problem braku automatycznego skalowania bazy danych i kolejki, przesuwając również odpowiedzialność za te elementy na usługodawcę.Minusy:Wymagana jest znajomość wykorzystywanych usług SaaS, a także ingerencja w kody źródłowe aplikacji (warstwa komunikacji z bazą/kolejką). Backend nadal oparty jest o maszyny wirtualne, ze wszelkimi tego konsekwencjami.Strategia “Rip and replace”Migracja strategią “Rip and replace” polega na całkowitym wykorzystaniu natywnych rozwiązań chmury. Oprócz usług SaaS, wykorzystujemy dodatkowo Cloud Functions, dzięki czemu eliminujemy minusy poprzednich dwóch strategii.Plusy:Optymalizujemy architekturę systemu, przesuwając granicę odpowiedzialności prawie całkowicie na stronę usługodawcy. Cloud Functions umożliwia wywoływanie funkcji po pojawieniu się wiadomości w usłudze Cloud Pub/Sub, dzięki czemu aplikacja frontendowa może zapisywać dane przyjęte od użytkownika bezpośrednio do kolejki, bez pośrednictwa aplikacji backendowej. Przepisując aplikację backendową na funkcję można zrezygnować również z jej pośrednictwa w odczycie danych z Cloud Firestore, ponieważ usługa ta udostępnia REST API, które umożliwia odczyt danych bezpośrednio przez aplikację frontendową. Dzięki automatycznemu skalowaniu (również do zera), optymalizujemy koszty utrzymania infrastruktury, które w przypadku zerowego ruchu użytkowników, można ograniczyć całkowicie do zera (w ramach GCP Free Tier).Minusy:Wykorzystanie natywnych rozwiązań wymaga od nas bardzo dobrej znajomości platformy, usług oraz komunikacji między nimi. Jak okaże się za chwilę, pojawiają się również nowe problemy, jak na przykład ograniczenia API usług, czy problem zimnego startu funkcji. Migracja z wykorzystaniem strategii “Rip and replace” jest również najbardziej czasochłonna, ponieważ wymaga całkowitego przepisania części systemu. Wykorzystując natywne rozwiązania platformy uzależniamy się również od usługodawcy (vendor lock-in).Architektura systemu docelowegoPonieważ złożoność systemu źródłowego jest niewielka i chciałbym w tym wpisie pokazać jak najwięcej, przeprowadzimy migrację z wykorzystaniem strategii “Rip and replace”. Architektura systemu docelowego będzie więc wyglądać następująco:  aplikacja frontendowa (pliki statyczne) zostanie umieszczona w Cloud Storage i udostępniona publicznie, wykorzystując HTTP/S Load Balancer. Alternatywą jest wykorzystanie platformy Firebase, w praktyce jednak, Firebase przechowuje pliki statyczne również w Cloud Storage, przykrywając wszystko własnym interfejsem graficznym;  aplikacja backendowa, z racji niskiej złożoności, zostanie przepisana całkowicie na Cloud Functions. Utracona zostanie walidacja danych wejściowych, ale dzięki temu użytkownik będzie mógł przesyłać dane do Cloud Pub/Sub bezpośrednio z aplikacji frontendowej. Dane te mogą być walidowane przez funkcję, która pobiera je z Cloud Pub/Sub, przetwarza i utrwala w Firestore. Gdyby walidacja była wymagana przed wrzuceniem danych do Cloud Pub/Sub, to pomiędzy aplikacją frontendową a Cloud Pub/Sub musiałaby znaleźć się dodatkowa funkcja, której zadaniem byłoby przyjęcie lub odrzucenie danych oraz przesłanie ich do Cloud Pub/Sub;  kolejka RabbitMQ zostanie zastąpiona całkowicie przez usługę Cloud Pub/Sub;  baza danych MongoDB zostanie zastąpiona całkowicie przez odpowiadającą jej usługę Cloud Firestore, która jest bazą nierelacyjną, przechowującą dane w formie dokumentów JSON.W systemie docelowym, jako że jest on w infrastrukturze chmury (czyli dostępny publicznie przez Internet), w celu zabezpieczenia przed nieupoważnionym dostępem, musimy zaimplementować mechanizm uwierzytelniania (chyba, że nie obawiamy się rachunków od Google). W tym celu wykorzystamy Google Sign-In. Aby to zrobić, musimy w APIs &amp;amp; Services stworzyć credential OAuth client ID i skonfigurować w Cloud IAM przykładowego użytkownika, który posiadać będzie uprawnienia do korzystania z aplikacji (może to być nasze główne konto, które posiada rolę właściciela).Dzięki wykorzystaniu Cloud Functions, zoptymalizujemy koszt utrzymania systemu. Funkcje generują koszt wyłącznie wtedy, kiedy działają - za liczbę wywołań oraz czas ich wykonywania. W przypadku bardzo niskiego ruchu (łapiącego się w darmowe limity wywołań funkcji), lub jego braku, opłaty mogą zostać zminimalizowane do zera. Funkcje skalują się automatycznie, dlatego cała odpowiedzialność za obsłużenie ruchu użytkowników i zapewnienie dostępności systemu leży po stronie usługodawcy.Usługa Cloud Pub/Sub działa na zasadzie kolejki, więc idealnie zastąpi RabbitMQ. Zazwyczaj Cloud Pub/Sub dostarcza każdą wiadomość jeden raz oraz w takiej kolejności, w jakiej została opublikowana. Google w dokumentacji usługi informuje, że mogą zdarzyć się sytuacje, w których wiadomość zostanie dostarczona poza kolejnością lub więcej niż jeden raz. Wiedząc o tym, należy zaimplementować odbiorcę wiadomości w sposób idempotentny, czyli odporny na wielokrotne dostarczenie tej samej wiadomości. W przypadku aplikacji demo nie stanowi to problemu, ponieważ każda wiadomość posiadać będzie unikalny identyfikator, a wielokrotne zapisanie tej samej wiadomości do Cloud Firestore spowoduje jej nadpisanie. W systemie źródłowym, wykorzystanie RabbitMQ również wymagało idempotentnego odbiorcy wiadomości.Pojawienie się wiadomości w Cloud Pub/Sub triggerować będzie wywołanie funkcji odpowiedzialnej za przetworzenie wiadomości i utrwalenie jej w Cloud Firestore. Zrezygnujemy z funkcji pomiędzy aplikacją frontendową a Cloud Pub/Sub, ponieważ zakładamy, że nie ma potrzeby walidacji danych przed wysłaniem ich do Cloud Pub/Sub. W aplikacji źródłowej taka walidacja była wykonywana przez aplikację backendową, zanim wiadomość została wysłana do kolejki RabbitMQ. Dzięki temu, podczas migracji zoptymalizujemy system, upraszczając jego architekturę. Oczywiście taka optymalizacja byłaby możliwa również w systemie źródłowym, ponieważ RabbitMQ umożliwia dostarczanie wiadomości poprzez REST API, wykorzystując w tym celu dodatkową wtyczkę.Odpowiednikiem bazy MongoDB jest usługa Cloud Firestore. Umożliwia ona zapis i odczyt za pomocą REST API, dzięki czemu odczyt może być wykonywany w samej aplikacji frontendowej, bez udziału aplikacji backendowej (nie jest potrzebny sterownik do bazy danych). To upraszcza jeszcze bardziej architekturę systemu docelowego.WdrażanieWszystkie elementy zostaną wdrożone ręcznie, za pomocą Cloud CLI (które umożliwia wykonywanie poleceń GCP w lokalnym terminalu) oraz Cloud Console.Aplikacja frontendowaMigracja aplikacji frontendowej wymaga drobnych zmian. Po pierwsze, musimy zaimplementować uwierzytelnianie, wykorzystujące Google Sign-In. W tym celu, w pliku environment.ts musimy skonfigurować Client ID, który wygenerowaliśmy wcześniej. Po drugie, główne zmiany zajdą w serwisie odpowiedzialnym za zapis oraz odczyt danych (w systemie źródłowym, wszystko odbywa się przez aplikację backendową). Przykładowy zapis oraz odczyt w aplikacji frontendowej (w systemie źródłowym), wygląda następująco:upsertData(data: Data): Observable&amp;lt;Object&amp;gt; {    console.log(`Sending ${JSON.stringify(data)}`);    return this.http.post(`${environment.apiUrl}/api`, data);}findByUuid(uuid: string): Observable&amp;lt;Object&amp;gt; {    console.log(`Find by uuid: ${uuid}`);    return this.http.get(`${environment.apiUrl}/api/${uuid}`);}Funkcja upsertdata przesyła dane do aplikacji backendowej, za pomocą żądania HTTP POST. Odczyt danych za pośrednictwem aplikacji backendowej wykonywany jest przez funkcję findByUuid, za pomocą żądania HTTP GET.W systemie docelowym, zapis wykonywany jest bezpośrednio z aplikacji frontendowej do usługi Cloud Pub/Sub, a odczyt bezpośrednio z Cloud Firestore:upsertData(data: Data): Observable&amp;lt;Object&amp;gt; {    console.log(`Sending ${JSON.stringify(data)}`);    let dataToSend = Object.assign({}, data);    dataToSend.uuid = uuid.v4();    return this.http.post(environment.pubSubUrl, {      &quot;messages&quot;: [        {          &quot;data&quot;: btoa(JSON.stringify(dataToSend))        }      ]    }).pipe(map(response =&amp;gt; ({      messageId: response[&#39;messageIds&#39;][0],      uuid: dataToSend.uuid    })));  }findByUuid(documentUuid: string): Observable&amp;lt;Data&amp;gt; {    console.log(`Find by uuid: ${documentUuid}`);    return this.http.get&amp;lt;any&amp;gt;(`${environment.firestoreUrl}/${documentUuid}`)        .pipe(map(object =&amp;gt;            ({                uuid: object.fields.uuid.stringValue,                manufacturer: object.fields.manufacturer.stringValue,                model: object.fields.model.stringValue            })));}Funkcja upsertData przygotowuje obiekt do wysłania do usługi Cloud Pub/Sub, dodając do niego unikalny identyfikator UUID i kodując wiadomość do formatu Base64. Odpowiedź z usługi jest mapowana na odpowiedni model.Funkcja findByUuid pobiera dane z usługi Cloud Firestore i mapuje je na odpowiedni model.Główna różnica w zapisie, to nadanie unikalnego identyfikatora UUID, który wcześniej był nadawany przez aplikację backendową, przygotowanie wiadomości Cloud Pub/Sub oraz przemapowanie odpowiedzi.Odczyt różni się natomiast tym, że z odpowiedzi usługi Cloud Firestore, wyciągane są jedynie potrzebne dane (oryginalna odpowiedź zawiera wiele dodatkowych informacji, takich jak znaczniki czasowe zapisu/aktualizacji itp., które nie są prezentowane użytkownikowi).Cała logika odpowiedzialna za przemapowanie żądania/odpowiedzi, która do tej pory była zaimplementowana w aplikacji backendowej, musi zostać zaimplementowana po stronie aplikacji frontendowej. Dzięki temu architektura znacząco się upraszcza.Aby uruchomić aplikację w chmurze, potrzebny jest bucket w Cloud Storage oraz usługa HTTP(S) Load Balancer. Po zbudowaniu aplikacji frontendowej, wygenerowane pliki statyczne (JS oraz CSS) umieszczamy w buckecie.Musimy pamiętać jeszcze o skonfigurowaniu go w taki sposób, aby pliki były dostępne publicznie. W tym celu, należy dodać w konfiguracji bucketu użytkownika o nazwie allUsers oraz nadać mu uprawnienie Storage Object Viewer.Po umieszczeniu plików w Cloud Storage, będą one dostępne publicznie, np. plik index.html będzie dostępny pod adresem:https://storage.googleapis.com/BUCKET_NAME/index.htmlAplikacja nie będzie jednak działać, gdyż do jej uruchomienia potrzebna będzie jeszcze usługa Cloud Load Balancing. Bez niej, po wejściu na podany wyżej adres, zostanie wyświetlona jedynie zawartość pliku index.html, a zawarte w nim skrypty JavaScript nie zostaną w żaden sposób zinterpretowane przez przeglądarkę, w efekcie czego wyświetli się jedynie pusta strona.Podczas konfiguracji usługi Cloud Load Balancing, jako backend musimy skonfigurować bucket zawierający pliki statyczne oraz musimy nadać statyczny adres IP. Dodatkowo możemy wykorzystać usługę Cloud CDN. Więcej na ten temat znajdziemy w dokumentacji: Setting up a load balancer with backend buckets.Dzięki temu, aplikacja będzie działać poprawnie pod adresem IP:http://LOAD_BALANCER_IP/index.htmlDo pełni szczęścia potrzebna jest jeszcze domena, która musi zostać skonfigurowana w usłudze Cloud Load Balancing. Google Sign-In, które zostało wykorzystane do uwierzytelniania, wymaga skonfigurowania adresu Origin dla wygenerowanego Client ID, czyli adresu, pod którym będzie dostępna aplikacja internetowa. Nie może to być adres IP, ponieważ mimo możliwości wpisania w tym miejscu adresu IP, podczas próby zalogowania się otrzymamy błąd redirect_uri_mismatch:The JavaScript origin in the request, does not match the ones authorized for the OAuth client,czyli taki sam, jak w przypadku braku adresu Origin.Po skonfigurowaniu domeny, możemy skonfigurować odpowiednio Client ID:KolejkaW konsoli usługi Cloud Pub/Sub musimy skonfigurować nowy topic, którego identyfikator zostanie przekazany do aplikacji frontendowej, ponieważ adres na który są wysyłane wiadomości (environment.pubSubUrl) musi wskazywać, na jaki topic wiadomość jest wysyłana (a także musi zawierać id projektu):https://pubsub.googleapis.com/v1/projects/PROJECT_ID/topics/TOPIC_NAME:publishKonfiguracja kolejki w zasadzie na tym się kończy, ponieważ usługa Cloud Pub/Sub nie wymaga żadnych dodatkowych konfiguracji.Aplikacja backendowaAplikacja backendowa, która odpowiedzialna jest za komunikację aplikacji frontendowej z kolejką RabbitMQ oraz bazą danych MongoDB, zostanie zastąpiona krótką funkcją.Zapis danych do usługi Cloud Pub/Sub oraz odczyt z Cloud Firestore odbywa się bezpośrednio przez REST API, dlatego jedyną odpowiedzialnością funkcji jest pobieranie danych pojawiających się w Cloud Pub/Sub, przetwarzanie ich (jak już wspomniałem, pominąłem przetwarzanie) oraz utrwalenie w Cloud Firestore.Funkcję napiszemy w jednym z dostępnych w Cloud Functions języków, a dokładnie w JavaScript, który wykorzystuje środowisko uruchomieniowe Node.js (w wersji 10).Aplikacja źródłowa składa się z kilku klas Javy, z których najważniejszą jest klasa DataService:public class DataService {    private final RabbitService rabbitService;    private final MongoService mongoService;    Data saveInQueue(Data data) {        log.info(&quot;&amp;gt; saveInQueue [data={}]&quot;, data);        data.setId(UUID.randomUUID().toString());        data.setTimestamp(LocalDateTime.now());        rabbitService.send(data);        log.info(&quot;&amp;lt; saveInQueue [data={}]&quot;, data);        return data;    }    Data findByUuid(String uuid) {        log.info(&quot;&amp;gt; findByUuid [uuid={}]&quot;, uuid);        Data data = mongoService.findByUuid(uuid);        log.info(&quot;&amp;lt; findByUuid [data={}]&quot;, data);        return data;    }    List&amp;lt;Data&amp;gt; findAll() {        log.info(&quot;&amp;gt; findAll&quot;);        List&amp;lt;Data&amp;gt; data = mongoService.findAll();        log.info(&quot;&amp;lt; findAll [data={}]&quot;, data);        return data;    }    void deleteAll() {        log.info(&quot;&amp;gt; deleteAll&quot;);        mongoService.deleteAll();        log.info(&quot;&amp;lt; deleteAll&quot;);    }}Składa się ona z metody saveInQueue, która zapisuje dane przesłane przez aplikację frontendową do kolejki RabbitMQ, za pomocą serwisu RabbitService oraz metod odczytujących dane z bazy MongoDB (metody findByUuid oraz findAll) za pomocą serwisu MongoService. Oprócz tego, zawiera ona metodę deleteAll, która usuwa wszystkie dane zapisane w bazie.Implementacja serwisu RabbitService wygląda następująco:public class RabbitService {    private final RabbitTemplate rabbitTemplate;    private final MongoService mongoService;    @RabbitListener(queues = RabbitConfig.QUEUE_NAME)    void listen(Data data) {        log.info(&quot;Received data from queue [queue={}, data={}]&quot;, RabbitConfig.QUEUE_NAME, data);        mongoService.saveInDatabase(data);    }    void send(Data data) {        log.info(&quot;Send data to queue [queue={}, data={}]&quot;, RabbitConfig.QUEUE_NAME, data);        rabbitTemplate.convertAndSend(RabbitConfig.QUEUE_NAME, data);    }}Składa się ona z metody send, z której korzysta serwis DataService. Zapisuje ona dane w kolejce RabbitMQ za pomocą klasy RabbitTemplate, która jest częścią biblioteki spring-boot-starter-amqp. Druga metoda (listen), odpowiada za pobieranie wiadomości pojawiających się w kolejce i zapisywanie ich w bazie danych za pomocą serwisu MongoService.Serwis MongoService wygląda następująco:public class MongoService {    private final MongoOperations mongoOperations;    void saveInDatabase(Data data) {        try {            mongoOperations.insert(data);            log.info(&quot;Saved data in database&quot;);        } catch (DuplicateKeyException ignore) {            log.warn(&quot;Duplicated data from queue ignored&quot;);        } catch (Exception e) {            log.error(&quot;Error saving data in database&quot;, e);            throw e;        }    }    Data findByUuid(String uuid) {        return mongoOperations.findById(uuid, Data.class);    }    List&amp;lt;Data&amp;gt; findAll() {        return mongoOperations.findAll(Data.class);    }    void deleteAll() {        mongoOperations.dropCollection(Data.class);    }}Wykorzystuje on klasę MongoOperations do komunikacji z bazą danych MongoDB, za pomocą odpowiedniego sterownika. Klasa ta jest częścią biblioteki spring-boot-starter-data-mongodb. Implementacja serwisu składa się z 4 metod, odpowiedzialnych za zapis danych w bazie (saveInDatabase), odczyt (findByUuid oraz findAll) oraz usunięcie wszystkich danych (deleteAll).W celu komunikacji aplikacji backendowej z aplikacją frontendową za pomocą REST API, zaimplementowany został kontroler DataController, wystawiający metody serwisu DataService jako endpointy REST API. Jego implementacja wygląda następująco:@RestController@RequestMapping(&quot;/api&quot;)@RequiredArgsConstructorpublic class DataController {    private final DataService dataService;    @PostMapping    public ResponseEntity&amp;lt;Data&amp;gt; save(@RequestBody @Valid Data data) {        log.debug(&quot;&amp;gt; save [data={}]&quot;, data);        Data savedData = dataService.saveInQueue(data);        log.debug(&quot;&amp;lt; save [savedData={}]&quot;, savedData);        return ResponseEntity.status(HttpStatus.ACCEPTED).body(savedData);    }    @GetMapping(&quot;/{uuid}&quot;)    public ResponseEntity&amp;lt;Data&amp;gt; getById(@PathVariable String uuid) {        log.debug(&quot;&amp;gt; getById [uuid={}]&quot;, uuid);        Data dataFromDb = dataService.findByUuid(uuid);        log.debug(&quot;&amp;lt; getById [dataFromDb={}]&quot;, dataFromDb);        if (dataFromDb == null) {            return ResponseEntity.notFound().build();        }        return ResponseEntity.ok(dataFromDb);    }    @GetMapping    public ResponseEntity&amp;lt;List&amp;lt;Data&amp;gt;&amp;gt; getAll() {        log.debug(&quot;&amp;gt; getAll&quot;);        List&amp;lt;Data&amp;gt; dataFromDb = dataService.findAll();        log.debug(&quot;&amp;lt; getAll [resultSize={}]&quot;, dataFromDb.size());        if (dataFromDb.isEmpty()) {            return ResponseEntity.notFound().build();        }        return ResponseEntity.ok(dataFromDb);    }    @DeleteMapping    @ResponseStatus(code = HttpStatus.ACCEPTED)    public void deleteAll() {        log.debug(&quot;&amp;gt; deleteAll&quot;);        dataService.deleteAll();        log.debug(&quot;&amp;lt; deleteAll&quot;);    }    @ResponseStatus(HttpStatus.BAD_REQUEST)    @ExceptionHandler(MethodArgumentNotValidException.class)    public Map&amp;lt;String, String&amp;gt; handleValidationExceptions(            MethodArgumentNotValidException ex) {        Map&amp;lt;String, String&amp;gt; errors = new HashMap&amp;lt;&amp;gt;();        ex.getBindingResult().getAllErrors().forEach(error -&amp;gt; {            String fieldName = ((FieldError) error).getField();            String errorMessage = error.getDefaultMessage();            errors.put(fieldName, errorMessage);        });        return errors;    }}Oprócz wystawienia metod serwisu DataService jako endpointy REST API, kontroler obsługuje również błędy (np. wynikające z niepoprawnego wywołania lub niepoprawnego modelu przekazywanych danych do zapisu) za pomocą metody handleValidationExceptions.Oprócz wyżej wymienionych klas, aplikacja backendowa składa się dodatkowo z konfiguracji kolejki RabbitMQ (klasa RabbitConfig - 24 linie kodu), konfiguracji CORS (klasa WebConfig - 19 linii kodu), modelu danych (klasa Data - 26 linii kodu) oraz głównej klasy BackendApplication, odpowiedzialnej za uruchomienie aplikacji (12 linii kodu). Cała aplikacja składa się z 8 klas Javy oraz pliku konfiguracyjnego application.yml.Implementacja funkcji, zastępująca aplikację backendową wygląda następująco:const Firestore = require(&#39;@google-cloud/firestore&#39;);const firestore = new Firestore({    projectId: process.env.GCP_PROJECT});exports.main = (event, context) =&amp;gt; {    const message = JSON.parse(Buffer.from(event.data, &#39;base64&#39;).toString());    console.log(`Save Pub/Sub message in db [data=${JSON.stringify(message)}, context=${JSON.stringify(context)}]`);    const document = firestore.collection(process.env.COLLECTION_NAME).doc(message.uuid);    document.set({        uuid: message.uuid,        manufacturer: message.manufacturer,        model: message.model    }).then(doc =&amp;gt; {        console.log(`Data saved [doc=${JSON.stringify(doc)}]`);    }).catch(err =&amp;gt; {        console.error(err);        throw new Error(`Error saving data...`);    });};Składa się ona z jednej funkcji main, której zadaniem jest zdekodowanie wiadomości z usługi Cloud Pub/Sub z formatu Base64 oraz zapisanie danych w usłudze Cloud Firestore.Funkcja wykorzystuje do komunikacji z usługą Cloud Firestore bibliotekę @google-cloud/firestore, a samo wywołanie funkcji wyzwalane jest przez usługę Cloud Pub/Sub, w momencie pojawienia się wiadomości w odpowiednim topicu.Ilość kodu została drastycznie zmniejszona. Aplikacja backendowa została całkowicie zastąpiona przez funkcję, składającą się z 21 linii kodu JavaScript. Dzięki migracji z wykorzystaniem strategii “Rip and replace”, udało się znacząco uprościć architekturę i zredukować tzw. boilerplate code, który często występuje w aplikacjach napisanych w języku Java.Funkcję możemy wdrożyć z poziomu GUI (Cloud Console), lub za pomocą polecenia:gcloud functions deploy demo-function \\    --entry-point main \\    --runtime nodejs10 \\    --trigger-topic demo-topic \\    --region europe-west3 \\    --timeout 10 \\    --memory 128MB \\    --max-instances 1 \\    --set-env-vars COLLECTION_NAME=demo-collection \\    --project PROJECT_IDNiezależnie od sposobu wdrażania, musimy odpowiednio skonfigurować funkcję:  entry-point - nazwa metody (w języku JavaScript zwanej funkcją), która jest wywoływana przy triggerowaniu funkcji;  runtime - środowisko uruchomieniowe;  trigger-topic - topic w Cloud Pub/Sub, w którym pojawienie się wiadomości triggeruje wywołanie funkcji;  region - region, w który funkcja fizycznie będzie uruchamiana;  timeout - maksymalny czas wykonywania funkcji, po przekroczeniu którego funkcja jest automatycznie zakańczana błędem;  memory - rozmiar pamięci, jaka będzie przydzielona funkcji w trakcie wywołania;  max-instances - maksymalna liczba instancji, które mogą zostać utworzone (automatyczne skalowanie);  set-env-vars - zmienne środowiskowe, które będą wykorzystane w funkcji, w naszym wypadku jest to nazwa kolejki w Cloud Firestore, do której będą zapisywane dane;  project - id projektu.Dodatkowo moglibyśmy skonfigurować ponawianie przetwarzania wiadomości, w przypadku błędu wywołania funkcji, ale nie będziemy tego robić.Baza danychKonfiguracja Cloud Firestore jest jeszcze łatwiejsza, niż konfiguracja Cloud Pub/Sub, ponieważ nie wymaga absolutnie żadnych czynności do jej uruchomienia.Dane zapisywane i odczytywane są poprzez REST API, wskazując w adresie url identyfikator projektu, domyślną bazę danych oraz strukturę w której dane mają być utrwalone. Zapis danych nie wymaga istnienia struktury, ponieważ zostanie ona utworzona przy pierwszym zapisie.Adres url do zapisu oraz odczytu (environment.firestoreUrl) danych do kolekcji o nazwie demo-collection wygląda następująco:https://firestore.googleapis.com/v1/projects/PROJECT_ID/databases/(default)/documents/demo-collectionZamiana bazy danych MongoDB, z którą komunikacja odbywała się poprzez sterownik bazodanowy w aplikacji backendowej, na usługę Cloud Firestore, niesie ze sobą pewne ograniczenia. Wynikają one z interfejsu REST API, który udostępnia usługa, przez co pewne operacje mogą być niemożliwe.W przypadku bazy MongoDB, można było wykonać dowolne zapytanie, na przykład usuwające wszystkie dane z bazy (w przykładzie wykorzystałem wysokopoziomowy interfejs MongoOperations, ale można również wywołać dowolne polecenie na bazie danych):void deleteAll() {    mongoOperations.dropCollection(Data.class);}Według dokumentacji REST API usługi Cloud Firestore, nie ma możliwości usunięcia wszystkich danych jednym żądaniem HTTP. Aby usunąć wszystkie dane z poziomu aplikacji, należałoby najpierw pobrać ich identyfikatory, a następnie usuwać je pojedynczo lub w paczkach, co byłoby mało wydajne. Zakładamy jednak, że usuwanie wszystkich danych przez aplikację, jest raczej mało realnym przypadkiem użycia, dlatego nie będziemy implementować takiej operacji. Chciałem jedynie na jej przykładzie przedstawić problemy, na jakie możemy trafić w przypadku usług SaaS i ich interfejsów.Gdybyśmy chcieli jednak zaimplementować taką operację, moglibyśmy na przykład utworzyć dodatkową funkcję, która przyjmowałaby polecenie z aplikacji frontendowej, a następnie pobierała dokumenty w paczkach z Cloud Firestore i usuwała je. Taka operacja powinna być wykonywana asynchronicznie i w sposób umożliwiający ponawianie, ponieważ w przypadku dużej ilości danych, proces ten może być czasochłonny.UprawnieniaAby umożliwić komunikację z usługami (zapis danych do usługi Cloud Pub/Sub oraz odczyt z usługi Cloud Firestore), należy skonfigurować uprawnienie dla użytkownika w usłudze Cloud IAM. Komunikacja z usługami odbywa się bezpośrednio z aplikacji frontendowej, dlatego do uwierzytelnienia wykorzystywane jest konto zalogowanego użytkownika. W przypadku, kiedy operacje te byłyby wykonywane przez inną usługę GCP, np. Compute Engine, to musielibyśmy utworzyć odpowiednie konto serwisowe (service account) i nadać mu odpowiednie uprawnienia. Konta serwisowe są wykorzystywane przez usługi GCP, w celu uwierzytelniania przed innymi usługami.W celach demonstracyjnych, możemy wykorzystać nasze konto, które posiada uprawnienie właściciela projektu. Gdybyśmy chcieli umożliwić korzystanie z systemu innym użytkownikom, powinniśmy utworzyć odpowiednią rolę, posiadającą wymagane uprawnienia oraz nadać tę rolę każdemu użytkownikowi. Jest wiele sposobów na zarządzanie użytkownikami w Google Cloud. Można zaimportować ich z pliku CSV, utworzyć ręcznie, wykorzystać rozwiązania zewnętrzne takie jak Okta czy Ping lub skorzystać z zalecanego rozwiązania, czyli narzędzia Google Cloud Directory Sync.OptymalizacjaEtap optymalizacji polega na monitorowaniu aplikacji w poszukiwaniu błędów, optymalizacji wydajności oraz wykonywaniu zmian, na które nie było czasu lub budżetu przed wdrożeniem.Poprawa błędówMonitorowanie logów aplikacji w usłudze Cloud Monitoring pozwoliło na wykrycie błędu w kodzie funkcji.Podczas jednego z zapisów, w logach zabrakło informacji o pomyślnym zapisaniu danych w Cloud Firestore, nie było również żadnego błędu. W efekcie, po wykonaniu trzech zapisów, utrwalone zostały jedynie dane z dwóch ostatnich:2020-08-12T18:29:51 g8xx0fyvsqhl Function execution started2020-08-12T18:29:51 g8xx0fyvsqhl Save Pub/Sub message in db2020-08-12T18:29:51 g8xx0fyvsqhl Function execution took 217 ms, finished with status: &#39;ok&#39;2020-08-12T18:50:15 2kcnvesdjzpp Function execution started2020-08-12T18:50:16 2kcnvesdjzpp Save Pub/Sub message in db2020-08-12T18:50:16 2kcnvesdjzpp Function execution took 413 ms, finished with status: &#39;ok&#39;2020-08-12T18:51:18 2kcnvesdjzpp Data saved2020-08-12T18:52:59 2kcnvlflowql Function execution started2020-08-12T18:52:59 2kcnvlflowql Save Pub/Sub message in db2020-08-12T18:52:59 2kcnvlflowql Function execution took 19 ms, finished with status: &#39;ok&#39;2020-08-12T18:53:00 2kcnvlflowql Data savedJak widać w powyższym logu, w pierwszym z trzech wywołań funkcji, brakuje informacji Data saved. Fakt, że informacja ta jest w pozostałych przypadkach logowana po informacji o zakończeniu wywołania funkcji sugeruje, że zapis do Cloud Firestore jest wykonywany asynchronicznie, czyli może trwać dłużej niż czas wykonywania samej funkcji. Przeglądając kod funkcji, szybko udaje się znaleźć błąd, w postaci brakującego słowa kluczowego return, dzięki któremu funkcja będzie czekała na zakończenie się wywołania asynchronicznej metody document.set:exports.main = (event, context) =&amp;gt; {    const message = JSON.parse(Buffer.from(event.data, &#39;base64&#39;).toString());    console.log(`Save Pub/Sub message in db [data=${JSON.stringify(message)}, context=${JSON.stringify(context)}]`);    const document = firestore.collection(process.env.COLLECTION_NAME).doc(message.uuid);    return document.set({        uuid: message.uuid,        manufacturer: message.manufacturer,        model: message.model    }).then(doc =&amp;gt; {        console.log(`Data saved [doc=${JSON.stringify(doc)}]`);    }).catch(err =&amp;gt; {        console.error(err);        throw new Error(`Error saving data...`);    });};UsprawnieniaNa etapie optymalizacji można również wykonywać zmiany, które są wynikiem dalszego zdobywania wiedzy po wdrożeniu lub pojawiania się nowych możliwości w Google Cloud. Jednym z nich jest na przykład możliwość utworzenia konfiguracji strony www w Cloud Storage.W dokumentacji usługi Cloud Storage znajduje się informacja o tym, że w przypadku nadania bucketowi nazwy, będącej poprawnym adresem URL, pojawi się dodatkowa opcja konfiguracji dla witryny internetowej. Dzięki temu, można skonfigurować plik, który ma być udostępniany użytkownikowi po wejściu na adres www.Aby nadać bucketowi nazwę domeny, należy wcześniej wykonać weryfikację własności domeny (pod tym adresem). Konfigurując w ten sposób bucket, nie potrzebujemy HTTP/S Load Balancera do działania aplikacji. Nie jest to jednak dobre rozwiązanie w przypadku środowiska produkcyjnego, ponieważ bez usługi Cloud Load Balancing nie ma możliwości skonfigurowania certyfikatu HTTPS, a wykorzystywanie nieszyfrowanego połączenia pomiędzy użytkownikiem a aplikacją internetową, jest złą praktyką.Dodatkowo, w konfiguracji domeny, zamiast podawać adres IP w rekordzie typu A, należy podać adres c.storage.googleapis.com, w rekordzie typu CNAME.Więcej informacji znajdziemy tutaj: Hosting a static website, Static website examples and tips.AutomatyzacjaInnym elementem etapu optymalizacji mogą być na przykład usprawnienia w procesie wdrażania aplikacji. Stwórzmy zatem proste CI/CD, wykorzystując do tego celu repozytorium GitHub, usługę Cloud Build oraz infrastrukturę jako kod (IaC), opartą o Terraform. Jako alternatywę dla Terraform moglibyśmy wykorzystać Cloud Deployment Manager, ale wtedy uzależnilibyśmy się od chmury Google (vendor lock).Przed implementacją, musimy jeszcze:  utworzyć bucket w usłudze Cloud Storage, który przechowywać będzie stan infrastruktury, wykorzystywany przez Terraform:  połączyć repozytorium GitHub, zawierające kody aplikacji, z usługą Cloud Build. W celu integracji, w repozytorium GitHub należy zainstalować aplikację Google Cloud Build, a następnie połączyć repozytorium z usługą Cloud Build:  w Cloud IAM przydzielić uprawnienia do konta serwisowego, z którego korzysta Cloud Build: Cloud Build Service Account, Cloud Functions Admin, Cloud Functions Developer, Service Account User, Pub/Sub Editor, Storage Admin:  nadać uprawnienie do zweryfikowanej domeny dla konta serwisowego Cloud Build (pod tym adresem):Konfiguracja automatyzująca tworzenie infrastruktury oraz wdrażanie aplikacji składa się z dwóch części:  konfiguracji Terraform, definiującej infrastrukturę, np. trigger w Cloud Build, który na podstawie zmian kodu w repozytorium uruchamia zadanie;  konfiguracji Cloud Build, czyli definicji zadań do wykonania, które są uruchamiane przez triggery utworzone przez Terraform.Konfiguracja TerraformKonfiguracja Terraform jest umieszczona w folderze infrastructure i składa się z:  konfiguracji tzw. backendu, który w Terraform jest abstrakcją, określającą m.in. w jaki sposób ładowany jest stan infrastruktury (infrastructure/backend.tf):terraform {  backend &quot;gcs&quot; {    bucket = &quot;demo-infrastructure&quot;    prefix = &quot;terraform/state&quot;  }}W drugiej linii definiujemy typ backendu, w tym przypadku gcs, który przechowuje stan w usłudze Cloud Storage. Następnie definiujemy nazwę utworzonego wcześniej bucketu, w którym będzie przechowywany stan, oraz prefix, będący ścieżką folderów, w których stan będzie się znajdował,  konfiguracji providera (infrastructure/main.tf):provider &quot;google-beta&quot; {  project = var.gcp-project  region = var.gcp-region  zone = var.gcp-zone}Terraform udostępnia dwóch dostawców dla Google Cloud: google, google-beta. W pierwszej linii definiujemy dostawcę google-beta, który umożliwia wykorzystywanie API, które w GCP jest w wersji beta. W kolejnych liniach definiujemy projekt, region oraz strefę, które w tym przypadku są pobierane ze zmiennych,      definicji zmiennych, wykorzystywanych przez konfigurację Terraform (infrastructure/variables.tf i infrastructure/terraform.tfvars) (niektóre ze zmiennych są również przekazywane przez Terraform do Cloud Build);        konfiguracji triggera Cloud Build, który na podstawie zmian kodów konfiguracji infrastruktury w repozytorium (folder infrastructure), uruchamia zdefiniowane zadanie w Cloud Build:  resource &quot;google_cloudbuild_trigger&quot; &quot;deploy-demo-infrastructure&quot; {  provider = google-beta  name = &quot;deploy-demo-infrastructure&quot;  description = &quot;[demo] Deploy main: infrastructure&quot;  filename = &quot;infrastructure/build/deploy.cloudbuild.yaml&quot;  included_files = [    &quot;infrastructure/**&quot;  ]  github {    owner = var.github-owner    name = var.github-repository    push {      branch = &quot;^main$&quot;    }  }}W pierwszej linii określamy rodzaj tworzonego zasobu (google_cloudbuild_trigger - trigger w usłudze Cloud Build). W kolejnych liniach definiujemy providera, nazwę triggera, jego opis, ścieżkę pod którą znajduje się definicja zadań do wykonania przez Cloud Build, ścieżkę w której zmiana plików spowoduje uruchomienie zadania oraz konfigurację repozytorium GitHub. W konfiguracji repozytorium definiujemy jego położenie (parametr owner i name) oraz sposób triggerowania zadania Cloud Build, w tym przypadku jest to wypushowanie zmian kodów do brancha main (dostępne są m.in. opcje triggerowania na push taga lub utworzenie pull requesta w aplikacji GitHub),  konfiguracji tworzącej buckety w Cloud Storage (infrastructure/frontend-bucket.tf i infrastructure/yarn-cache.storage.tf), które wykorzystywane są m.in. do przechowywania plików statycznych aplikacji frontendowej oraz kopii podręcznej zależności, wykorzystywanych przez aplikację frontendową i funkcję (w dalszej części wyjaśnię o co chodzi);  konfiguracji topicu w usłudze Cloud Pub/Sub (infrastructure/pubsub.tf);  konfiguracji triggerów w usłudze Cloud Build, które na podstawie zmian w repozytorium GitHub, uruchamiają zdefiniowane zadania (o tym też za chwilę).Konfiguracja Cloud BuildDrugą z konfiguracji, jest konfiguracja usługi Cloud Build, czyli definicja wykonania zadań, potrzebnych do zbudowania i wdrożenia poszczególnych elementów systemu. Definicja wdrażania dla każdego elementu systemu znajduje się w folderze zawierającym jego kody źródłowe. Składa się ona z:  konfiguracji wdrażania infrastruktury (infrastructure/build/deploy.cloudbuild.yml):steps:  - name: &#39;hashicorp/terraform:0.12.26&#39;    entrypoint: &#39;sh&#39;    args:      - &#39;-c&#39;      - |        terraform init &amp;amp;&amp;amp; terraform apply -auto-approve    dir: &#39;infrastructure&#39;Składa się ona z jednego kroku, który za pomocą powłoki systemu Linux (sh), wykonuje w folderze infrastructure następujące polecenia: terraform init (inicjalizujące Terraform), terraform apply -auto-approve (tworzące infrastrukturę za pomocą konfiguracji Terraform),  konfiguracji budowania i wdrażania aplikacji frontendowej (frontend/deploy.cloudbuild.yml);  konfiguracji wdrażania funkcji (function/deploy.cloudbuild.yml).Dependency cacheKopia podręczna zależności, wykorzystywanych przez aplikację frontendową oraz funkcję, przechowywana jest w przeznaczonym do tego celu buckecie.Aplikacje JavaScript (a więc te oparte o framework Angular, jak i funkcje napisane w czystym JavaScript) wykorzystują zewnętrzne zależności (biblioteki, moduły), które przechowują w folderze node_modules. Za każdym razem, kiedy aplikacja lub funkcja są budowane, muszą one pobrać wszystkie zależności z których korzystają. Bardzo często, zależności liczone są w dziesiątkach, a nawet setkach, a pobranie wszystkich za każdym razem, powoduje niepotrzebne wykorzystywanie łącza internetowego oraz dłuższy czas wykonywania zadania w usłudze Cloud Build (która posiada darmowy limit w postaci 120 minut, za które w ciągu dnia nie trzeba dodatkowo płacić).Chcąc zoptymalizować czas budowania i wdrażania, a także koszty z tym związane, utworzone zostały skrypty (w folderze yarn-cache-builder), które przed pobraniem zależności z Internetu, sprawdzają najpierw, czy nie istnieje już paczka (plik .zip), zawierająca te zależności w Cloud Storage. Jeśli tak, to zależności te pobierane są z bucketu i wypakowywane, co zajmuje o wiele mniej czasu, niż pobieranie ich ponownie.Kopia podręczna tworzona jest na podstawie sumy kontrolnej MD5 pliku yarn.lock, który zawiera listę wymaganych zależności. Jeśli zależności te się nie zmieniły (np. nie dodano nowych lub nie zmieniono wersji którejś z nich), to można wykorzystać zależności przechowywane w buckecie. Jeśli jednak coś się zmieniło, to zostaną one pobrane z Internetu, a następnie spakowane do archiwum .zip, zawierającego w nazwie sumę kontrolną i umieszczone w buckecie.Konfiguracja Terraform (infrastructure/yarn-cache.storage.tf) tworząca bucket, przeznaczony na kopie podręczne zależności, wygląda następująco:resource &quot;google_storage_bucket&quot; &quot;demo-yarn-cache&quot; {  provider = google-beta  name = var.cache-bucket-name  location = var.gcp-region  lifecycle_rule {    condition {      age = &quot;7&quot;    }    action {      type = &quot;Delete&quot;    }  }}W pierwszej linii określamy rodzaj tworzonego zasobu (google_storage_bucket - bucket w usłudze Cloud Storage). W kolejnych liniach definiujemy providera, nazwę tworzonego bucketu, region w którym będzie utworzony oraz regułę cyklu życia plików. Reguła definiuje akcję usunięcia plików starszych niż 7 dni.Skrypt tworzący kopię podręczną zależności (yarn-cache-builder/yarn-cache-push.sh) wygląda następująco:lockChecksum=$(md5sum yarn.lock | cut -d&#39; &#39; -f1)cacheArchive=$(basename &quot;$PWD&quot;).node_modules.${lockChecksum}.tar.gzif gsutil -q stat ${1}/&quot;$cacheArchive&quot;; then  echo &quot;cache archive already exists (${cacheArchive})&quot;else  tar -czf &quot;$cacheArchive&quot; node_modules  gsutil -m cp &quot;$cacheArchive&quot; ${1}/fiW pierwszej linii tworzy on sumę kontrolną MD5 pliku yarn.lock. W drugiej definiuje nazwę archiwum .zip, zawierającą sumę kontrolną. W kolejnych liniach definiuje warunek sprawdzający, czy plik o takiej nazwie już istnieje (co sugeruje brak potrzeby odkładania do bucketu kopii podręcznej zasobów), w przeciwnym wypadku tworząc archiwum zawierające folder node_modules i umieszczając je w buckecie. Do komunikacji z usługą Cloud Storage wykorzystywane jest narzędzie gsutil, a nazwa bucketu przekazywana jest do skryptu w parametrze ${1}.Skrypt pobierający kopię podręczną zależności (yarn-cache-builder/yarn-cache-push.sh) wygląda następująco:lockChecksum=$(md5sum yarn.lock | cut -d&#39; &#39; -f1)cacheArchive=$(basename &quot;$PWD&quot;).node_modules.${lockChecksum}.tar.gzif gsutil -q stat ${1}/&quot;$cacheArchive&quot;; then  gsutil -m cp ${1}/&quot;$cacheArchive&quot; .  tar -xzf &quot;$cacheArchive&quot;fiPierwsze dwie linie są identyczne, jak w skrypcie tworzącym. W kolejnych liniach zdefiniowany jest warunek, sprawdzający czy plik o odpowiedniej nazwie istnieje. W przypadku istnienia pliku z odpowiednią sumą kontrolną, archiwum .zip pobierane jest za pomocą narzędzia gsutil, a następnie wypakowywane. Dzięki temu, zależności w trakcie budowania aplikacji, będą już istnieć w folderze node_modules i nie będą pobierane z Internetu.Konfiguracja frontenduKonfiguracja aplikacji frontendowej składa się z:  konfiguracji Terraform (infrastructure/frontend.deploy.tf):resource &quot;google_cloudbuild_trigger&quot; &quot;deploy-main-demo-frontend&quot; {  provider = google-beta  name = &quot;deploy-main-demo-frontend&quot;  description = &quot;[demo] Deploy main: frontend&quot;  filename = &quot;frontend/deploy.cloudbuild.yaml&quot;  included_files = [    &quot;frontend/**&quot;  ]  github {    owner = var.github-owner    name = var.github-repository    push {      branch = &quot;^main$&quot;    }  }  substitutions = {    _CACHE_BUCKET = &quot;gs://${var.cache-bucket-name}&quot;    _FRONTEND_BUCKET = &quot;gs://${var.frontend-bucket-name}&quot;  }}Konfiguracja Terraform aplikacji frontendowej nie różni się za bardzo od konfiguracji infrastruktury, ponieważ również tworzy trigger w usłudze Cloud Build. Różnicą jest tutaj inna ścieżka do definicji zadań Cloud Build oraz inna ścieżka do folderu, w którym zmiany wyzwolą uruchomienie zadania, a także przekazanie zmiennych do konfiguracji Cloud Build,  konfiguracji Cloud Build (frontend/deploy.cloudbuild.yaml):steps:  - id: fetch-dependencies-cache    name: gcr.io/cloud-builders/gsutil    entrypoint: bash    args: [&#39;../yarn-cache-builder/yarn-cache-fetch.sh&#39;, &#39;${_CACHE_BUCKET}&#39;]    dir: &#39;frontend&#39;  - id: yarn-install    name: node:12.13.1    entrypoint: yarn    args: [&#39;install&#39;]    dir: &#39;frontend&#39;  - id: yarn-build    name: node:12.13.1    entrypoint: yarn    args: [&#39;build&#39;]    dir: &#39;frontend&#39;  - id: copy-dist    name: gcr.io/cloud-builders/gsutil    args: [&#39;cp&#39;,&#39;-r&#39;,&#39;frontend/dist/frontend-app/*&#39;, &#39;${_FRONTEND_BUCKET}&#39;]    waitFor:      - yarn-build  - id: push-dependencies-cache    name: gcr.io/cloud-builders/gsutil    entrypoint: bash    args: [&#39;../yarn-cache-builder/yarn-cache-push.sh&#39;, &#39;${_CACHE_BUCKET}&#39;]    dir: &#39;frontend&#39;Konfiguracja Cloud Build składa się z kolejnych kroków, których wykonanie zbuduje i wdroży aplikację frontendową. W kroku fetch-dependencies-cache pobierana jest kopia podręczna zależności (jeśli taka istnieje), następnie wykonywane są polecenia yarn-install oraz yarn-build, które budują aplikację. Po zbudowaniu aplikacji, pliki statyczne są kopiowane do bucketa w usłudze Cloud Storage, a po zakończeniu, tworzona jest nowa kopia podręczna zależności. Wszystkie kroki są wykonywane w katalogu frontend (parametr dir), a nazwy bucketów przekazane są w postaci zmiennych: ${_FRONTEND_BUCKET} (bucket przeznaczony na pliki statyczne), ${_CACHE_BUCKET} (bucket zawierający kopię podręczną zależności). Do komunikacji z usługą Cloud Storage wykorzystywane jest narzędzie gsutil,  konfiguracji Terraform, tworzącej bucket na pliki statyczne (infrastructure/frontend-bucket.tf):resource &quot;google_storage_bucket&quot; &quot;frontend-website-bucket&quot; {  provider = google-beta  name = var.frontend-bucket-name  location = var.gcp-region  force_destroy = true  website {    main_page_suffix = &quot;index.html&quot;  }}resource &quot;google_storage_bucket_iam_member&quot; &quot;frontend-website-bucket-public-permissions&quot; {  bucket = google_storage_bucket.frontend-website-bucket.name  role = &quot;roles/storage.objectViewer&quot;  member = &quot;allUsers&quot;}Składa się ona z dwóch części. W pierwszej tworzony jest bucket w Cloud Storage. Konfiguracja jest podobna do tej, która tworzy bucket przeznaczony na kopię podręczną zależności. Główną różnicą jest tutaj parametr force_destroy, który umożliwia usunięcie bucketu, nawet jeśli zawiera on pliki, a także konfiguracja witryny internetowej (opisana wcześniej). W drugiej części nadawane są uprawnienia publiczne do bucketu (rola roles/storage.objectViewer dla allUsers).Konfiguracja funkcjiKonfiguracja funkcji składa się z:  konfiguracji Terraform (infrastructure/function.deploy.tf):resource &quot;google_cloudbuild_trigger&quot; &quot;deploy-main-demo-function&quot; {  provider = google-beta  name = &quot;deploy-main-demo-function&quot;  description = &quot;[demo] Deploy main: function&quot;  filename = &quot;function/deploy.cloudbuild.yaml&quot;  included_files = [    &quot;function/**&quot;  ]  github {    owner = var.github-owner    name = var.github-repository    push {      branch = &quot;^main$&quot;    }  }  substitutions = {    _CACHE_BUCKET = &quot;gs://${var.cache-bucket-name}&quot;    _PUBSUB_TOPIC = var.pubsub-topic    _FIRESTORE_COLLECTION = var.firestore-collection    _GCP_REGION = var.gcp-region  }}Konfiguracja Terraform funkcji jest podobna do konfiguracji aplikacji frontendowej. Jedynymi różnicami są inne ścieżki, a także przekazywane zmienne.  konfiguracji Cloud Build (function/deploy.cloudbuild.yaml):steps:  - id: fetch-dependencies-cache    name: gcr.io/cloud-builders/gsutil    entrypoint: bash    args: [&#39;../yarn-cache-builder/yarn-cache-fetch.sh&#39;, &#39;${_CACHE_BUCKET}&#39;]    dir: &#39;function&#39;  - id: yarn-install    name: node:12.13.1    entrypoint: yarn    args: [&#39;install&#39;]    dir: &#39;function&#39;  - id: deploy-function    name: gcr.io/cloud-builders/gcloud    args: [      &#39;beta&#39;, &#39;functions&#39;, &#39;deploy&#39;, &#39;demo-function&#39;,      &#39;--runtime&#39;, &#39;nodejs10&#39;,      &#39;--trigger-topic&#39;, &#39;${_PUBSUB_TOPIC}&#39;,      &#39;--region&#39;, &#39;${_GCP_REGION}&#39;,      &#39;--timeout&#39;, &#39;10&#39;,      &#39;--memory&#39;, &#39;128MB&#39;,      &#39;--max-instances&#39;, &#39;1&#39;,      &#39;--entry-point&#39;, &#39;main&#39;,      &#39;--set-env-vars&#39;, &#39;COLLECTION_NAME=${_FIRESTORE_COLLECTION}&#39;    ]    dir: &#39;function&#39;  - id: push-dependencies-cache    name: gcr.io/cloud-builders/gsutil    entrypoint: bash    args: [&#39;../yarn-cache-builder/yarn-cache-push.sh&#39;, &#39;${_CACHE_BUCKET}&#39;]    dir: &#39;function&#39;Konfiguracja Cloud Build składa się z kolejnych kroków, których efektem będzie działająca funkcja, wyzwalana za pomocą Cloud Pub/Sub. Kroki związane z kopią podręczną zależności są analogiczne do konfiguracji aplikacji frontendowej. Ponieważ funkcja jest napisana w czystym JavaScript, to nie wymaga budowania z TypeSript do JavaScript (jak w przypadku aplikacji frontendowej), dlatego pominięty został krok wykonujący polecenie yarn-build. W linii 7 wykonywane jest jedynie polecenie yarn-install, które pobiera wymagane przez funkcję zależności. W kroku deploy-function tworzona jest funkcja na podstawie kodów źródłowych, zawartych w folderze function. Do konfiguracji zostały przekazane zmienne: ${_PUBSUB_TOPIC} (zawierająca nazwę topicu, który triggeruje funkcję), ${_GCP_REGION} (zawierająca nazwę regionu, w którym zostanie uruchomiona funkcja), ${_FIRESTORE_COLLECTION} (zawierająca nazwę kolekcji w Cloud Firestore, w której funkcja będzie utrwalać dane) oraz zmienna zawierająca nazwę bucketu przechowującego kopię podręczną zależności.Ponieważ triggery uruchamiające Cloud Build po zmianach konfiguracji infrastruktury (w folderze infrastructure) są tworzone przez ten sam kod, to za pierwszym razem trzeba utworzyć je ręcznie. W tym celu, w folderze infrastructure należy wykonać polecenia:  terraform init - inicjalizuje Terraform, tworząc folder .terraform zawierający stan i wykorzystywane wtyczki;  terraform plan - wykonuje plan działania Terraform, dzięki czemu można zweryfikować konfigurację pod kątem błędów, jeszcze przed uruchomieniem narzędzia;  terraform apply - akceptuje zmiany wynikające ze stanu aktualnego oraz wynikającego z konfiguracji, czyli wdraża infrastrukturę do chmury. W efekcie tego polecenia, utworzone zostaną wszystkie zasoby (triggery Cloud Build, buckety Cloud Storage, topic Cloud Pub/Sub).Kolejne zmiany kodu infrastruktury będą uruchamiać zadanie w Cloud Build, dzięki czemu zmiany infrastruktury będą automatycznie nanoszone w chmurze.Utworzone triggery Cloud Build można również wyzwolić ręcznie, za pomocą Cloud Console. Można tam znaleźć historię wykonywanych zadań oraz logi zawierające ich przebieg.W przedstawionym rozwiązaniu, kody aplikacji przechowywane są w repozytorium GitHub i zawierają wrażliwe dane (konfigurację w plikach frontend/src/environments/environment.ts oraz infrastructure/terraform.tfvars). Jeśli chcemy wykorzystać takie rozwiązanie, powinniśmy utworzyć prywatne repozytorium GitHub, w przeciwnym wypadku, nasza konfiguracja wycieknie do sieci. Do przechowywania konfiguracji możemy wykorzystać np. usługę Cloud Secret Manager lub zaimplementować własne rozwiązanie, oparte chociażby o Cloud Storage.Zaletą IaC, oprócz automatyzacji tworzenia infrastruktury, jest również wersjonowanie. Każda zmiana infrastruktury wiąże się ze zmianą kodu Terraform i może wymagać np. pull requesta, lub napisania testów, sprawdzających poprawność konfiguracji. Dzięki temu, proces wdrażania infrastruktury jest powtarzalny oraz bardziej odporny na błędy, niż ręczne tworzenie zasobów z poziomu GUI, czy konsoli.Migracja do chmury - podsumowanieOpisany przeze mnie proces migracji do chmury Google Cloud Platform, jest jedynie przykładem, zawierającym minimalną ilość wiedzy, potrzebnej do rozpoczęcia przygody z chmurą Google. Nie powinniśmy podejmować się takiego procesu nie posiadajac wcześniej wiedzy na temat usług GCP, ponieważ w najgorszym wypadku, może się to skończyć bardzo wysokimi rachunkami.Przed rozpoczęciem zabawy z GCP, proponuję zapoznać się z materiałami od Google, np. w formie kursów wideo, dostępnych na platformach Coursera czy Pluralsight. Bardzo dużo informacji znajdziemy również w oficjalnej dokumentacji GCP - od specyfikacji i API usług, po gotowe rozwiązania w formie “best practices” czy “how to”.Warto pamiętać również o GCP Free Tier, w ramach którego otrzymujemy na start $300, które w połączeniu z darmowymi limitami, w zupełności wystarczą do zapoznania się z platformą. Kursy wideo zawierają również zadania techniczne (Qwiklabs), które wykonujemy na prawdziwej chmurze, wykorzystując do tego wygenerowane na potrzeby zadania konta - z tego również warto skorzystać.",
"url": "/2021/03/03/przyklad-migracji-do-chmury.html",
"author": "Michał Hoja",
"authorUrl": "/authors/mhoja.html",
"image": "mhoja.jpg",
"highlight": "/assets/img/posts/2021-03-03-przyklad-migracji-do-chmury/clouds.jpeg",
"date": "03-03-2021",
"path": "pl/2021-03-03-przyklad-migracji-do-chmury"
}
,


"2021-02-19-wszystko-co-chcielibyscie-wiedziec-o-fontach-ale-baliscie-sie-zapytac-html": {
"title": "Wszystko, co chcielibyście wiedzieć o fontach, ale baliście się zapytać",
"lang": "pl",
"tags": "font czcionka rem woff font-face",
"content": "Osoby pracujące na frontendzie często mają do czynienia z fontami, czy to wrzucają je do projektu, czy po prostu zmieniają sposób, w jaki tekst wyświetla się na stronie.Na pierwszy rzut oka może się wydawać, że to bajecznie proste.Jest jednak kilka ważnych rzeczy, na które warto zwracać uwagę i być ich świadomym.Sposoby określania wielkości fontów (EM, REM, PX)Zacznijmy od początku. Jak poprawnie zdefiniować wielkość fontów? W większości przypadków wyboru dokonujemy pomiędzy PX, EM a REM.Przejdźmy do tego pierwszego.Piksele są wygodne do definiowania, ponieważ każdy intuicyjnie rozumie jak one działają i jaką przestrzeń zajmują na stronie. Tworzenie animacji, nadawanie ramek czy tworzenie różnych innych elementów graficznych często wydaje nam się prostsze w px. Schody zaczynają się, kiedy będziemy chcieli być przyjaznym dla użytkownika i umożliwić mu zmianę wielkości fontów w przeglądarce (i chodzi tutaj o ustawienia fontów w ustawieniach, a nie funkcjonalność przybliżania i oddalania strony, ona nadal będzie działać poprawnie). Używając pikseli nie jesteśmy w stanie tego obsłużyć i zawsze będziemy nadpisywać preferencje użytkownika. A więc jak można rozwiązać ten problem? Z pomocą przychodzą nam jednostki względne.Takimi jednostkami są właśnie EM i REM. Nazywamy je względnymi, ponieważ aby dowiedzieć się, jaka jest faktycznie ich wartość, należy spojrzeć, gdzie dana deklaracja się znajduje. W przypadku EM występuje dziedziczenie względem elementu nadrzędnego, a REM dziedziczy po roocie aplikacji. Przy pracy nad większymi projektami często faworyzuje się REM nad EM ze względu na prostotę w utrzymaniu. Przy pracy nad tymi jednostkami warto jednak zwrócić uwagę na kilka rzeczy.Żeby wyliczyć, na ile ustawić wartość REM, aby odpowiadała konkretnej wartości w pikselach, weźmy przykład:font-size domyślnie mamy 16px, a potrzebujemy nadać pewnemu elementowi wielkość 10px. Możemy skorzystaćz internetowego kalkulatoraalbo rozwiązać równanie, którenaprowadzi nas na poprawną wartość: 16px * x= 10px. Nie brzmi zachęcająco? Warto w takim razie zastanowić się, czyfaktycznie ważne jest, aby było to dokładnie 10px, ponieważ możemy przyjąć wartość 0.6rem, która może się okazaćdostatecznie bliska. Jeżeli nie godzimy się na takie przybliżanie i musimy trzymać się konkretnych wartości,a korzystamy z preprocessora do CSS, warto napisać metodę, która policzy to za nas, przyjmując w parametrze docelowąwartość w px. Tworzenie zmiennych określających różne wartości REM zawierające w nazwie odniesienie do konkretnych wartość w PX nie jest dobrąpraktyką, ponieważ w przypadku zmiany wielości w root’cie aplikacji wszystkie te zmienne przestają mieć sens.Przejdźmy jednak do wcześniej przytoczonego problemu z nadpisywaniem preferencji użytkownika dotyczących  wielkości fontów. Tak jak wcześniej wspomniałem, większość przeglądarek ma wartość domyślną fontu ustawioną na 16px. Jeżeli nadpisalibyśmy ją używając px narzucamy własne style ponad ustawienia użytkownika, ale jeżeli będziemy używać jednostek względnych – takiego problemu nie będzie. Przyjrzyjmy się pewnemu przykładowi:Na pierwszy rzut oka wszystko wydaje się takie samo, jedyną różnicą jest to, że po prawej stronie zadeklarowaliśmy wielkość fontu,która jest taka sama jak domyślna. Po zmianie wielkości fontu w ustawieniach przeglądarki nagle widać różnicę:Na koniec jedna uwaga  dotyczącą używania pikseli. Na urządzeniach przenośnych piksele są innej wielkości, niż na standardowym monitorze. Mimo iż urządzenie ma większą rozdzielczość, przeglądarka traktują ją, jak gdyby była nawet kilkukrotnie mniejsza. Dzieje się tak, ponieważ ekran na telefonie mamy mały, a zagęszczenie pikseli wysokie. Powoduje to, że co na ekranie monitora jest czytelne, na ekranie komórki jest ledwo zauważalne. Na przykład, kiedy wejdziemy w przeglądarce Chrome w opcje developerskie i ustawimy, aby przeglądarka zmieniła wielkość okna i dopasowała się do iPhone’a X, wyświetli nam się rozdzielczość 375 x 812, gdy realna rozdzielczość urządzenia wynosi 1125 x 2436.Więc jakie jest rozstrzygnięcie? Czy jesteśmy w stanie wybrać i pozostać przy tylko jednym sposobie określania wielkości? Nie, wszystko zależy od projektu, w jakim pracujemy, doboru narzędzi i makiet dostarczonych przez klienta. Racjonalnym wyborem często stają się jednostki względne, dzięki łatwiejszym możliwościom ich skalowania. W ten sposób szanujemy również użytkownika i pozwalamy mu na dostosowywanie fontów według jego potrzeb. Z kolei kiedy wiemy, że w projekcie będzie dużo elementów graficznych, związanych z animacjami, gdzie wielkość, czytelność fontów i możliwość ich powiększania schodzi na drugi plan – sensowniejszym wyborem staje się użycie px.Warto też być świadomym, że łączenie obu jednostek nie zawsze jest złe. Czasami chcemy, aby różne elementy powiększały się razem z fontem, a czasami zostały stałe, niezależnie od wielkości elementów dookoła (na przykład wielkości ramek, cieni, czy czasem odstępów pomiędzy elementami, kluczowy jest tutaj kontekst).Rozszerzenia fontówPrzejdźmy teraz do rozszerzeń plików z fontami. Kiedy będziemy chcieli użyć na stronie innych fontów niż domyślne, będziemy musieli wrzucić do projektu plik z definicją fontu. Rozszerzeń takich plików jest sporo i często budzi to konsternację: wystarczy pojedynczy plik, czy może po jednym z każdego z rodzaju? Przed odpowiedzeniem na to pytanie przyjrzyjmy się popularnym rozszerzeniom:  EOT (Embedded OpenType) – przestarzały już format, obsługiwany tylko i wyłącznie przez Internet Explorer. IE w wersji 9+ potrafi już odczytywać inne formaty, więc jeżeli nie zależy nam na starych wersjach IE, to nie musimy się martwić tym rozszerzeniem, tym bardziej, że często fonty o tym rozszerzeniu nie są kompresowane i zajmują więcej miejsca.  TTF/OFT (TrueType) - fonty o tym rozszerzeniu powinny zawierać w sobie wszystkie możliwe grubości i odmiany danego kroju. Częściej jest on stosowany w programach graficznych, niż do wyświetlania tekstów na stronach internetowych. Może okazać się przydatny dla starszych przeglądarek, szczególnie mobilnych, ale z uwagi na brak kompresji pliki mogą ważyć więcej. OFT jest nowszą odmianą standardu, lecz specyfikację ma bardzo podobną. Jeżeli zależy nam głównie na aktualnych przeglądarkach możemy odpuścić sobie oba rozszerzenia.  WOFF (Web Open Font Format) – jest to forma TTF, ale z dodaną kompresją i większym wsparciem przeglądarek. Aktualnie jest wspierany przez wszystkie przeglądarki i jest domyślnym wyborem dla używania zewnętrznych fontów w projektach.  WOFF2 (Web Open Font Format 2) – jak po nazwie można się domyślić, jest ulepszoną wersją WOFF. Oferuje mniejsze wielkości plików i lepszą wydajność. Aktualnie jest wspierany przez wszystkie przeglądarki za wyjątkiem IE.Dodając fonty do aplikacji możemy ograniczyć się tylko do WOFF i WOFF2, jeżeli chcemy wpierać tylko aktualne przeglądarki. Oferują one małe rozmiary plików, a przeglądarka dobrze sobie radzi z wyświetlaniem ich na stronie. Jeżeli na urządzeniach, na których aplikacja będzie uruchamiana brakuje dla nich wsparcia, warto zostać przy domyślnych systemowych fontach z uwagi na wydajność aplikacji. Jeżeli jednak chcemy rozszerzać wsparcie na tak wiele urządzeń, jak to możliwe i nie boimy się ich wspierania, możemy wtedy skorzystać z plików EOT i TTF/OTF.Dołączanie fontów do aplikacjiNa koniec przyjrzyjmy się sposobom dołączania fontów do projektu. Są różne możliwości, a kilka z nich,razem z ich zaletami i wadami, przedstawiam poniżej:@Font-faceNajbardziej znany i lubiany sposób definiowania nowych fontów, które chcemy użyć w projekcie. Tworzymy nowy plik i wrzucamy do niego ścieżki do fontów (najczęściej w formacie WOFF i WOFF2). Plik ma strukturę, w której definiujemy nazwę i ścieżki do fontów:@font-face {  font-family: &#39;NowyFont&#39;;  src: url(&#39;nowyFont.woff2&#39;) format(&#39;woff2&#39;),       url(&#39;nowyFont.woff&#39;) format(&#39;woff&#39;),}Struktura jest czytelna, więc możemy łatwo ją modyfikować i dodawać kolejne fonty. Aplikacja nie czeka aż fonty zostaną załadowane, więc nie musimy się martwić, że wydłużamy czas ładowania aplikacji. Może to być poręczne, ale powoduje również pewne problemy, ponieważ przez to mamy często do czynienia z ‘mrugnięciem fontu’, czyli z szybką zmianą wyglądu tekstu. Są różne strategie radzenia sobie z tym przez przeglądarki. Najczęściej tekst jest na chwilę niewidzialny i jeżeli po pewnym czasie font nie zostanie załadowany, tekst się wyświetla, ale z użytym domyślnym fontem. W momencie kiedy font się załaduje, tekst ‘wskakuje’ we właściwy font. Im font więcej waży i im mamy gorszy transfer tym dłużej może to potrwać. Wszystkie żądania odbywają się niezależnie, więc trudno je razem zgrupować, aby mogły załadować się w tym samym momencie.font-displayNa ratunek z pojawiającym się nagle tekstem przychodzi własność font-display: swap, którą możemy ustawić w naszym pliku @font-face. Dzięki niemu nie uświadczymy już dwóch mrugnięć (pojawienie się tekstu i zmiana fontu), a jednego: tekst pojawia się od razu i czeka, aż font zostanie załadowany. Zaletą tego rozwiania jest prostota w konfiguracji. Minusem jest to, że tak naprawdę niewiele nam to jeszcze daje.Wczytywanie fontu przed renderowaniem zawartości stronyInnym sposobem radzenia sobie z ładowaniem fontów jest dołączanie do pliku html linku, wskazującego na font, z którego chcemy skorzystać:    &amp;lt;link rel=&quot;preload&quot; href=&quot;https://fonts.gstatic.com/s/opensans/v13/k3k702ZOKiLJc3WVjuplzBampu5_7CjHW5spxoeN3Vs.woff2&quot; as=&quot;font&quot; type=&quot;font/woff2&quot; crossorigin&amp;gt;Rozwiązanie jest łatwe w implementacji, jeden link i mamy wszystko czego potrzebujemy. Nie musimy się martwić, że teksty na stronie będą źle wyglądać bez fontów. Rozwiązanie to może wydawać się bardzo dobre, za wyjątkiem jednej rzeczy. Przez to, że wczytujemy font wcześniej, opóźniamy wejście na stronę. Czas wejścia na stronę jest dłuższy, ponieważ załadowanie fontu ma wyższy priorytet i odbędzie się przed wyświetleniem aplikacji.Wykrywanie momentu załadowania fontówInnym rozwiązaniem, potrafiącym rozwiązać problemy, na które możemy napotkać podczas ‘wskakujących fontów’ jest używanie biblioteki pozwalającej wykryć moment (lub posługując się bezpośrednio CSS Font Loading API), w którym dany font, czy też wszystkie fonty, zostały już załadowane. Przeważnie używamy tego, aby nadać jakąś klasę na najwyższym elemencie struktury html, która poinformuje, że font został załadowany. Będzie to sygnał, który pozwoli nam zmienić dynamicznie wygląd aplikacji. Nie powinno się raczej mieszać javascriptu do zarządzania css, ale czasami nie ma innego rozwiązania. Szczególnie wtedy, kiedy musimy wiedzieć, czy font został już poprawnie załadowany, aby uzależnić od tego określone elementy aplikacji.Oczywiście istnieją bardziej zaawansowane metody wczytywania fontów, które w inny sposób rozwiązują problemy migającego tekstu (często w sieci nazywane jako FOIT, kiedy mamy do czynienia z pojawiającym się fontem, lub FOUT, kiedy tekst zmienia nagle swój styl), lecz w zdecydowanej większość przypadków wyżej wymienione rozwiązania są w pełni wystarczające. Warto również zwrócić uwagę, że wsparcie fontów w przeglądarkach dynamicznie się zmienia, patrząc chociażby na kolejne wersji najpopularniejszych przeglądarek. Sprawia to, że warto śledzić zmiany, ponieważ prędzej czy później cała zawarta tutaj wiedza może się zdezaktualizować.…i jeszcze jedna drobna kwestia, stanowiąca bardziej ciekawostkę. W artykule posługiwałem się pojęciem font, unikając słowa czcionka. Przyjęło się używać te słowa zamiennie, aczkolwiek, poprawnym słowem jest font, można też mówić czcionka komputerowa. Samo słowo czcionka oznacz kawałek metalu z pojedynczą literą służącą do dawnych metod drukarstwa. Jest to jednak mało istotne i nie ma to dużego znaczenia w komunikacji, a osoby, które nadmiernie zwracają na to uwagę można oskarżyć o snobizm :)",
"url": "/2021/02/19/wszystko-co-chcielibyscie-wiedziec-o-fontach-ale-baliscie-sie-zapytac.html",
"author": "Piotr Grobelny",
"authorUrl": "/authors/pgrobelny.html",
"image": "pgrobelny.webp",
"highlight": "/assets/img/posts/2021-02-01-wszystko-co-chcielibyscie-wiedziec-o-fontach-ale-baliscie-sie-zapytac/thumbnail.jpeg",
"date": "19-02-2021",
"path": "pl/2021-02-01-wszystko-co-chcielibyscie-wiedziec-o-fontach-ale-baliscie-sie-zapytac"
}
,


"2021-02-10-odpowiedzi-na-pytania-cd-tech-5-html": {
"title": "Domain Driven Design w oparciu o Axon Framework - odpowiedzi na pytania!",
"lang": "pl",
"tags": "ddd axon consdata tech cdtech#5",
"content": "Zgodnie z obietnicą, przyszedł czas odpowiedzieć na pytania, które pozostały bez odpowiedzi podczas naszego minionego eventu - Consdata Tech.Nie przedłużając więc, przejdę do części właściwej. ;)Jeśli replayEvents jest wyłączony, to czy framework przy starcie aplikacji potrafi odczytać stan np. Issue z bazy danych i dalej na nim operować?Zacznę od pytania, przy którym zatrzymam się trochę dłużej, bo jest tu parę kwestii do omówienia.Należałoby na samym początku rozróżnić odtwarzanie zdarzeń na dwie podgrupy:  Odtwarzanie stanu agregatu  Odtwarzanie stanu systemu (replay)Pierwszy typ odtwarzania służy do skonstruowania stanu agregatu (na podstawie wszystkich zdarzeń lub snapshota + delty zdarzeń), w momencie, gdy pojawia się nowy command w systemie.Jest to konieczne, aby sprawdzić niezmienniki/warunki i w odpowiedzi na command wyemitować zdarzenie, lub też nie.Na ten mechanizm nie mamy wpływu i nie da się go wyłączyć.Drugi typ odtwarzania to właśnie te “replay events”, którego zapewne tyczy się pytanie.Tu już jak najbardziej mamy wiele możliwości konfiguracyjnych w Axonie.Wszystko rozbija się o Event Processory i składowanie Tracking Tokenów w Token Store.W momencie, gdy odpalimy aplikację na domyślnej konfiguracji, wszystkie Tokeny trafiają do InMemoryTokenStore, czyli siłą rzeczy po restarcie aplikacji przepadają.Ta wiedza pozwoliła mi zaprezentować EventSourcing na ostatniej prelekcji.Oczywiście domyślne konfiguracje nie są zalecane na produkcji, a dokumentacja jawnie mówi o wymogu dostarczenia implementacji Token Store do konfiguracji.Możemy to zrobić samemu lub wykorzystać gotowca, jeśli istnieje (np dla Mongo istnieje taki Token Store).W momencie, gdy dostarczymy takową implementację, wówczas Tracking Tokeny zaczną trafiać do bazy.Nadal jednak można będzie wykonać replay events (np zapiąć jakiś endpoint na taką akcję), o ile nie wyłączymy tego całkowicie w konfiguracji (opisuję to w kolejnym akapicie).Czy można wyłączyć domyślne zachowanie Axona w kontekście odtwarzania eventów przy starcie aplikacji?Tak, odpowiedziałem na to pytanie podczas webinaru, lecz w tej chwili mogę wrócić już z konkretnym przykładem w kodzie.Podczas transmisji wspominałem o tym, że taka możliwość istnieje i można wręcz wskazać w konfiguracji Event Processor (czyli obiekt odpowiadający za przetwarzanie zdarzeń), który ma być ignorowany podczas odtwarzania.Event Processory domyślnie dostają nazwę stworzoną na podstawie javowego package, w którym się znajdują (zdaje się, że wyjątkiem są tu Sagi).Możemy je też jawnie nazywać z użyciem adnotacji @ProcessingGroup.Mając nazwę processora, w konfiguracji można wykluczyć go z akcji odtwarzania stanu, poprzez sprytne wskazanie, aby Tracking Tokeny dla niego tworzyły się na końcu “jego kolejki”.Wygląda to następująco:@Configurationpublic class AxonConfig {    @Autowired    public void customTrackingConfig(EventProcessingConfigurer configurer) {        var trackingProcessorConfig = TrackingEventProcessorConfiguration                .forSingleThreadedProcessing()                .andInitialTrackingToken(StreamableMessageSource::createHeadToken);        // This prevents from replaying in MovieSaga        configurer.registerTrackingEventProcessor(&quot;MovieSagaProcessor&quot;,                org.axonframework.config.Configuration::eventStore,                c -&amp;gt; trackingProcessorConfig);        // This prevents from replaying MultipleMoviesRefreshedEvent in RefreshEventHandler        configurer.registerTrackingEventProcessor(&quot;com.kociszewski.moviekeeper.notreplayable&quot;,                org.axonframework.config.Configuration::eventStore,                c -&amp;gt; trackingProcessorConfig);    }}Opisując powyższy kod językiem ludzkim: klasa MovieSaga oraz wszystkie klasy obsługujące zdarzenia, które znajdują się w package’u com.kociszewski.moviekeeper.notreplayable zostaną pominięte podczas operacji replay events.Uwaga! Jeśli wskażecie package za wysoko, to nie zadziała - musi to być nazwa package’u, w którym znajdują się klasy obsługujące zdarzenia (zagnieżdżenia nie będą uwzględniane).Namiary na projekt podane są na końcu, jest to ten sam projekt, który omawiałem we wcześniejszych wpisach.Jak sobie radzicie w Consdacie z organizacją Event Stormingów w czasach pracy zdalnej?Z pełnym spokojem mogę polecić miro - świetne narzędzie z mnóstwem możliwości.Na koncie bezpłatnym można naprawdę wiele zdziałać. Oczywiście to wciąż nie to samo, co ‘realne’ spotkanie w grupie projektującej, ale większość zalet Event Stormingu wyciągniemy nawet w remote.Co w momencie, gdy Axon zacznie odtwarzać zdarzenia z klasy, która wrzuca lub aktualizuje coś w bazie?Tutaj jest kilka możliwych rozwiązań. Najprostszym z nich jest opatrzyć klasę/metodę adnotacją @DisallowReplay, powinno to skutecznie zatrzymać Event Processor.Jeśli jednak chcielibyśmy, aby klasa/metoda brała udział w odtwarzaniu zdarzeń (przykładowo chcemy móc migrować na inną bazę, ale nie chcemy aby nam się duplikowały krotki w bazie), to widzę tu dwa rozwiązania.Jednym z nich jest konfiguracja sterowana profilem - powinno zadziałać, ale nie sprawdzałem.Drugi sposób to obsługa takiego przypadku ręcznie, samemu:public class MovieProjection {    ...    @EventHandler    public void handle(MovieSavedEvent event) {        movieRepository.findByExternalMovieId(event.getExternalMovie().getExternalMovieId()).ifPresentOrElse(                movie -&amp;gt; handleMovieDuplicate(),                () -&amp;gt; persistMovie(event));    }        private void handleMovieDuplicate() {        notifySubscribers(new MovieDTO(MovieState.ALREADY_ADDED));    }    ...}W tym przypadku wywołanie notifySubscribers powoduje wyemitowanie (przy użyciu queryUpdateEmittera) nowego filmu z ustawionym statusem ALREADY_ADDED.Pojawienie się filmu o takim statusie jest obsługiwane wyżej i skutkuje zwróceniem kodu błędu 409, mówiącym o konflikcie.Co dalej?Jeśli szukacie projektów związanych z tematem DDD i Axona, to zerknijcie tu:  Repozytorium, które omawiałem na Consdata Tech  Repozytorium, które cytuję tutaj",
"url": "/2021/02/10/odpowiedzi-na-pytania-cd-tech-5.html",
"author": "Mateusz Kociszewski",
"authorUrl": "/authors/mkociszewski.html",
"image": "mkociszewski.webp",
"highlight": "/assets/img/posts/2021-02-10-odpowiedzi-na-pytania-cd-tech-5/lightboard.jpeg",
"date": "10-02-2021",
"path": "pl/2021-02-10-odpowiedzi-na-pytania-cd-tech-5"
}
,


"2021-02-09-migracja-schematow-bazy-danych-html": {
"title": "Migracja schematów bazy danych",
"lang": "pl",
"tags": "migration database liquibase flyway kubernetes job initContainers",
"content": "Niemożliwe jest rozwijanie aplikacji bez równoczesnego rozwijania schematu bazy danych.Tak jak podczas rozwijania naszej aplikacji, stosujemy wzorce projektowe przy pisaniu kodu, tak w przypadku rozwijania schematów baz danych również powinniśmy stosować się do takich wytycznych, aby rozwijanie bazy danych było przyjemnością, a nie ostatecznością.Migracja schematu, czyli ewolucyjny projekt bazy danych (ang. Evolutionary Database Design)Aby umożliwić proste rozwijanie naszej bazy danych, możemy skorzystać z przetestowanego już zbioru zaleceń 🔗¹ 🔗², dzięki którym zmiany będą mniej inwazyjne, a nawet bezprzerwowe (zero downtime deployment).Przechowywanie zmian w repozytorium kodówPodobnie jak kod aplikacji, schemat bazy danych jest częścią tworzonego systemu i dobrą praktyką jest przechowywanie go w repozytorium. Jako że zmiany schematu bazy danych są przyrostowe, to repozytorium powinno zawierać wszystkie zmiany umożliwiające odtworzenie zawsze tej samej bazy danych.Dzięki temu uzyskamy szereg korzyści:  możliwość zweryfikowania zmian,  prostsze utrzymywanie kolejności zmian schematu bazy danych, dzięki utrzymywaniu zmian w jednym miejscu,  bezproblemowe odtworzenie zawsze dokładnie takiej samej bazy danych (np. lokalnie do celów testowych).Jeśli kilka projektów korzysta z bazy, to mimo wszystko zmiany powinny odbywać się tylko na jednym głównym repozytorium (nawet na osobnym).Każda zmiana schematu powinna być migracjąKażdorazowe modyfikowanie schematu, powinno odbywać się za pomocą migracji, którą zapiszemy w repozytorium, unikając ręcznego modyfikowania schematu.Dzięki temu nie będzie sytuacji, w której po odtworzeniu bazy, będzie się ona różnić od oryginału.Dodatkowo zmiany wykonane ręcznie (z pominięciem migracji), mogą wpłynąć na jego późniejsze wykonanie za pomocą migracji, np. gdy wykonujemy CREATE TABLE bezpośrednio na bazie, a później dodajemy migrację schematu, która to procesuje, to w takim wypadku otrzymamy błąd informujący o tym, że taka tabela już istnieje.Wersjonowanie (rosnące) każdej zmiany  Każda zmiana powinna być wersjonowana, np. w osobnych plikach, w których zachowanie kolejności będzie wykonane za pomocą podbijania licznika lub dodania znacznika czasu do nazwy pliku.Jest to bardzo ważne, ponieważ inna kolejność uruchomienia migracji schematów bazy danych może całkowicie zmienić jej sens albo nawet całkowicie ją uniemożliwić.  Zalecane jest, aby każda zmiana była jak najmniejsza i najlepiej możliwa do odwrócenia. Przykładowo tworząc indeksy na istniejących tabelach, najlepiej rozbić ich tworzenie do osobnych wersji.Jeśli nie zostaną one wykonane w osobnych migracjach schematu, wtedy narażamy się na ryzyko takie jak opisane niżej:          w tej samej migracji schematu bazy danych tworzymy indeks A oraz indeks B,      stworzenie indeksu A zajmuje 5 minut i przebiega poprawnie,      stworzenie indeksu B zajmuje ponad 5 minut i powoduje błąd TimeoutException,      oba indeksy zostają wycofane i indeks A musi być ponownie założony,      w przypadku, gdyby tworzenie indeksów było rozdzielone na osobne migracje, wtedy nie będzie konieczności ponownego tworzenia indeksu A (i ponownego poświęcania 5 minut na ten cel).        Wykonywane zmiany powinny być przyrostowe, czyli zmiana dla danej wersji powinna być uruchomiona tylko raz.Przykładowe biblioteki  SQL          Liquibase                  liquibase.org                    FlywayDB                  flywaydb.org                    MyBatis Migration                  mybatis.org/migrations                      NoSQL          Mongock                  github.com/cloudyrock/mongock          mongock.io                    Migrate-mongo                  npmjs.com/package/migrate-mongo          github.com/seppevs/migrate-mongo                    liquibase-mongodb                  github.com/liquibase/liquibase-mongodb                    w powyższych bibliotekach tworzenie i uruchamianie przyrostowe zmian jest wbudowane.Zmiany powinny być kompatybilne wsteczJest to szczególnie ważne, jeśli chcemy bezprzerwowo aktualizować naszą bazę danych, w takim wypadku aktualizacja bazy danych nie powinna spowodować, że starsza wersja aplikacji przestanie działać.Przykładowo nowa kolumna powinna mieć domyślną wartość lub przyjmować null-e.Zmiana nazwy kolumny lub jej usunięcie powinno być rozbite na kilka etapów, tak aby jej prawdziwe usunięcie było wykonane nie w docelowej wersji, tylko np. w następnej iteracji, gdy będziemy pewni, że żadna aplikacja z niej nie korzysta.Aplikowanie zmianAplikowanie zmian wykonanych w ramach ewolucyjnej bazy danych jest już zależne od konkretnego przypadku.Jeśli baza danych jest ściśle związana z jedną aplikacją, to możemy ją uruchamiać bezpośrednio z kodu 🔗⁵ .W przypadku gdy aplikacja jest rozproszona i nie chcemy blokować wszystkich instancji aplikacji na czas migracji schematu lub gdy kilka różnych aplikacji korzysta z tej bazy danych, możemy uruchamiać migrację niezależnie od aplikacji.W tym przypadku mamy następujące możliwości:  Wykonywanie zmian uruchamianych za pomocą CI/CD (np. automatycznie po otrzymaniu nowej wersji).Na repozytorium wykonujemy merge z migracjami schematu bazy danych, Jenkins wykrywa zmianę na repozytorium i wykonuje ją na bazie wskazanej w konfiguracji.  Z wykorzystaniem mechanizmów dostarczonych przez platformę, na której będzie to uruchamiane. Przykładowo dla Kubernetesa możemy:          wykorzystać initContainers, celem odpalenia migracji schematu bazy danych przed uruchomieniem docelowego kontenera z aplikacją (w takim wypadku każda replika uruchomi migrację schematu, a to mechanizm migracji musi zapewnić, że zmiany zostaną wykonane wszystkie na jednym kontenerze i do tego jednorazowo) 🔗⁵ ,      wykorzystać do tego celu Joby, które jednorazowo uruchomią migrację (a w przypadku problemów, wykonają automatyczne ponowienie n-razy) 🔗³ 🔗⁴ 🔗⁵ ,      wykorzystać dwa powyższe mechanizmy 🔗⁵,uruchomić joba, aby wykonał migrację schematu bazy danych, oraz initContainers tak, aby poczekał na zakończenie migracji schematu(a jeśli wszystkie migracje schematu wymagane przez aplikację, są już zaaplikowane, to uruchomienie docelowego kontenera).      Przykłady - KubernetesPrzykładowe rozwiązanie łączące mechanizm initContainer oraz job-a, dla różnych bibliotek do migracji schematów bazy danych:   Liquibase   FlywayDB   MyBatis Migration   Migrate-mongoBibliografia  https://www.martinfowler.com/articles/evodb.html  https://en.wikipedia.org/wiki/Evolutionary_database_design  https://cloud.google.com/solutions/addressing-continuous-delivery-challenges-in-a-kubernetes-world#related_kubernetes_concepts_2  https://kubernetes.io/docs/concepts/workloads/controllers/job/  https://andrewlock.net/deploying-asp-net-core-applications-to-kubernetes-part-7-running-database-migrations/",
"url": "/2021/02/09/migracja-schematow-bazy-danych.html",
"author": "Jakub Goszczurny",
"authorUrl": "/authors/jgoszczurny.html",
"image": "jgoszczurny.jpg",
"highlight": "/assets/img/posts/2021-02-09-migracja-schematow-bazy-danych/bird-migrations.jpg",
"date": "09-02-2021",
"path": "pl/2021-02-09-migracja-schematow-bazy-danych"
}
,


"2021-01-11-pisanie-pluginow-do-intellij-html": {
"title": "Jak napisać plugin w IntelliJ",
"lang": "pl",
"tags": "intellij ide plugin",
"content": "IntelliJ IDEA to obecnie jedno z popularniejszych, jeśli nie najpopularniejsze IDE dla Javy. Jedną z jego zalet jest duża baza pluginów, dostarczana przez samo JetBrains, jak i społeczność. W tym artykule przedstawię podstawy tworzenia pluginów dla tej platformy. Większość z zebranych tu informacji (poza elementami związanymi z Javą) ma zastosowanie do dowolnego IDE ze stajni JetBrains.Sposoby rozszerzania IDEW platformie mamy do dyspozycji trzy podstawowe metody rozszerzenia IDE:  actions - kod uruchamiany w momencie, gdy użytkownik kliknie np. w element menu kontekstowego lub paska narzędziowego,  listeners - pozwalają nasłuchiwać na eventy generowane przez platformę lub inne pluginy,  extension points - interfejsy pozwalające na rozszerzenie konkretnych elementów platformy lub innych pluginów.Więcej informacji o tym, co i jak działa, możecie znaleźć w dokumentacji IDE. Ja natomiast przedstawię ujęcie praktyczne, w ramach którego stworzymy prosty plugin dla IntelliJ, który wykorzysta wszystkie opisane wyżej metody. Przykładowy plugin, na podstawie adnotacji, wygeneruje plik tekstowy z prostą dokumentacją dla klas Javy (do tego wykorzystane zostaną akcje i listenery) oraz pozwoli na wyświetlenie tej dokumentacji w ramach Quick Documentation (Extension point). Mając klasę:@Doc(&quot;To jest testowa klasa&quot;)public class TestClass {    @Doc(&quot;To jest testowe pole&quot;)    private String field;    @Doc(&quot;To jest testowe pole &quot; + Constants.ADDITIONAL_TEXT)    private String fieldWithExtraInfo;}plugin wygeneruje dokumentację w postaci:TestClass: To jest testowa klasafield: To jest testowe polefieldWithExtraInfo: To jest testowe pole z dodatkowym opisem ze stałeNa githubie znajduje się repozytorium z pluginem oraz repozytorium z testową aplikacjąBaza pluginuPrzygodę z własnym pluginem rozpoczynamy od stworzenia nowego projektu typu Intellij Platform Plugin. Wygenerowana zostanie domyślna struktura katalogów oraz plik XML z manifestem (META-INF/plugin.xml). Zawiera on podstawowe informacje o pluginie tj. jego unikalny identyfikator, opis, dane autora itd. W tym pliku definiujemy również zależność pluginu, w naszym przypadku będą takie trzy:&amp;lt;depends&amp;gt;com.intellij.modules.platform&amp;lt;/depends&amp;gt;&amp;lt;depends&amp;gt;com.intellij.modules.lang&amp;lt;/depends&amp;gt;&amp;lt;depends&amp;gt;com.intellij.modules.java&amp;lt;/depends&amp;gt;  *.platform - zawiera interfejsy i klasy potrzebne do implementacji serwisów, akcji, UI, rozszerzeń itd.;  *.lang - zawiera elementy związane z przetwarzaniem plików, nawigacją itd.;  *.java - zawiera elementy ułatwiające prace z plikami Javy;AkcjeMając już gotowy projekt bazowy możemy przejść do implementacji akcji czyli dodamy element do menu kontekstowego edytora i do drzewa projektu - przycisk ‘Generuj’. Na razie będzie wyświetlał tylko popup z informacją.Każda akcja w platformie musi rozszerzać abstrakcyjną klasę AnAction i implementować metodę actionPerformed. Tworzymy zatem naszą akcję, którą nazwiemy DocAction.Na razie implementacja actionPerformed sprowadzać będzie się do wyświetlenia okna dialogowego. Do tego celu wykorzystamy gotowego helpera w platformie - Messages:public class DocAction extends AnAction {    @Override    public void actionPerformed(@NotNull AnActionEvent event) {        Messages.showMessageDialog(event.getProject(), &quot;Witam!&quot;, &quot;&quot;, null);    }}Na koniec musimy jeszcze powiązać naszą akcję z jakimś elementem IDE, który ją wywoła. Akcję definiujemy w manifeście w następujący sposób:    &amp;lt;actions&amp;gt;        &amp;lt;action id=&quot;com.consdata.article.DocAction&quot;                class=&quot;com.consdata.article.DocAction&quot;                text=&quot;Generuj&quot;&amp;gt;            &amp;lt;add-to-group group-id=&quot;EditorPopupMenu&quot; anchor=&quot;last&quot;/&amp;gt;            &amp;lt;add-to-group group-id=&quot;ProjectViewPopupMenu&quot; anchor=&quot;last&quot;/&amp;gt;        &amp;lt;/action&amp;gt;    &amp;lt;/actions&amp;gt;Atrybut id to unikalny identyfikator naszej akcji (zwykle Fully qualified name klasy), w atrybucie class wskazujemy implementację akcji. W atrybucie text określamy tekst przycisku w menu. Wewnątrz węzła akcji definiujemy, do której grupy akcji chcemy ją ’przypiąć’. W naszym przypadku jest to EditorPopupMenu - menu kontekstowe edytora oraz ProjectViewPopupMenu - menu kontekstowe drzewa projektu. Po uruchomieniu projektu i klknięciu prawym przyciskiem myszy na edytor pliku w menu kontekstowym pokaże nam się opcja ‘Generuj’Po kliknięcu w ‘Generuj’ pojawi się komunikat ‘Witam!’Dobrą praktyką jest organizowanie kilku akcji pluginu w ramach grupy. Co prawda nasz plugin będzie dostarczał tylko jedną akcję, niemniej warto w celach dydaktycznych to zaprezentować. Analogicznie do akcji, grupy rozszerzają dedykowaną klasę ActionGroup. Implementacja grupy jest już zadaniem nieco bardziej złożonym, dlatego platforma dostarcza domyślną implementację w postaci DefaultActionGroup. Tworzymy zatem nową klasę DocGroup:public class DocGroup extends DefaultActionGroup {}Stworzoną wcześniej akcję dodajemy do nowej grupy, a następnie naszą grupę DocGroup dołączamy do menu kontekstowego edytora i drzewa projektu:    &amp;lt;actions&amp;gt;        &amp;lt;group id=&quot;com.consdata.article.DocGroup&quot;               class=&quot;com.consdata.article.DocGroup&quot; popup=&quot;true&quot;               text=&quot;Dokumentacja&quot;&amp;gt;            &amp;lt;add-to-group group-id=&quot;EditorPopupMenu&quot; anchor=&quot;last&quot;/&amp;gt;            &amp;lt;add-to-group group-id=&quot;ProjectViewPopupMenu&quot; anchor=&quot;last&quot;/&amp;gt;            &amp;lt;action id=&quot;com.consdata.article.DocAction&quot;                    class=&quot;com.consdata.article.DocAction&quot;                    text=&quot;Generuj&quot;&amp;gt;            &amp;lt;/action&amp;gt;        &amp;lt;/group&amp;gt;    &amp;lt;/actions&amp;gt;Po uruchomieniu w miejscu przycisku ‘Generuj’ pojawi się przycisk ‘Dokumentacja’, kierujący do naszego podmenu.Plugin będzie generował dokumentację na podstawie adnotacji Doc, dlatego też nie ma sensu uruchamiać akcji na plikach, które takiej adnotacji nie zawierają. W naszym pluginie przycisk ‘Dokumentacja’ pozostawimy widocznym, ale jednocześnie, jeśli celem akcji jest plik niezawierający adnotacji Doc, to wówczas warto by był nieaktywny. Widoczność i aktywność przycisku aktualizujemy w metodzie update wywoływanej w przypadku, gdy użytkownik kliknie w jakiś element interfejsu zawierający tę akcję lub grupę:public class DocGroup extends DefaultActionGroup {    @Override    public void update(@NotNull AnActionEvent event) {        event.getPresentation().setVisible(true);        event.getPresentation().setEnabled(ofNullable(CommonDataKeys.PSI_FILE.getData(event.getDataContext()))                .map(this::hasDocAnnotation)                .orElse(false));    }    private boolean hasDocAnnotation(PsiFile psiFile) {        if (psiFile instanceof PsiJavaFile) {            PsiJavaFile javaFile = (PsiJavaFile) psiFile;            return Arrays.stream(javaFile.getClasses())                    .anyMatch(psiClass -&amp;gt; psiClass.hasAnnotation(&quot;com.consdata.doc.Doc&quot;)                            || Arrays.stream(psiClass.getFields()).anyMatch(field -&amp;gt; field.hasAnnotation(&quot;com.consdata.doc.Doc&quot;))                            || Arrays.stream(psiClass.getAllMethods()).anyMatch(method -&amp;gt; method.hasAnnotation(&quot;com.consdata.doc.Doc&quot;)));        }        return false;    }}W powyższym kodzie skorzystaliśmy z jednego z podstawowych elementów platformy tj. PSI (Program Structure Interface). Najprościej rzecz ujmując, PSI odpowiada w platformie za parsowanie plików i generowanie modelu kodu, który zawierają. Dzięki temu w prosty sposób możemy, przejść po wszystkich klasach, ich polach i metodach zwartych w danym pliku. Pełny model PSI można w prosty sposób podejrzeć za pomocą pluginu np. PsiViewer. W powyższym przykładzie pobieramy model PSI z kontekstu zdarzenia akcji (podobnie możemy odnieść się do otwartego edytora, czy zaznaczonego fragmentu tekstu - patrz com.intellij.openapi.actionSystem.CommonDataKeys). Następnie weryfikujemy, czy jest to model Javy. Jeśli tak, to sprawdzamy, czy klasy, ich pola lub metody zawierają adnotację Doc. Po tej zmianie, kliknięcie prawym przyciskiem myszy w plik, który nie zawiera tej adnotacji zdezaktywuje przycisk ‘Generuj’. Na koniec pozostaje już tylko zaimplementować sam mechanizm generowania dokumentacji oraz zapisać tę dokumentację we wskazanym pliku.Generowanie dokumentacjiDokumentacja będzie składać się z wierszy zawierających nazwę dokumentowanego elementu (np. nazwa pola) oraz opisu z adnotacji. Przyjmując, że na wejsciu mamy model PSI pliku z kodem, wygenerowanie takiej dokumentacji może wyglądać mniej więcej tak:    private String getDoc(PsiFile psiFile) {        if (psiFile instanceof PsiJavaFile) {            PsiJavaFile javaFile = (PsiJavaFile) psiFile;            Map&amp;lt;String, String&amp;gt; doc = Arrays.stream(javaFile.getClasses())                    .filter(psiClass -&amp;gt; psiClass.hasAnnotation(&quot;com.consdata.doc.Doc&quot;))                    .collect(Collectors.toMap(PsiClass::getName, psiClass -&amp;gt; evaulate(psiClass.getProject(), psiClass.getAnnotation(&quot;com.consdata.doc.Doc&quot;).findAttributeValue(&quot;value&quot;))));            doc.putAll(Arrays.stream(javaFile.getClasses())                    .map(PsiClass::getFields)                    .flatMap(Arrays::stream)                    .filter(psiField -&amp;gt; psiField.hasAnnotation(&quot;com.consdata.doc.Doc&quot;))                    .collect(Collectors.toMap(PsiField::getName, psiField -&amp;gt; evaulate(psiField.getProject(), psiField.getAnnotation(&quot;com.consdata.doc.Doc&quot;).findAttributeValue(&quot;value&quot;)))));            doc.putAll(Arrays.stream(javaFile.getClasses())                    .map(PsiClass::getMethods)                    .flatMap(Arrays::stream)                    .filter(psiMethod -&amp;gt; psiMethod.hasAnnotation(&quot;com.consdata.doc.Doc&quot;))                    .collect(Collectors.toMap(PsiMethod::getName, psiMethod -&amp;gt; evaulate(psiMethod.getProject(), psiMethod.getAnnotation(&quot;com.consdata.doc.Doc&quot;).findAttributeValue(&quot;value&quot;)))));            return String.join(&quot;\\n&quot;, doc.entrySet().stream().map(e -&amp;gt; e.getKey() + &quot;: &quot; + e.getValue()).collect(Collectors.toList()));        } else {            return &quot;&quot;;        }    }Korzystając z PSI przechodzimy po wszystkich klasach w pliku oraz ich polach i metodach. Jeśli zawierają adnotację Doc, to do wierszy dokumentacji dodajemy nazwę pola, znak ‘:’ oraz wartość adnotacji. Nie możemy jednak wartości adnotacji pobrać wprost, gdyż nie jest to String. Zależnie od tego co znajduje się w adnotacji będzie to model jakiegoś wyrażenia. Musimy takie wyrażenie wyliczyć (aby np. rozwiązać użyte w nim stałe). Dla ułatwienia tego typu zadań powstał ewaluator stałych, który jest częścią fasady Javy dla PSI:    private String evaulate(Project project, PsiAnnotationMemberValue expression) {        return JavaPsiFacade.getInstance(project).getConstantEvaluationHelper().computeConstantExpression(expression).toString();    }Helper ten automatycznie rozwiąże nam wszystkie wyrażenie z adnotacji. Należy jednak pamiętać o tym, aby projekt był zaindeksowany, a elementy wyrażenia widoczne w ramach projektu.Na koniec naszą mapę konwertujemy do oczekiwanego Stringa i zapisujemy w pliku:    private void save(String doc, AnActionEvent event) throws IOException {        final FileSaverDialog dialog = FileChooserFactory.getInstance().createSaveFileDialog(                new FileSaverDescriptor(&quot;Generate to&quot;, &quot;&quot;, &quot;txt&quot;), event.getProject());        VirtualFileWrapper wrapper = dialog.save(event.getProject().getProjectFile(), &quot;&quot;);        VirtualFile vFile = wrapper.getVirtualFile(true);        vFile.setCharset(StandardCharsets.UTF_8);        vFile.setBinaryContent(doc.getBytes());    }Przy zapisie do pliku zahaczamy o kolejny ważny element platformy - warstwę plików wirtualnych (Virtual File System). Udostępnia ona wspólne api do wszystkich operacji na plikach niezależnie od ich położenia. Pliki w tej warstwie nie są fizycznymi plikami z dysku, tylko snapshotem systemu plików z danego czasu. Czasami możemy spotkać się z komunikatem w IDE, pytającym o to, czy wczytać zmiany z dysku oraz czy korzystać wciąż z lokalnych plików. Zdarza się też, że zmiany w projekcie nie odzwierciedlają zmian na dysku. Wtedy klikamy zwykle w ‘Synchronize’ lub ‘Reload all from disk’ - synchornizując wtedy właśnie pliki wirtualne.Podobnie jak w innych systemach, IntelliJ udostępnia gotowe definicje okien dialogowych dla np. zapisu plików. W naszym przypadku jest to FileSaverDialog. Po wybraniu pliku i kliknięciu ‘zapisz’, dialog zwraca nam wrapper na pliku wirtualnym. Z wrappera pobieramy instancję pliku wirtualnego i zapisujemy do niego treść naszej dokumentacji.Tym samym zaimplementowaliśmy w pełni działającą akcję naszego pluginu. Teraz przyszedł czas na kolejny element rozszerzający działanie IDE - listenery.ListeneryListenery pozwalaja pluginowi reagować na eventy w ramach platformy, emitowane czy to przez IDE, czy przez inne pluginy. W przykładzie plugin najpierw wyemituje event po zapisie dokumentacji. Jeśli plugin otrzyma event z szyny, to otworzy plik z dokumentacją w edytorze. Takie przechodzenie pomiędzy plikami jest, co prawda trochę przekombinowane, ale w prosty sposób zaprezentuje działanie całego mechanizmu.W pierwszej kolejności musimy zdefiniować interfejs naszego listenera:public interface DocSaved {    void onSave(VirtualFile virtualFile);}Interfejs zawiera jedną definicję metody. Argumentem jest plik wirtualny, do którego dokumentacja została zapisana. W kolejnym korku tworzymy implementację listenera:public class DocListener implements DocSaved {    private final Project project;    public DocListener(Project project) {        this.project = project;    }    @Override    public void onSave(VirtualFile virtualFile) {        Optional.ofNullable(virtualFile).ifPresent(vFile -&amp;gt; new OpenFileDescriptor(project, vFile).navigate(true));    }}Listener będzie operować na poziomie pojedynczego projektu. W metodzie onSave wykorzystaliśmy OpenFileDescriptor, który pozwala na otwarcie edytora danego pliku z poziomu kodu.Podobnie jak inne elementy rozszerzające działanie platformy, listenery także musimy zdefiniować w manifeście pluginu:    &amp;lt;projectListeners&amp;gt;        &amp;lt;listener class=&quot;com.consdata.article.DocListener&quot;                  topic=&quot;com.consdata.article.DocSaved&quot;/&amp;gt;    &amp;lt;/projectListeners&amp;gt;Atrybut class zawiera implementację interfejsu naszego listenera, zdefiniowanego w polu topic. Listenery działające w obrębie całej aplikacji definiujemy w węźle applicationListeners.Coś musi jeszcze wywołać event, na który nasłuchujemy. W metodzie save naszej akcji dodajemy linię:   event.getProject().getMessageBus()                    .syncPublisher(Topic.create(&quot;com.consdata.article.DocSaved&quot;, DocSaved.class))                    .onSave(vFile);W efekcie po zapisie dokumentacji do pliku automatycznie otworzy nam się edytor pliku dokumentacji.Extension PointsOstatnim elementem, jaki poznamy będą Extension Pointy. Są to wprost nazwane interfejsy, które pozwalają na rozszerzenie konkretnych elementów platformy lub innych pluginów. Pełna lista dostępnych extension pointów znajduje się na https://jetbrains.org/intellij/sdk/docs/appendix/resources/extension_point_list.html.W ramach naszego projektu stworzymy rozszerzenie dla extension pointa com.intellij.documentationProvider. Pozwala on na wzbogacenie  dokumetancji prezentowanej np. w Quick Documentation (Ctrl + Q) o własne treści.Tworzym klasę DocProvider implementująca interfejs DocumentationProvider oraz ExternalDocumentationProvider.public class DocProvider implements DocumentationProvider, ExternalDocumentationProvider {    @Nullable    @Override    public String fetchExternalDocumentation(Project project, PsiElement psiElement, List&amp;lt;String&amp;gt; list) {        return getDoc((PsiClass) psiElement);    }    @Override    public boolean hasDocumentationFor(PsiElement psiElement, PsiElement psiElement1) {        if (psiElement instanceof PsiClass) {            PsiClass psiClass = (PsiClass) psiElement;            return psiClass.hasAnnotation(&quot;com.consdata.doc.Doc&quot;)                    || Arrays.stream(psiClass.getFields()).anyMatch(field -&amp;gt; field.hasAnnotation(&quot;com.consdata.doc.Doc&quot;))                    || Arrays.stream(psiClass.getMethods()).anyMatch(field -&amp;gt; field.hasAnnotation(&quot;com.consdata.doc.Doc&quot;));        }        return false;    }    private String getDoc(PsiClass psiClass) {        Map&amp;lt;String, String&amp;gt; doc = new HashMap&amp;lt;&amp;gt;();        doc.put(psiClass.getName(), evaulate(psiClass.getProject(), psiClass.getAnnotation(&quot;com.consdata.doc.Doc&quot;).findAttributeValue(&quot;value&quot;)));        doc.putAll(Arrays.stream(psiClass.getFields())                .filter(psiField -&amp;gt; psiField.hasAnnotation(&quot;com.consdata.doc.Doc&quot;))                .collect(Collectors.toMap(PsiField::getName, psiField -&amp;gt; evaulate(psiField.getProject(), psiField.getAnnotation(&quot;com.consdata.doc.Doc&quot;).findAttributeValue(&quot;value&quot;)))));        doc.putAll(Arrays.stream(psiClass.getMethods())                .filter(psiMethod -&amp;gt; psiMethod.hasAnnotation(&quot;com.consdata.doc.Doc&quot;))                .collect(Collectors.toMap(PsiMethod::getName, psiMethod -&amp;gt; evaulate(psiMethod.getProject(), psiMethod.getAnnotation(&quot;com.consdata.doc.Doc&quot;).findAttributeValue(&quot;value&quot;)))));        return doc.entrySet().stream().map(e -&amp;gt; &quot;&amp;lt;b&amp;gt;&quot; + e.getKey() + &quot;&amp;lt;/b&amp;gt;: &quot; + e.getValue()).collect(Collectors.joining(&quot;&amp;lt;br/&amp;gt;&quot;));    }    private String evaulate(Project project, PsiAnnotationMemberValue expression) {        return JavaPsiFacade.getInstance(project).getConstantEvaluationHelper().computeConstantExpression(expression).toString();    }        @Nullable    @Override    public List&amp;lt;String&amp;gt; getUrlFor(PsiElement element, PsiElement originalElement) {        return Collections.singletonList(&quot;&quot;);    }    @Override    public boolean canPromptToConfigureDocumentation(PsiElement psiElement) {        return false;    }    @Override    public void promptToConfigureDocumentation(PsiElement psiElement) {    }}W fetchExternalDocumentation generujemy dokumentację dla wskazanej klasy analogicznie jak w akcji, tylko dla jednej klasy. Dodatkowo wzbogacamy dokumentację o znacznik HTML (w oknie QuickDocumentation rednerowany jest HTML). W hasDocumentationFor decydujemy czy dla danego elementu powinniśmy wygenerować dodatkową dokumentację, podobnie jak w przypadku grupy, gdy decydowaliśmy o jej aktywności.Tak zdefiniowany provider możemy teraz zdefiniować w manifeście pluginu jako rozszerzenie dla documentationProvider:    &amp;lt;extensions defaultExtensionNs=&quot;com.intellij&quot;&amp;gt;        &amp;lt;documentationProvider implementation=&quot;com.consdata.article.DocProvider&quot;/&amp;gt;    &amp;lt;/extensions&amp;gt;Po uruchomieniu pluginu, przechodzimy do klasy UseCase. Najeżdżamy na typ TestClass i wybieramy Ctrl + Q. Po wywołaniu skrótu pojawi się popup z wygenerowaną dokumentacją:Po ponownym użyciu skrótu otworzy się szuflada:Oczywiście w przypadku innych extension pointów potrzeba będzie zdefiniować inne mechanizmy. Co do zasady sprawa jest jednakowa dla wszystkich extension pointów. Definujemy implementację dla konkretnego przypadku i łączymy ją z określonym extensionPointem w manifeście.Plugin w IntelliJ - dystrybucjaGotowy plugin pakujemy do paczki klikając prawym przyciskiem myszy na projekt pluginu i wybieramy Prepare plugin module ... for deployment. Wygenerowany JAR pluginu możemy opublikować w repo IntelliJ lub samodzielnie udostępniać go z prywatnego repo.Przydatne linki  Kompleksowe wprowadzenie do pluginów  Forum odnośnie api i pisania pluginów  Lista platformowych extension pointów  Tworzenie prywatnego repo pluginów  Repo z pluginem  Repo z testową aplikacją",
"url": "/2021/01/11/pisanie-pluginow-do-intellij.html",
"author": "Bartosz Radliński",
"authorUrl": "/authors/bradlinski.html",
"image": "bradlinski.webp",
"highlight": "/assets/img/posts/2021-01-11-pisanie-pluginow-do-intellij/top.jpeg",
"date": "11-01-2021",
"path": "pl/2021-01-11-pisanie-pluginow-do-intellij"
}
,


"2020-11-24-testy-e2e-cypress-html": {
"title": "Testy e2e z Cypress",
"lang": "pl",
"tags": "Cypress e2e e2e testing JavaScript",
"content": "Łamy bloga Consdata Tech gościły już wiele wpisów dotyczących testowania aplikacji - jednak żaden z nich nie zajmował się tematem testów e2e (end-to-end).Na rynku testów e2e, czyli takich, które sprawdzają funkcjonalność od początku do końca, symulując zachowanie użytkownika i weryfikując UI, stale dominuje Selenium - narzędzie wielu programistom znane, z historią sięgającą 2004 roku.Dziś na warsztat wezmę dość młody framework - Cypress, który może okazać się kuszącą alternatywą dla wcześniej wspomnianego narzędzia.Kilka słów o CypressNależy zacząć od tego, że Cypress nie jest nakładką na Selenium - jest to całkowicie niezależny byt, zbudowany na JavaScript.Również pisanie testów odbywa się w tym języku, a jeśli ktoś miał wcześniej styczności z narzędziami takimi jak Chai, czy Mocha, to będzie czuł się jak w domu - Cypress zaadoptował i dopasował wykorzystywane przez nie rozwiązania do swoich potrzeb.Wykonywane scenariusze testowe mogą być przez nas podglądane na żywo - są one uruchamiane na wybranej przez nas przeglądarce, może to być Edge, Chrome, Firefox lub wbudowany Electron. Podczas ich wykonywania każdy kolejny krok zapisywany jest pod postacią snapshotów - migawek, do których możemy zajrzeć w każdym momencie i zweryfikować stan aplikacji.Przez to, że Cypress uruchamiany jest w naturalnym środowisku twojej aplikacji, możemy korzystać ze wszystkich dobrodziejstw nowoczesnych DevToolsów: od debugowania kodu, poprzez kontrolę sieci czy podglądanie DOM.W Cypressie developer nie musi pamiętać, aby pisać jawne oczekiwanie na zakończenie poleceń (wait znane z Selenium). Jest to zrobione za nas przez twórców frameworka - wszystko dzieje się automatycznie i kolejne polecenia oraz asercje wykonują się w odpowiednim momencie.Kolejną z rzeczy wartych wspomnienia jest łatwość użycia i konfiguracji — jedno polecenie instaluje framework, a kolejne uruchamia dashboard, którym posługiwanie się jest intuicyjne.Rys. 1. Dashboard CypressRozpoczynanie pracyW kwestii wymagań, Cypress nie potrzebuje wiele: wystarczy Node.js oraz ulubione IDE.Aby umożliwić rozpoczęcie pracy, wystarczy wykonać następujące polecenia w katalogu projektu:  npm init, aby stworzyć projekt node’owy  npm install cypress --save-dev dla instalacji CypressaPo zakończeniu instalacji powstanie domyślna struktura:Rys. 2. Struktura projektu CypressObjaśnienia:  fixtures - miejsce, gdzie możemy przechowywać gotowe zestaw danych, np. służące do zamockowania usług;  integration - lokalizacja testów;  plugins - miejsce do załączania zewnętrznych rozszerzeń dla Cypressa;  support - tutaj znajdą się np. stałe powtarzające się w wielu scenariuszach czy też nowe, customowe polecenia do globalnego reużycia.Następnie przy pomocy komendy npx cypress open uruchamiamy dashboard i jeden z przykładowych testów.Scenariusze testoweW ramach tego wpisu, na warsztat weźmiemy stronę główną bloga Consdata Tech -  https://blog.consdata.tech/ i stworzymy dla niej dwa przypadki testowe.Pierwszy scenariusz będzie polegał na wejście na stronę, poczekaniu aż się załaduje i zweryfikowaniu kilku elementów, która potwierdzą nam, że portal jest w pełni działający.Drugi scenariusz, nieco bardziej rozbudowany, dokona kilku interakcji z blogiem.Pierwszy testW katalogu integration tworzymy nowy plik o nazwie blog.spec.js, przechodzimy do jego edycji i uzupełniamy go o następującą treść:describe(&#39;Blog Consdata Tech&#39;, () =&amp;gt; {    it(&#39;should check page fully loaded&#39;, () =&amp;gt; {        // ..    });})Tak jak wcześniej wspomniałem, jeśli ktoś miał wcześniej styczność z testami w JS, wszystko będzie dla niego jasne.Idąc od początku, describe nazywa nam całościowy kontekst, którego będą dotyczyć poszczególne scenariusze. it rozpoczyna konkretny przypadek testowy i to wewnątrz niego będziemy implementować właściwą logikę.W tym momencie uruchamiamy dashboard Cypressa. Zobaczymy w nim, że pojawił się nasz plik. Po kliknięciu w niego uruchomi się przeglądarka oraz scenariusz testowy:Rys. 3. Uruchomiony scenariusz w CypressZostawmy przeglądarkę i dashboard włączony w tle - Cypress nasłuchuje na zmiany i od razu je wykonuje. Dzięki temu mamy ciągłą pętlę zwrotną z informacją czy to, co tworzymy przynosi oczekiwane efekty.Wracając do tworzenia testu, jak wynika z nazwy should check page fully loaded, pierwszy z nich będzie sprawdzał czy strona, na którą wchodzimy poprawnie się załadowała. Dodajemy ciało do pustej metody:cy.visit(&#39;https://blog.consdata.tech/&#39;).then(() =&amp;gt;{    cy.get(&#39;.header-logo&#39;).should(&#39;be.visible&#39;);    cy.get(&#39;footer&#39;).should(&#39;be.visible&#39;);});Jak widać, napisany kod jest właściwie samoopisujący się. W pierwszej linii odwiedzamy podany adres, następnie, gdy to się odbędzie, szukamy w DOM elementu o klasie header-logo i sprawdzamy czy jest widoczny. Operacje powtarzamy dla elementu footer. W przeglądarce uświadczy nas poniższy obrazek:Rys. 4. Scenariusz testowy zakończony powodzeniemInterakcjeTwórcy Cypressa przygotowali zestaw metod, który ułatwi nam wykonywanie operacji na stronie. W skład tych poleceń wchodzi m.in. click(), type(), select() czy check().Jeszcze zanim przejdziemy do implementacji drugiego scenariusza, trzeba zauważyć, że każdy z nich będzie zaczynał się od tej samej akcji - otworzenia strony. Możemy wynieść ten kawałek kodu do metody beforeEach(), która jest uruchamiana przed każdym testem:beforeEach(() =&amp;gt; {    cy.visit(&#39;https://blog.consdata.tech/&#39;)})W ten sposób będziemy zgodni z regułą DRY :)Wracając do właściwego testu, rzućmy na niego okiem:it(&#39;search for a specific post&#39;, () =&amp;gt; {       const searchBoxElementId = &#39;#search-box&#39;;       cy.get(searchBoxElementId).should(&#39;be.not.visible&#39;); // weryfikacja czy element nie jest widoczny       cy.get(&#39;.desktop-navbar .search-icon&#39;).click(); // kliknięcie w ikonę wyszukiwarki       cy.get(searchBoxElementId).should(&#39;be.visible&#39;); // element powinien się pojawić       cy.get(searchBoxElementId).type(&#39;ansible&#39;); // wpisanie wartości       cy.get(searchBoxElementId).type(&#39;{enter}&#39;).then(() =&amp;gt; { // symulacja wciśnięcia przycisku enter, aby wysłać formularz           cy.get(&#39;.post-title&#39;).should(&#39;contain&#39;, &#39;Ansible - jak uporządkować chaos?&#39;); // weryfikacja oczekiwanego efektu       })   });Podobnie jak poprzednio, idąc linijka po linijce, jesteśmy w stanie łatwo rozczytać, co się wydarzyło, nawet bez pomocy komentarzy.Gdy zajrzymy do źródła strony w przeglądarce, okaże się, że w DOMie jest więcej elementów z klasą post-title, a mimo tego test przechodzi poprawnie. Jest to oczekiwane zachowanie - łańcuch komend get().should() znajduje wszystkie elementy i sprawdza czy w jakimkolwiek z nich znajduje się podana treść. Gdybyśmy chcieli sprawdzić czy pierwszy element zawiera konkretną wartość, możemy wykorzystać first():  cy.first(&#39;.post-title&#39;).should(&#39;contain&#39;, &#39;Ansible - jak uporządkować chaos?&#39;);PodsumowaniePrzystępność Cypressa (szczególnie dla web developerów, w związku z silnym zakorzenieniem w ekosystemie JavaScript) i minimum czasu potrzebnego na przygotowanie działającej konfiguracji mogą okazać się kluczowe dla osób szukających nietrudnego sposobu na automatyzację testów. Być może będzie to również okazja dla zrażonych do Selenium, by dać testom E2E drugą szansę.Źródła  https://docs.cypress.io/",
"url": "/2020/11/24/testy-e2e-cypress.html",
"author": "Adrian Marszałek",
"authorUrl": "/authors/amarszalek.html",
"image": "amarszalek.jpg",
"highlight": "/assets/img/posts/2020-11-24-testy-e2e-cypress/e2e-cypress.jpeg",
"date": "24-11-2020",
"path": "pl/2020-11-19-testy-e2e-cypress"
}
,


"2020-11-17-processing-java-stream-html": {
"title": "Java Stream - przetwarzanie elementów",
"lang": "pl",
"tags": "java stream benchmarking",
"content": "Java StreamStrumienie zostały dodane do Javy w wersji 8, wzbogacając język o namiastkę programowania funkcyjnego oraz alternatywę dla dobrze znanych pętli.Pierwsze pytania jakie się nasuwają, to czy warto stosować strumienie oraz jak robić to świadomie.Artykuł ma na celu analizę kilku podstawowych właściwości strumieni oraz porównanie ich z pętlami pod względem wydajności.Intermediate vs TerminalJava Stream API definiuje dwa rodzaje operacji, jakie możemy wykonać w trakcie przetwarzania strumieni, operacje pośrednie – intermediate,oraz operacje końcowe - terminal.Operacje pośrednie w sposób deklaratywny opisują, jak dane powinny być przetworzone, a dodatkowo zawsze zwracają obiekt typu Stream, co daje możliwość łączenia ich z kolejnymi operacjami pośrednimi.Jak widać na przykładzie strumień danych stworzony z listy filtruje te, które zaczynają się na literę “a”,następnie zamienia wszystkie litery na litery wielkie, a na koniec wyświetla przetworzone elementy.Niestety uruchamiając taki kod nic się nie zadzieje, a konsola nie wypisze żadnego elementuponieważ w naszym strumieniu brakuje operacji końcowej -  czyli takiej, która kończy strumień i sprawia, że nie można go już więcej przetwarzać.    private static final List&amp;lt;String&amp;gt; LIST = Arrays.asList(&quot;aab&quot;, &quot;aab&quot;,                                                            &quot;aac&quot;, &quot;aac&quot;,                                                            &quot;bbb&quot;, &quot;ccc&quot;, &quot;ddd&quot;,                                                            &quot;eee&quot;, &quot;fff&quot;, &quot;ggg&quot;);    public static void main(String[] args) {        LIST.stream()                .filter(e -&amp;gt; e.startsWith(&quot;a&quot;))                .map(String::toUpperCase)                .peek(System.out::println);    }Po dodaniu do kodu metody collect, dostajemy zamierzony efekt i mimo, że stworzona z przetworzonych danych lista nie jest przypisana do zmiennej, to strumień miał powód aby się wykonać.Listę metod pośrednich oraz końcowych znajdziemy tutaj.    private static final List&amp;lt;String&amp;gt; LIST = Arrays.asList(&quot;aab&quot;, &quot;aab&quot;,                                                            &quot;aac&quot;, &quot;aac&quot;,                                                            &quot;bbb&quot;, &quot;ccc&quot;, &quot;ddd&quot;,                                                            &quot;eee&quot;, &quot;fff&quot;, &quot;ggg&quot;);    public static void main(String[] args) {        LIST.stream()                .filter(e -&amp;gt; e.startsWith(&quot;a&quot;))                .map(String::toUpperCase)                .peek(System.out::println)                .collect(Collectors.toList());    }Kolejność przetwarzaniaKod prezentuje przykładowe przetwarzanie danych oparte o metodę filter oraz map.    private static final List&amp;lt;String&amp;gt; LIST = Arrays.asList(&quot;aab&quot;, &quot;aab&quot;,                                                            &quot;aac&quot;, &quot;aac&quot;,                                                            &quot;bbb&quot;, &quot;ccc&quot;, &quot;ddd&quot;);    public static void main(String[] args) {        List&amp;lt;String&amp;gt; a = LIST.stream()                .filter(e -&amp;gt; {                    System.out.println(&quot;Stream - filter: &quot; + e);                    return e.startsWith(&quot;a&quot;);                })                .map(e -&amp;gt; {                    System.out.println(&quot;Stream - map:&quot; + e);                    return e.toUpperCase();                })                .collect(Collectors.toList());    }Z logów wyświetlonych w konsoli widzimy, że operacje wykonywane są horyzontalnie, to znaczy dla każdego elementu od początku do końca. Dzięki temu, sekwencja skończy się na pierwszym niespełnionym warunku a nadmiarowy kod nie jest wykonywany.Stream - filter: aabStream - map:aabStream - filter: aabStream - map:aabStream - filter: aacStream - map:aacStream - filter: aacStream - map:aacStream - filter: bbbStream - filter: cccStream - filter: dddPrzetwarzanie równoległeWartościowym mechanizmem, który został dodany do Java Stream API jest możliwość równoległego przetwarzania strumieni, co daje możliwość wykorzystania większej liczby rdzeni procesora do wykonania zadania.Jak widać na załączonym kodzie wystarczy, że skorzystamy z metody parallelStream zamiast stream i nasze dane zostaną odpowiednio podzielone i przetworzone równolegle z użyciem wielu wątków.Stream tworzony w domyślny sposób korzysta ze wspólnej puli wątków, więc przetwarzanie w taki sposób większej ilości danych może zmniejszyć zasoby przeznaczone na inne procesy w aplikacji.Warto zwrócić uwagę na to, że samo dzielenie danych wymaga dodatkowej pracy, dlatego przeliczanie równoległe mniejszej ilości danych może trwać dłużej niż zrobienie tego samego z wykorzystaniem jednego wątku.Dodatkowo kolejność przetwarzanie elementów przez wiele wątków jednocześnie jest niedeterministyczna.    private static final List&amp;lt;String&amp;gt; LIST = Arrays.asList(&quot;aab&quot;, &quot;aab&quot;,                                                            &quot;aac&quot;, &quot;aac&quot;,                                                            &quot;bbb&quot;, &quot;ccc&quot;, &quot;ddd&quot;);    public static void main(String[] args) {        LIST.parallelStream()                .filter(e -&amp;gt; {                    System.out.println(&quot;Stream - filter: &quot; + e);                    return e.startsWith(&quot;a&quot;);                })                .map(e -&amp;gt; {                    System.out.println(&quot;Stream - map:&quot; + e);                    return e.toUpperCase();                })                .collect(Collectors.toList());    }BenchmarkWiemy już, jak przetwarzać Java Stream bardziej efektywnie. Nasuwa się pytanie, jak ich szybkość wypada w porównaniu do pętli, aby to sprawdzić zostały przeprowadzone testy przy użyciu Java Microbenchmark Harness i procesora Intel i7 10700K. Kod zaprezentowany niżej został uruchomiony na kolekcjach liczącej 1, 100, 10 000 oraz 1 000 000 elementów.@Benchmark@Fork(value = 1, warmups = 2)public void forLoop() {    List&amp;lt;String&amp;gt; convertedList = new ArrayList&amp;lt;&amp;gt;();    for (String word : DataProvider.WORDS) {        if (word.startsWith(PREFIX)) {                convertedList.add(word.toUpperCase());        }    }}@Benchmark@Fork(value = 1, warmups = 2)public void stream() {    List&amp;lt;String&amp;gt; convertedList = DataProvider.WORDS.stream()            .filter(word -&amp;gt; word.startsWith(PREFIX))            .map(String::toUpperCase)            .collect(Collectors.toList());}@Benchmark@Fork(value = 1, warmups = 2)public void parallelStream() {    List&amp;lt;String&amp;gt; convertedList = DataProvider.WORDS.parallelStream()            .filter(word -&amp;gt; word.startsWith(PREFIX))            .map(String::toUpperCase)            .collect(Collectors.toList());}Jednostką pomiarową jest liczba operacji na sekundę, czyli innymi słowy, ile razy wywołano daną metodę w ciągu jednej sekundy.Jak widać na wykresie dla 1 elementu, pętla okazała się zdecydowanie szybsza niż strumienie. Test 100 elementów pokazał że strumienie powoli doganiają zwykłą pętlę, jednocześnie strumień równoległy okazał się zdecydowanie najwolniejszym rozwiązaniem.W kolejnych testach dla 10 000 i 1 000 000 elementów, widzimy siłę przetwarzania równoległego. Dodatkowo warto zauważyć, że dla testu największej kolekcji, przetwarzanie w pętli uzyskało delikatnie słabszy wynik niż w jednowątkowym strumieniu.PodsumowanieNiekiedy złożone warunki filtrowania i przetwarzania kolekcji sprawiają, że kod napisany przy użyciu pętli możeby być mało czytelny.Czytelność można poprawić korzystając z Java Stream, a znając mechanizmy działające podczas przetwarzania strumieni możemy tworzyć rozwiązania równie szybkie jak przy użyciu petli.",
"url": "/2020/11/17/processing-java-stream.html",
"author": "Tomasz Drąg",
"authorUrl": "/authors/tdrag.html",
"image": "tdrag.jpg",
"highlight": "/assets/img/posts/2020-11-17-java-stream-processing/waterfall-stream.jpeg",
"date": "17-11-2020",
"path": "pl/2020-11-17-processing-java-stream"
}
,


"2020-11-04-nowa-forma-meetupu-html": {
"title": "Consdata Tech Webinar - rozmowa z Marcinem Mergo o nowej formie meetupu",
"lang": "pl",
"tags": "consdatatech meetup",
"content": "Consdata Tech to inicjatywa, której założeniem od początku było dzielenie się wiedzą i doświadczeniem w formie cyklicznych meetupów oraz integracja lokalnej społeczności zafascynoweanej światem Java. Wszystkie dotychczasowe edycje odbywały się w Poznaniu, w przestrzeni +Jeden, jednakże trwająca pandemia uniemożliwia zachowanie takiej formuły eventu. Ta sytuacja stała się motywatorem do tego, by kolejne edycje Consdata Tech zorganizować w postaci webinarowej i tym samym przenieść je z przestrzeni biurowej wprost do domów swoich uczestników. O nowej formie i oczekiwaniach względem niej oraz przede wszystkim, czy warto dołączać do webinaru, rozmawiamy z Marcinem Mergo - pomysłodawcą cyklu eventów.Chociaż wielu z nas ma już dosyć dyskutowania o wpływie koronawirusa na nasze życie, to tutaj nie sposób od tego nie zacząć. Mianowicie dotychczasowe edycje konferencji Consdata Tech odbywały się w salach konferencyjnych, gdzie prelegenci i goście uczestniczyli w meetingu twarzą w twarz. Co spowodowało, że postanowiliście przejść na wersję webinarową zamiast poczekać aż życie wróci do normy, gdy ponownie będzie można się spotkać w dotychczasowej formie?Przede wszystkim niepewność związana z koronawirusem. Bardzo ciężko jest na ten moment określić, kiedy tak naprawdę moglibyśmy wrócić do stacjonarnej formy przeprowadzania kolejnych edycji, a jednocześnie nie chcieliśmy czekać z założonymi rękami na rozwój sytuacji.Dodatkowo, zdalna forma organizacji meetupów nie jest dla nas całkowitą nowością - wszystkie dotychczasowe edycje Consdata Tech, poza tym, że odbywały się w formie stacjonarnej, były również transmitowane na żywo w internecie. Mamy już w związku z tym rozpracowane  sporo rozwiązań typowych dla zdalnej formy organizacji spotkań technicznych, co ułatwiło nam decyzję o przejściu na formę webinarów - po prostu nie jest to dla nas skok w kompletnie nieznane.Ostatnim powodem, który ostatecznie przekonał nas do tej formy, jest ogólna zmiana podejścia do pracy oraz komunikacji przez internet. To co dla wielu z nas jeszcze niedawno było marzeniem - w pełni zdalna praca - dzisiaj jest rzeczywistością, nie tylko w branży IT. Odcisnęło to swoje piętno nie tylko na sposobie pracy, ale też na inicjatywach pokroju Consdata Tech. Meetupy, spotkania i konferencje organizowane w formie online’owej są dzisiaj dla uczestników równie naturalne, co jeszcze niedawno stacjonarne. Z tą może różnicą, że ciężej jest się obłowić w siatkę pełną gadżetów 😉Jakie będą największe zmiany względem poprzednich edycji. Już wiemy, że na pewno forma - ale na co jeszcze powinni się nastawiać dotychczasowi uczestnicy?Faktycznie, poza samą formą pojawiło się kilka istotnych zmian. Prawdopodobnie największa z nich dotyczy długości trwania samego wydarzenia, a także jego częstotliwości. Dotychczas, jeszcze w tradycyjnej formie stacjonarnego meetupu, składało się na niego kilka prelekcji - około trzech, czterech, a sam meetup odbywał się raz na pół roku. Nowa forma pozwoliła nam na bardziej elastyczne podejście do tematu - webinar będzie obejmował tylko jedną prelekcję, a same webinary odbywać się będą w odstępie około miesiąca. Czujemy, że taka forma będzie wygodniejsza dla uczestników - prościej znaleźć godzinę w ciągu dnia na wysłuchanie pojedynczej prelekcji, niż wygospodarować sporą część wieczoru. Oczywiście takie założenie jest prawdziwe jedynie dla zdalnej formy wydarzenia - w przypadku wydarzeń stacjonarnych szkoda zachodu stać w korkach dla pojedynczej prelekcji.Jedną z większych wartości dotychczasowych edycji była jej praktyczność oraz to, że uczestnicy często wdawali się w dyskusję z prelegentami odnośnie treści wystąpień. Czy nie boicie się zmiana formy na webinarową sprawi, że trudniej będzie utrzymać ten poziom?Myślę, że nie. Ostatnie pół roku dobitnie pokazało, że komunikując się zdalnie można osiągnąć bardzo wiele, a webinary i ich praktyczność nie są tutaj wyjątkiem. Co więcej, taka forma otwiera przed nami możliwości, których wcześniej nie mieliśmy - dzięki wykorzystaniu lightboarda zyskujemy dodatkowe medium prezentacji informacji oraz interakcji z uczestnikami. Odnośnie dyskusji z prelegentem - już w trakcie pierwszych edycji istniała możliwość zadawania pytań za pośrednictwem internetu, także taka forma nie jest dla nas nowością, i wierzymy, że sprawdzi się i tym razem.Oczywiście stacjonarne meetupy mają swoje niezaprzeczalne zalety - inne nawet niż darmowa pizza 😉 Zdalnie ciężko jest zadbać choćby o networking, i to faktycznie jest cena, którą ponosimy za zmianę formy organizacji eventu.Pomówmy trochę o tematyce Consdata Tech. Czy poszczególne webinary będą ze sobą tematycznie powiązane?Dotychczas każda z edycji oscylowała wokół jednego konkretnego tematu - Event Sourcing, Mikrofrontendy, OpenShift. Dzieki temu osoby zainteresowane konkretnym tematem, np. mikrofrontendami, otrzymywały skondensowaną dawkę wiedzy na ten temat, nierzadko z różnych punktów widzenia. W nowym podejściu zdecydowaliśmy się od tego odejść - przede wszystkim dlatego, że webinary będą organizowane pojedynczo, wiązanie ich tematycznie byłoby sztuczne. Dzięki temu jednak zyskaliśmy swobodę, która pozwala nam poruszać zupełnie różne tematy w ramach poszczególnych spotkań. Na pierwszym webinerze Marek Pawełczyk opowie o reaktywnym programowaniu biorąc na warsztat framework Spring Webflux. Nie oznacza to jednak, że kolejne również będą dotyczyły tej tematyki. Myślę, że mogę uchylić rąbka tajemnicy i zdradzić, że kolejny webinar będzie dotyczył między innymi Domain Driven Designu.Na koniec jeszcze jedno, bardzo ogólne pytanie- załóżmy że ktoś dotychczas nie uczestniczył w Consdata Tech - dlaczego miałby dołączyć?Oczywiście z uwagi na ciekawe prelekcje! Dużą wagę przykładamy nie tylko do tego, aby prezentacje dotyczyły aktualnych trendów i technologii w IT, ale też do interesującej formy. Ostatecznie każdy może zajrzeć do dokumentacji, ale nie o to przecież chodzi. Zależy nam na tym, aby nawet pozornie zawiłe kwestie przedstawić w zrozumiały sposób - tak, aby każdy z uczestników poczuł, że było warto 🙂Dziękuję za rozmowę.Rozmawiał Adam Pijanowski - Specjalista ds. EB &amp;amp; KomunikacjiDołącz do wydarzenia poprzez Facebook lub MeetUp.",
"url": "/2020/11/04/nowa-forma-meetupu.html",
"author": "Marcin Mergo",
"authorUrl": "/authors/mmergo.html",
"image": "mmergo.webp",
"highlight": "/assets/img/posts/2020-11-04-nowa-forma-meetupu/consdata_tech_webinar.jpeg",
"date": "04-11-2020",
"path": "pl/2020-11-04-nowa-forma-meetupu"
}
,


"2020-11-03-proces-migracji-do-chmury-html": {
"title": "Migracja do chmury - czyli od czego zacząć?",
"lang": "pl",
"tags": "cloud serverless gcp aws azure googlecloud",
"content": "Planując migrację systemu informatycznego do chmury, zastanawiamy się od czego zacząć? Proces migracji składa się z kilku etapów, które pomogą nam przejść z punktu A (systemu źródłowego), do punktu B (systemu docelowego).Przed rozpoczęciem lektury warto zapoznać się z wpisem Grzegorza pt.“W chmurze czyli jak? O możliwych kierunkach rozwoju aplikacji chmurowych”który przedstawia poszczególne modele wdrożenia w chmurze.Swoją wiedzę opierać będę na doświadczeniach z chmurą Google (Google Cloud Platform, GCP), dlatego w tekście będę nawiązywał do usług i pojęć związanych z tą platformą.SzacowaniePierwszym z etapów migracji do chmury jest etap szacowania, na którym przeprowadzona zostaje dokładna analiza istniejącego systemu oraz infrastruktury.Podczas szacowania analizujemy wykorzystywane zasoby, zależności pomiędzy elementami systemu oraz wymagania.Wybieramy te elementy systemu, które chcemy przenieść do chmury, co pozwoli na obliczenie dokładnych kosztów utrzymania infrastruktury.W przypadku kiedy przenosimy maszyny wirtualne, szacujemy zasoby, które są nam potrzebne, aby określić np. typ maszyn Compute Engine oraz ich parametry.Jeśli znamy już dokładnie system źródłowy i jego wymagania, musimy zapoznać się z platformą usługodawcy, do którego będziemy migrować nasz system. Próg wejścia będzie tutaj zależał od modelu wdrożenia, ponieważ w zależności od niego musimy poznać mniej lub bardziej całą platformę.Pierwszy etap migracji jest tym miejscem, w którym zapoznajemy się z możliwościami chmury oraz zdobywamy potrzebne do migracji kompetencje (na przykład poprzez szkolenia czy certyfikacje).Pomóc może nam w tym również przetestowanie platformy, wdrożenie PoC oraz eksperymenty, które pozwolą rozwiać wszelkie wątpliwości i przewidzieć potencjalne problemy.Rozważyć należy między innymi:  porównanie wydajności i możliwości aktualnej bazy danych z usługami bazodanowymi w chmurze (np. Cloud SQL, Cloud Firestore, Cloud Spanner),  przetestowanie opóźnień sieciowych,  zastąpienie mechanizmów logowania oraz monitorowania aplikacji przez dedykowane usługi (np. Operations/Stackdriver),  przetestowanie wydajności funkcji (np. Cloud Functions) oraz ich problemu z tzw. zimnym startem (cold start), który może zdyskwalifikować wykorzystanie ich w systemie docelowym,  porównanie czasów budowania, wdrażania i uruchamiania aplikacji (on-premises vs infrastruktura w chmurze),  koszty migracji i utrzymania systemu docelowego w zależności od zastosowanego modelu (IaaS, PaaS, CaaS, FaaS).Powinniśmy również ustalić kolejność migrowanych elementów, zaczynając od mniej złożonych aplikacji, co pozwoli nabrać doświadczenia i ograniczyć ryzyko. Zdobytą wiedzę wykorzystamy później podczas migracji bardziej złożonych systemów. Dzięki temu programiści będą mogli uruchamiać i testować pierwsze aplikacje w chmurze, skupiając się na samej migracji, a nie złożoności systemu.Aplikacje będące dobrymi kandydatami:  nie są krytyczne pod względem biznesowym, co zmniejsza ryzyko migracji,  mogą być przydatne do zbudowania bazy wiedzy,  są wspierane przez zespół developerski, który jest zmotywowany do zdobycia kompetencji w pracy z chmurą,  są bezstanowe (stateless), czyli nie przechowują żadnych informacji pomiędzy interakcjami (np. sesji użytkownika) i mają małą liczbę zależności(takie aplikacje są łatwiejsze do przeniesienia i nie pociągają za sobą potrzeby przeniesienia innych elementów systemu),  wymagają minimalnych zmian w kodzie i konfiguracji,  nie wymagają przenoszenia dużych ilości danych,  nie wymagają licencji innych firm, ponieważ niektórzy dostawcy nie licencjonują swoich produktów w chmurze lub mogą wymagać zmiany typu licencji,  nie są wrażliwe na przestój spowodowany migracją, co pozwala na łatwiejsze zaplanowanie migracji danych z bazy.Przydatne mogą okazać się narzędzia takie jak Google Cloud Pricing Calculator (który pomoże nam obliczyć koszt utrzymania infrastruktury w chmurze Google) czy CloudPhysics (które pozwoli nam zarządzać własną infrastrukturą, porównać koszty utrzymania infrastruktury on-premises vs chmura oraz wybrać te elementy, których migracja do chmury przyniesie najwięcej korzyści).Efektem etapu szacowania jest szczegółowa analiza systemu źródłowego, określone wymagania i wykorzystywane zasoby oraz posiadane licencje na oprogramowanie.W wyniku takiej analizy może okazać się, że niektóre elementy nie mogą zostać przeniesione do chmury.Powody mogą być różne:  wymagania co do lokalizacji zapisanych danych,  specyficzne systemy operacyjne,  oprogramowanie którego nie oferuje chmura,  obostrzenia wynikające z licencji,  zbyt duże ryzyko lub brak opłacalności migracji.PlanowanieDrugim z etapów migracji do chmury jest etap planowania, na którym powstaje podstawowa infrastruktura w chmurze oraz planowane jest przeniesienie systemu.Planowanie obejmuje takie elementy jak zarządzanie tożsamością, struktura i organizacja projektu w GCP, infrastruktura sieci oraz komunikacja pomiędzy elementami systemu.Wybrana zostaje również strategia migracji, co zostało opisane w dalszej części.Etap ten jest ściśle związany z konkretną platformą, ponieważ od niej zależą czynności które należy wykonać.W przypadku Google Cloud Platform musimy:  skonfigurować użytkowników i konta systemowe (Google Accounts, Service Accounts, Google Groups, G Suite domains, Cloud Identity domains),  zaprojektować strukturę zasobów (organizacje, foldery, projekty),  zdefiniować grupy i role posiadające uprawnienia do tych zasobów (minimum to: organization admin, network admin, security admin, billing admin),  zaprojektować topologię sieci i skonfigurować połączenie istniejącego środowiska z chmurą (minimum to przynajmniej jedna sieć VPC).Do połączenia własnej infrastruktury z chmurą można wykorzystać połączenia hybrydowe, takie jak: Internet publiczny, Cloud VPN, Peering (Direct/Carrier), Cloud Interconnect (Dedicated/Partner).Efektem etapu planowania jest gotowy plan migracji oraz architektura systemu w chmurze.Plan migracji uwzględnia elementy, które będą przenoszone do chmury, zasoby, które będą wykorzystywane, szacowane koszty utrzymania systemu docelowego oraz listę rzeczy niezbędnych do wykonania przed rozpoczęciem migracji.Strategie migracjiPodczas etapu planowania wybieramy strategię migracji, która wpływa na to jak wyglądać będzie system docelowy, w jakim modelu zostanie on wdrożony, ile będzie kosztowała sama migracja oraz jakie będą płynąć z niej korzyści.Na wybór strategii wpływa wiele czynników:  Infrastruktura źródłowa - czy jest to on-premises, prywatny hosting czy inna publiczna chmura?  Złożoność systemu - czy można go łatwo przepisać na natywne rozwiązania chmurowe lub zastąpić usługami SaaS?  Budżet - czy proces migracji można wydłużyć w czasie, obniżając dzięki temu koszty z nim związane?  Czas - czy migracja musi odbyć się w określonym czasie, np. w celu zwolnienia zasobów, czy może być wykonywana w dłuższej perspektywie czasu?  Kompetencje - czy dobrze znana jest platforma docelowa, czy można poświęcić czas na zdobycie nowych kompetencji?Strategia “Lift and shift”Lift and shift - czyli “podnieś i przesuń”, jest strategią, w której system przenoszony jest do chmury z minimalną liczbą modyfikacji.Ewentualne zmiany są związane tylko z przystosowaniem systemu do infrastruktury docelowej.Jest to najszybszy sposób na migrację, ponieważ ilość zmian jest ograniczona do minimum.Strategię “lift and shift” wybiera się wtedy, kiedy system ma zostać przeniesiony do chmury w jak najkrótszym czasie.Również ograniczona możliwość modyfikacji aplikacji (lub jej brak) oraz koszty z tym związane mogą być powodem wyboru te strategii.Zalety:  szybkość procesu migracji i niskie koszty z tym związane,  niewielkie ryzyko i niski próg wejścia (programiści wykorzystują znane im rozwiązania, np. migrując maszyny wirtualne do Compute Engine czy kontenery z orkiestracji Kubernetes/OpenShift do Google Kubernetes Engine (GKE)),  możliwość automatyzacji (np. wykorzystując narzędzie Migrate for Compute Engine).Wady:  brak wykorzystania natywnych rozwiązań chmury (np. skalowania horyzontalnego, usług SaaS),  droższa i trudniejsza w utrzymaniu infrastruktura docelowa,  duża odpowiedzialność za system docelowy po naszej stronie.Strategia “Improve and move”Improve and move - czyli “ulepsz i przenieś”, to strategia, podczas której dokonywane są modyfikacje unowocześniające system o rozwiązania chmurowe.Modyfikacje te są wykonywane w celu wykorzystywania natywnych możliwości chmury, a nie tylko przystosowania systemu do nowej infrastruktury.Pozwalają one ulepszyć system pod kątem wydajności, funkcjonalności, kosztów utrzymania oraz wrażenia na użytkowniku końcowym.Strategię “improve and move” wybiera się wtedy, kiedy architektura systemu umożliwia łatwe i szybkie przepisanie lub zastąpienie pewnych elementów usługami SaaS,ale nie możemy z jakiegoś powodu przepisać całego systemu, w celu wykorzystania wszystkich rozwiązań chmurowych.Niezbędne do tego celu jest lepsze poznanie możliwości oferowanych przez chmurę (wyższy próg wejścia) oraz poświęcenie większej ilości czasuna przystosowanie systemu do nowej infrastruktury.Zalety:  wykorzystanie niektórych natywnych rozwiązań chmury, które wpływają bezpośrednio na dostępność systemu, jego bezpieczeństwo oraz wydajność,  niższe koszty utrzymania infrastruktury docelowej niż w przypadku strategii “lift and shift”,  przeniesienie części odpowiedzialności za system docelowy na operatora chmury,  podczas modyfikacji można dodatkowo zastosować rozwiązania, które ułatwią w przyszłości migrację do innych platform.Wady:  dłuższy czas migracji niż w przypadku strategii “lift and shift” oraz wyższe koszty z tym związane,  nadal brak wykorzystywania pełni możliwości chmury.Strategia “Rip and replace”Rip and replace - czyli “zniszcz i zastąp”, to strategia, w której wykorzystujemy w pełni natywne rozwiązania chmury, modyfikując lub całkowicie zastępując elementy systemu źródłowego.Modyfikacje mogą polegać na przepisaniu aplikacji np. na funkcje lub na zastąpieniu pewnych elementów usługami SaaS (np. bazy danych czy kolejki).Strategię “rip and replace” wybiera się wtedy, kiedy jakiś element systemu źródłowego nie spełnia oczekiwań lub wymagań i może zostać całkowicie przepisany lub zastąpiony usługą SaaS.Taka zmiana pozwoli wykorzystać natywne rozwiązania chmury, które zapewnią skalowanie horyzontalne, wysoką wydajność i dostępność systemu, większe bezpieczeństwo, optymalizację kosztów utrzymania oraz minimalną odpowiedzialność po stronie użytkownika (patrz wykres odpowiedzialności we wpisie Grzegorza).Zalety:  wykorzystujemy w pełni natywne rozwiązania oferowane przez chmurę, które pozytywnie wpływają na wydajność, dostępność i bezpieczeństwo systemu,  przepisując lub zastępując elementy systemu eliminujemy dług techniczny,  usługi SaaS zapewniają aktualizacje przeźroczyste dla działania systemu,  rozwiązania w modelu FaaS/SaaS mogą nie generować żadnych kosztów w przypadku braku ruchu użytkowników, dzięki czemu minimalizujemy koszty utrzymania,  zmniejszamy naszą odpowiedzialność za system docelowy (konfiguracja, zarządzanie, skalowanie, klastrowanie, bezpieczeństwo i wiele więcej), dzięki czemu stajemy się mniej DevOps, a bardziej NoOps.Wady:  bardzo wysoki próg wejścia, ponieważ musimy dokładnie poznać wszystkie usługi i rozwiązania, jakie oferuje dana platforma,  wysokie ryzyko migracji, ponieważ natywne rozwiązania chmury mogą mieć swoje ograniczenia (np. konkretne API),  wysoki koszt i czasochłonność migracji,  uzależnianie się od operatora chmury (vendor lock-in).WdrażanieTrzecim z etapów migracji do chmury jest etap wdrażania, na którym projektujemy, implementuje oraz wdrażamy system docelowy w chmurze.Część elementów systemu może zostać przeniesiona bez większych zmian, inne zaś wymagać mogą gruntownej refaktoryzacji lub zastąpienia innym rozwiązaniem (np. usługami SaaS).Niezbędne są również takie elementy jak CI/CD, czyli proces budowania i wdrażania aplikacji (wykorzystujący na przykład Terraform oraz usługę Cloud Build). Konieczne może okazać się również dopracowanie infrastruktury w chmurze, aby sprostać wymaganiom systemu.Do wyboru mamy następujące procesy wdrażania:  Wdrażanie ręczne - pozwala na szybkie testowanie i eksperymentowanie z chmurą, ale jest podatne na błędy, często niepowtarzalne.Zasoby można tworzyć ręcznie z poziomu Google Cloud Console, a polecenia mogą być wprowadzane z poziomu terminala lokalnego lub Cloud Shell.  Narzędzia do wdrażania - takie jak Ansible, Chef, Puppet czy SaltStack umożliwiają konfigurowanie środowiska w sposób zautomatyzowany, powtarzalny oraz kontrolowany.Narzędzia te nie umożliwiają jednak wdrażania bezprzerwowego lub typu Blue-Green (niektóre umożliwiają implementację własnej logiki wdrażania, ale jest to dodatkowym obciążeniem).  Orkiestracja kontenerów - jeżeli aplikacje zostaną skonteneryzowane, można skorzystać z usługi Google Kubernetes Engine (GKE).Kubernetes nie tylko zarządza, automatyzuje i skaluje aplikacje kontenerowe, ale oferuje również wiele metod bezprzerwowego wdrażania.  Automatyzacja wdrażania - wdrażając proces CI/CD, można zautomatyzować budowanie i wdrażanie aplikacji.  Infrastruktura jako kod (IaC) - rozwiązanie to pozwala w sposób deklaratywny tworzyć zasoby i wdrażać aplikacje.Umożliwia to również utworzenie testów do kodu tworzącego infrastrukturę.Aby zaimplementować infrastrukturę jako kod, można wykorzystać Cloud Deployment Manager lub jego alternatywę w postaci Terraform,który umożliwi wdrożenie nie tylko w chmurze Google.W przypadku migracji maszyn wirtualnych (VMware vSphere, Amazon AWS, Microsoft Azure) do GCP, możemy skorzystać z narzędzia Migrate for Compute Engine, które zautomatyzuje ten proces.Efektem etapu wdrażania jest gotowy i działający system w architekturze chmurowej. Skonfigurowane są takie elementy jak sieć, uprawnienia, skalowanie czy monitoring aplikacji.OptymalizacjaOstatnim i czwartym z etapów migracji do chmury jest etap optymalizacji, na którym monitoruje się działający system w celu optymalizacji jego działania.Podczas optymalizacji wykonujemy testy wydajnościowe, które możemy porównać z testami systemu źródłowego oraz konfigurujemy wykorzystywane usługi i zasoby tak, aby system działał stabilnie, bezawaryjnie oraz wydajnie.Optymalizacja aplikacji w chmurze może być również efektem zmian usług platformy chmurowej - na przykład nowym API, nowymi funkcjami lub nowymi usługami.Również nabieranie doświadczenia i kompetencji przez zespół programistów może być powodem późniejszych usprawnień systemu w chmurze.Z czasem coraz więcej elementów infrastruktury zostaje zautomatyzowanych, co redukuje koszty utrzymania oraz czas wdrażania aplikacji.Zastępowane są również kolejne elementy infrastruktury usługami SaaS, dążąc do w pełni natywnych rozwiązań chmurowych.Działający system jest optymalizowany pod kątem wykorzystywanych zasobów (na przykład zmieniając typ maszyn Compute Engine), dostosowując je do aktualnych wymagań, a także skonfigurowanezostaje automatyczne skalowanie.Przydatne mogą okazać się narzędzia z grupy Operations/Stackdriver, a także narzędzia i usługi przydatne w automatyzacji infrastruktury i procesu CI/CD (np. Terraform i Cloud Build).Proces optymalizacji nie ma swojego końca, ponieważ działający system może wymagać okresowych zmian w konfiguracji, aby sprostać nowym wymaganiom i obciążeniu generowanemu przezużytkowników. Powinniśmy stale analizować miesięczne koszty utrzymania, trendy i produkty, które są wykorzystywane najczęściej.Dzięki temu można zmniejszać koszty, na przykład podpisując umowy na korzystanie z Compute Engine (committed use discounts), czy zmieniając model płatności za usługę BigQuery na flat-rate.Migracja do chmury - podsumowanieNa koniec chciałbym zaznaczyć, że opisane tutaj strategie migracji mogą być różnie opisywane w literaturze.Możemy spotkać się z artykułami opisującymi 4 czy 6 strategii, np.“6 Strategies for Migrating Applications to the Cloud”Opisywane strategie pokrywają się mniej lub bardziej, a rozbieżności są spowodowane innym punktem widzenia (niekoniecznie programisty).Najważniejsze w migracji jest jednak to jak ją zaczniemy. Jeśli zabierzemy się do tego bez podstawowej wiedzy na temat platformy, z której chcemy skorzystać, to odbije się to na wydłużonym czasie migracji oraz niekoniecznie najlepszej architekturze systemu docelowego.Warto przed przystąpieniem do planowania migracji zdobyć odpowiednie kompetencje, a na pytanie czy jesteśmy na to gotowi może nam odpowiedzieć np. Google Cloud Adoption Framework.",
"url": "/2020/11/03/proces-migracji-do-chmury.html",
"author": "Michał Hoja",
"authorUrl": "/authors/mhoja.html",
"image": "mhoja.jpg",
"highlight": "/assets/img/posts/2020-11-03-proces-migracji-do-chmury/clouds.jpeg",
"date": "03-11-2020",
"path": "pl/2020-11-03-proces-migracji-do-chmury"
}
,


"2020-10-21-modele-wdrozenia-w-chmurze-html": {
"title": "W chmurze czyli jak? O możliwych kierunkach rozwoju aplikacji chmurowych",
"lang": "pl",
"tags": "cloud iaas caas paas faas gcp aws azure",
"content": "Co to znaczy być w chmurze? Co to właściwie jest IaaS, PaaS, CaaS, FaaS? Jak przenieść się do chmury? Czy jestem już wystarczająco zachmurzony? Jeżeli nie znasz odpowiedzi na którekolwiek z powyższych pytań, to ten wpis jest stworzony specjalnie dla Ciebie!Czym właściwie jest chmura? W uproszczeniu możemy powiedzieć, że to dowolna usługa (lub zestaw usług), która dostarcza mechanizmy automatycznego tworzenia zasobów na żądanie i rozliczania ich zgodnie z faktycznym ich wykorzystaniem (The NIST Definition of Cloud Computing). Sama definicja jest rozległa i możemy pod nią umieścić wiele skrajnie różnych rozwiązań. W kolejnych akapitach przyjrzymy się krótkiej charakterystyce podejść oferowanych przez wiodących dostawców chmur publicznych.On-premises - własna infrastrukturaZanim omówimy podejścia do migracji w chmurze, określmy zgrubnie punkt wyjścia, do którego będziemy odnosić kolejne modele wdrożeniowe:Powyższy diagram przedstawia uproszczony model elementów, za które jesteś odpowiedzialny wdrażając system we własnej serwerowni.IaaS - Infrastructure as a serviceDostawca chmury przejmuje odpowiedzialność za fizyczny sprzęt potrzebny do uruchomienia infrastruktury. Użytkownik zarządza systemem na poziomie konkretnych maszyn wirtualnych.W najbardziej oczywistym aspekcie mówimy o zakupie serwerów, ale należy uwzględnić też fizyczną ochronę serwerowni, serwis i utrzymanie sprzętu, budowę sieci, koszty pomieszczeń, opłat bieżących i licencji. Do tego dochodzi koszt zatrudnienia i szkolenia administratorów o odpowiednich kompetencjach, koszt wypracowania standardów i konfiguracji, kończąc na takich kwestiach, jak procedury migracji, disaster recovery czy tworzenie strategii rozwoju uwzględniających zakup sprzętu potencjalnie potrzebnego w przyszłości.W uproszczeniu, IaaS zdejmuje z Ciebie takie zmartwienia, jak umierające dyski SSD czy telefon w środku nocy (bo  np. zabrakło prądu lub włączył się alarm przeciwpożarowy) oraz ograniczenia sprzętowe przy eksperymentowaniu czy planowaniu nowych produktów.W praktyce oznacza to, że na żądanie możesz tworzyć dowolnie skonfigurowane instancje maszyn wirtualnych spiętych w wirtualne sieci o dowolnej topologii. Dla maszyn, poza technicznymi parametrami, możesz określać dostępność w sieci czy szczegółowe uprawnienia dostępu.Migrując w modelu IaaS zyskujesz:  fizyczne zabezpieczenie serwerowni,  bezpieczeństwo i ciągłość działania serwerów,  bezpieczny storage,  stabilność i bezpieczeństwo sieci,  audytowalność zmian i monitoring infrastruktury,  skalowanie liczby i rozmiaru maszyn pod bieżące potrzeby.Nadal jednak musisz zajmować się bezpieczeństwem, aktualizacjami i konfiguracją systemów operacyjnych wdrożonych maszyn wirtualnych.W przypadku migracji systemu do modelu IaaS mówimy o podejściu rehosting i jest to najprostszy model wdrożenia do chmury. Zakłada jedynie przerzucenie (czy nawet import) systemów z aplikacjami do infrastruktury dostawcy. Koszt migracji jest minimalny, a dla legacy systemów drastycznie niższy niż w pozostałych modelach wdrożenia. Pomimo pierwszych zalet, w modelu IaaS trudno mówić o wdrożeniu chmurowym. Nadal nie wykorzystujesz w pełni możliwości niskokosztowej i skalowalnej infrastruktury.Model IaaS może być prostym pierwszym krokiem, który pozwoli szybko wskoczyć do publicznej chmury i otworzyć możliwości szerszej integracji z systemami dostawcy i przyrostowych zmian we wdrażanym systemie (ewolucja przez refaktoring, podmiana pojedynczych elementów czy integracja z konkretnymi usługami zamiast długotrwałego przepisywania całego systemu od podstaw).CaaS - Container as a serviceDostawca chmury przejmuje odpowiedzialność za utrzymanie maszyn wirtualnych obsługujących infrastrukturę. Użytkownik zarządza systemem na poziomie kontenerów aplikacji.Najpopularniejszym rozwiązaniem w tej kategorii jest klaster Kubernetes w pełni zarządzany przez dostawcę chmury. W praktyce zarządzanie maszynami wirtualnymi zostaje ograniczone do zdefiniowania ich rozmiaru i liczby, a czasem wręcz do określenia limitów minimalnej i maksymalnej liczby węzłów. Aktualizację, konfigurację, zabezpieczenie czy  monitorowanie systemów maszyn wirtualnych oddajesz w ręce specjalistów. Dzięki temu możesz skupić się na tworzeniu niezawodnego oprogramowania.Migrując w modelu CaaS zyskujesz:  aktualizacje i konfiguracje systemów maszyn wirtualnych,  bezpieczeństwo zainstalowanych na maszynach systemów operacyjnych,  aktualizacje, konfiguracje i bezpieczeństwo klastra dostarczanego przez dostawcę,  równoważenie obciążenia hostów klastra i optymalizację kosztów działania systemu,  możliwości skalowania i niezawodność systemu wynikające z łatwego powielania instancji aplikacji i węzłów klastra.Dzięki abstrakcji na infrastrukturę, którą zapewnia konteneryzacja aplikacji możesz skorzystać z gotowych usług dostarczanych przez dostawcę chmury. W zależności od operatora dostępne będą np. load balancer z firewallem i ochroną przed DDoS, bezpieczny storage w postaci persistent volumes czy centralne zarządzanie logami lub metrykami działania systemu. Dodatkowo, takie mechanizmy, jak kontrola uprawnień czy skalowanie systemu, są już w standardzie.Jednym z najważniejszych ograniczeń w przypadku CaaS jest konieczność pełnej obsługi stosu aplikacyjnego. Przykładowo, udostępniając usługi HTTP musisz zadbać o uruchomienie, konfigurację i bezpieczeństwo serwera WWW, kontrolę dostępu do usługi czy poprawną konfigurację liczby wątków i połączenia do bazy danych, a nawet definicję odpowiednich portów serwera i przygotowanie certyfikatów SSL. Migrowane aplikacje często będą wymagały zmiany w konfiguracji, czy nawet kodzie źródłowym, w celu dostosowania do infrastruktury i dostępnych usług. Bez tych zmian nie będzie możliwe pełne wykorzystanie potencjału wdrożenia w chmurze.Dodatkowo należy pamiętać, że po stronie użytkownika pozostaje zapewnienie bezpieczeństwa i stabilności dostarczanego kontenera. Przykładowo: przeprowadzanie analizy kontenerów pod kątem znanych podatności, aktualizowanie oprogramowania i obrazów bazowych oraz rozwiązywanie problemów wynikających z błędnie zdefiniowanych obrazów i deskryptorów wdrożenia.W przypadku modelu CaaS możemy już mówić o strategii migracji typu refactoring. Nawet jeżeli unikniesz zmian funkcjonalnych w kodzie, będziesz modyfikował sposób wdrażania aplikacji. W ramach migracji należy przeprowadzić konteneryzację przenoszonych aplikacji i przygotować deskryptory wdrożenia. Koszt migracji jest większy niż w przypadku modelu IaaS, ale możliwe jest przygotowanie się do migracji w ramach istniejących wdrożeń przed rozpoczęciem faktycznych migracji w chmury. Należy oczywiście zauważyć, że systemy już wdrażane w kontenerach lub klastrach Kubernetes dostają możliwość migracji do chmury przy minimalnych kosztach zmian.Model CaaS pozwala w praktyce wdrożyć wiele rozwiązań kojarzonych z publicznymi chmurami obliczeniowymi - skalowanie, niezawodność, szybkość zmian. Dla wielu będzie to docelowy model wdrożenia w chmurze, optymalnie łączący koszt i zakres zmian z natychmiastowymi zyskami i możliwościami dalszego rozwoju. Ewolucyjna natura podejścia, możliwość migracji do innego dostawcy czy nawet powrót do wdrożeń on-premises będą przemawiać za modelem CaaS.PaaS - Platform as a serviceDostawca chmury przejmuje odpowiedzialność za wdrożenie i utrzymanie aplikacji. Użytkownik zarządza systemem na poziomie pojedynczych aplikacji.W modelu PaaS to dostawca chmury odpowiedzialny jest za budowanie i osadzanie aplikacji oraz utrzymanie kompletnej infrastruktury. Platforma dostarcza rozszerzenia zapewniające prostą konfigurację i integrację tworzonych aplikacji z usługami dostępnymi w chmurze. Osadzana usługa będzie miała zapewnione wsparcie dla monitorowania, uwierzytelniania czy komunikacji HTTPS na poziomie platformy (bez zmian w samej osadzanej aplikacji).Aplikacje są automatycznie zarządzane przez platformę. Wersjonowanie, load balancing i automatyczne skalowanie to tylko niektóre z dostępnych funkcjonalności. Jako użytkownik nie musisz zarządzać procesem budowania aplikacji. W przypadku platformy PaaS możemy myśleć jedynie o aplikacjach, a wszystkie aspekty ich wdrożenia i klastrowania są zarządzane przez systemy chmury.Migrując w modelu PaaS zyskujesz:  aktualizację i bezpieczeństwo kontenerów uruchamiających tworzone aplikacje,  w pełni zarządzaną infrastrukturę  wspierającą wersjonowanie i skalowanie na żądanie,  koszty wynikające wprost z czasu działania systemu,  wdrożenia gotowe do obsługi produkcyjnego ruchu - bez konieczności znajomości zagadnień DevOps,  brak kosztów związanych z zarządzaniem infrastrukturą.Właściwie wszystkie elementy wdrożenia chmurowego są już ukryte przed użytkownikiem. W praktyce oznacza to, że oddając coraz więcej elastyczności tworzonej aplikacji obniżyłeś do minimum konieczność zarządzania systemem. Koncepty, którymi nadal zajmuje się użytkownik, wynikają już ze stosowanych narzędzi i frameworków oraz natury tworzonej aplikacji.Platforma PaaS najczęściej określa języki i frameworki, dla których dostarcza wsparcie w postaci automatycznego procesu budowania i osadzania, narzędzi i bibliotek wspierających tworzenie usług oraz automatycznej konfiguracji frameworków.W przypadku migracji systemu w modelu PaaS możemy wpaść w jedną z dwóch ścieżek: dla aplikacji tworzonych we wspieranym przez chmurę języku/frameworku możliwa będzie refaktoryzacja, jednak dla pozostałych aplikacji konieczne może być ich przepisanie.Model sprawdzi się dobrze w aplikacjach webowych i serwerach usług HTTP - wszędzie tam, gdzie przejście na model FaaS nie jest możliwe ze względu na jego ograniczenia.FaaS - Function as a serviceDostawca chmury przejmuje odpowiedzialność za wdrożenie i utrzymanie pojedynczych funkcji. Użytkownik zarządza systemem na poziomie kodu źródłowego konkretnych funkcji.W modelu FaaS dostawca chmury odpowiedzialny jest za wszystkie aspekty uruchomienia funkcji biznesowej w chmurze. Platforma dostarcza nie tylko infrastrukturę systemu, ale też obsługuje np. elementy stosu HTTP. Model funkcyjny jest modelem zdarzeniowym - użytkownik skupia się na obsłudze zdarzeń w systemie, np. zdarzeniem żądania HTTP, pojawieniem się komunikatu w kolejce czy utworzeniem nowego konta użytkownika lub pliku w storage.Przykładowo, dla funkcji HTTP, abstrahowane są nie tylko sposób uruchomienia, obsługa wątków, SSL czy port, na którym uruchomiona jest usługa, ale też adres publikacji endpointu HTTP, obsługiwane metody jego wywołania czy parsowanie żądania i odpowiedzi. Z punktu widzenia użytkownika wywołanie funkcji HTTP niewiele różni się od wywołania jej przez inny proces. Nawet konkretny serwer obsługujący żądania HTTP jest ukryty przed użytkownikiem i może ulec zmianie na poziomie infrastruktury dostawcy.Każda osadzona funkcja jest:  niezależnie budowana i wersjonowana (zarówno jej kod, jak i deployment),  wykonywana z niezależnym zestawem zależności (bibliotek),  obsługiwana niezależnie przez load balancer,  niezależnie skalowana,  zabezpieczona dedykowanymi dla niej regułami uwierzytelniania i autoryzacji.Podsumowując, każda funkcja jest traktowana jak niezależny artefakt i należy ją postrzegać jako samodzielną aplikację.Migrując w modelu FaaS zyskujesz:  zerowy koszt utrzymania i zarządzania infrastrukturą,  ograniczenie tworzenia kodu niepowiązanego bezpośrednio z realizowaną funkcjonalnością,  skalowanie od zera do dowolnego obciążenia,  błyskawiczne tempo wprowadzania nowych zmian i eksperymentów.Model FaaS zakłada analizę i fundamentalne przeprojektowanie systemu. Zmiana myślenia o architekturze i przetwarzaniu sprawia, że dotychczasowe problemy będziesz mógł rozwiązać na nowe sposoby. Użytkownicy mogą w całości skupić się na tworzeniu funkcjonalności biznesowych, drastycznie skracając czas potrzebny do wdrażania nowych rozwiązań.Warto też zauważyć, że skalowanie systemu pozwala nie tylko obsługiwać użytkowników na środowiskach produkcyjnych, ale też skalować środowiska programistyczne i testowe. Jeżeli koszt infrastruktury wprost wynika z liczby użytkowników, to nie ma powodu tworzyć uproszczonych wersji środowisk testowych i można wewnętrznie zapewnić infrastrukturę w pełni zgodną z produkcyjnym wdrożeniem.W przypadku FaaS należy pamiętać, że z największymi możliwościami i prostotą pojawiają się największe ograniczenia (największe uzależnienie kodu od konkretnej chmury). Tworzona funkcja będzie w pełni polegać na środowisku uruchomieniowym dostawcy. Kod będzie tworzony w języku programowania i w ramach frameworków wskazanych przez usługę FaaS. Często lokalne testowanie będzie utrudnione lub będzie wymagało emulatora. Jednym z największych ograniczeń wielu popularnych chmur jest czas zimnego startu funkcji, tj. natychmiastowe skalowanie od zera do w zasadzie dowolnego obciążenia wiąże się z częstym tworzeniem nowych instancji usługi. To oznacza, że część użytkowników trafiających na nową usługę może zauważyć dłuższe czasy odpowiedzi. W praktyce problem jest zauważalny w systemach o małym ruchu lub gdy niedopuszczalne są nawet pojedyncze żądania przekraczające założony czas odpowiedzi.FaaS najlepiej sprawdza się w przetwarzaniu opartym o zdarzenia. Zastosowanie funkcji do tworzenia aplikacji webowych lub usług API należy poprzedzić analizą wymagań, ocenić np. dopuszczalne SLA usługi przy uwzględnieniu czasu zimnego startu. Możliwość skalowania usług od zera do dowolnego obciążenia oraz ograniczenie czasu tworzenia nowych funkcjonalności do minimum sprawia, że FaaS to zawsze opcja warta rozważenia.Niskie koszty utrzymania infrastruktury, wysokie tempo wprowadzania zmian i wzorowe możliwości skalowania pozwolą Twojemu produktowi zyskać przewagę nad konkurencją.Migracja a rozwójPrzytaczane do tej pory koszty związane z migracją zakładały dostosowanie i wdrożenie działających systemów. Przy ich ocenie zakładałem, że zespół nie posiada wiedzy i doświadczenia w tworzeniu aplikacji w chmurze. W takiej sytuacji koszt szkoleń, prób i błędów oraz faktycznego wdrożenia rośnie wraz ze wzrostem “chmurowości” rozwiązania.Sytuacja będzie wyglądać zupełnie inaczej dla zespołu tworzącego i utrzymującego już istniejące rozwiązania chmurowe. W takim wypadku koszt nowej technologii został już poniesiony i nie wpływa na koszt przygotowywania nowych rozwiązań. Dla doświadczonych zespołów koszt dostarczenia nowej funkcjonalności w środowisku chmurowym będzie wyraźnie niższy niż w tradycyjnym podejściu ze względu na:  abstrakcję infrastruktury,  minimalny koszt wdrożenia i utrzymania aplikacji,  przeniesienie ciężaru na rozwój funkcjonalny aplikacji,  dostępność gotowych do integracji z systemem usług w modelu SaaSVendor lock-inPlanując migrację do chmury musisz zmierzyć się z tematem uzależnienia kodu od konkretnego dostawcy.Czy vendor lock-in jest zły? Zagadnienie warto traktować jako ryzyko, a nie zagrożenie jako takie. Dla ryzyk możliwe jest zaplanowanie ścieżek przeciwdziałania i przygotowanie planu wyjścia z chmury (czy to do innego dostawcy, czy zupełnie w kierunku chmury prywatnej lub rozwiązania on-premises).Dla podejść IaaS i CaaS możemy przyjąć, że ryzyko wyjścia z chmury niesie za sobą minimalne koszty - w szczególności, gdy system nie korzysta z natywnych usług oferowanych przez chmurę obliczeniową.W przypadku PaaS i FaaS prawdopodobnie zaprojektowałeś system pod kątem oferty konkretnej chmury. Najwięksi dostawcy zapewniają ujednoliconą ofertę produktów. O ile różnice pomiędzy oferowanymi rozwiązaniami w wielu przypadkach uniemożliwiają migrację systemu bez żadnych zmian, to jednak znajdziemy pomiędzy nimi odpowiadające sobie rozwiązania pozwalające zrealizować system w podobnej architekturze.Warto zauważyć, że problem vendor lock-in, jako poważne ryzyko migracji do chmury, często jest adresowany wprost przez dostawców. Przykładowo w Google Cloud Platform większość produktów jest zbudowana w oparciu o otwarte standardy oraz narzędzia open-source. Przykładowo Google Kubernetes Engine do działania wykorzystuje platformę Kubernetes, Cloud Bigtable jest zgodny z HBase, itd. Podobne podejście jest stosowane przez wiodących dostawców chmur publicznych.Szacując koszty wyjścia z chmury powinieneś rozważyć również utracone zyski wynikające z obaw przed vendor lock-in. Obawa przed uzależnieniem może doprowadzić do zwiększenia złożoności systemu, zwiększenia kosztu produkcji i utrzymania systemu, utraty funkcjonalności dających przewagę, itp. Czy ew. koszt wyjścia z chmury będzie większy niż oszczędności wynikające z jej wykorzystania? W przypadku dużego prawdopodobieństwa lub wysokiego kosztu wyjścia z chmury warto rozważyć ograniczenie migracji do modelu CaaS.On-prem, IaaS, CaaS, PaaS, FaaSMając już w głowie argumentacje i przykłady wszystkich podejść, możemy nakreślić podsumowanie odpowiedzialności w różnych modelach.Im bardziej zaadaptujemy rozwiązania chmurowe, tym więcej odpowiedzialności przerzucimy na stronę dostawcy.Dojrzałość chmurowaZnając system możemy pokusić się o ocenę jego dojrzałości chmurowej. Z jednej strony mamy klasyczne rozwiązania on-premises, a z drugiej rozwiązania w pełni serverless. Podsumowując wady i zalety opisywanych do tej pory podejść naszkicujmy prosty diagram szans i ryzyk zależny od stopnia migracji do chmury:Im bardziej zbliżymy się do podejść serverless, tym więcej korzyści osiągniemy z migracji. Należy jednak zwrócić uwagę, że im dalej się przesuwamy, tym droższa będzie migracja oraz wzrośnie ryzyko uzależnienia się od konkretnego dostawcy.Cele migracjiPodsumowując, zanim wybierzesz odpowiedni kierunek migracji do chmury, powinieneś zidentyfikować kluczowe problemy, które starasz się rozwiązać. Na ich podstawie możesz ustalić cele i ograniczenia, które pomogą Ci wybrać odpowiednią strategię dla Twojego systemu.Rzućmy okiem na kilka przykładowych celów. Pokażę Ci, jakie decyzje mógłbyś rozważyć, stojąc przed nimi:  “Chcemy być w chmurze” - przy minimalnym nakładzie pracy możesz wskoczyć we wdrożenia w ramach IaaS i z sukcesem odhaczyć wymaganie. Jednak, o ile ten cel nie ma ukrytej intencji, samo przeniesienie do chmury dla prestiżu nie wydaje się być sensownie obraną strategia rozwoju.  “Chcemy obniżyć koszty sprzętu”- wdrożenia IaaS mogą obniżyć koszt sprzętu, jednak koszt utrzymania pojedynczej wirtualnej maszyny odpowiadającej już posiadanym maszynom może być większy.  “Chcemy obniżyć koszty administrowania” - wdrożenia CaaS zdejmują z użytkownika potrzebę administrowania systemami operacyjnymi maszyn wirtualnych. Przejście na model PaaS i FaaS pozwala dalej radykalnie ograniczać te koszty.  “Chcemy obniżyć koszty infrastruktury” - model CaaS wyraźnie zwiększa utylizację infrastruktury. Dodatkowo przejście na PaaS i FaaS pozwala nie utrzymywać infrastruktury w ogóle gdy nie jest potrzebna.  “Chcemy zwiększyć skalowalność systemu” - każdy kolejny etap migracji zwiększa standaryzację wdrażanych rozwiązań, przez co pozwala dostawcy chmury na efektywniejsze skalowanie systemu bez naszego udziału.  “Chcemy poprawić bezpieczeństwo systemu” - każdy kolejny model (CaaS, PaaS, FaaS) poprawia bezpieczeństwo systemu przenosząc odpowiedzialność za utrzymanie i zarządzanie kolejnymi elementami przez dostawcę infrastruktury.  “Chcemy obniżyć koszty wprowadzenia nowych zmian” - modele chmurowe sprzyjają szybkiemu wprowadzaniu zmian i prowadzeniu eksperymentów przy minimalnym narzucie związanym z wdrożeniem i administracją.  “Nie chcemy płacić za infrastrukturę” - cel zdefiniowany przewrotnie, jednak jego intencją jest pokrycie kosztów infrastruktury przez użytkowników - gdy nie używają systemu, to ten nie generuje kosztów utrzymania. Jak się już pewnie domyślasz, docieramy do Graala rozwiązań chmurowych. W tym miejscu łakomie zerkasz już w stronę serverless, kuszą Cię jego możliwości skalowania do zera, brak stałych kosztów utrzymania i możliwość określenia precyzyjnego kosztu obsługi każdego użytkownika w systemie :)Potraktuj przykładowe cele jako inspiracje i przygotuj własną listę oczekiwań i driverów migracji do chmury. Znając problemy i oczekiwania, skuteczniej ocenisz potencjalne zyski i sensowność wybranego podejścia.Co wybrać?To co ostatecznie wybrać? Wszystko po trochu!Sposób migracji aplikacji należy dobierać do jej potrzeb, ograniczeń i szans na rozwój. Naturalnym podejściem będzie wdrożenie części systemów wymagających minimalnych czasów odpowiedzi do użytkownika w modelu CaaS lub PaaS oraz zaprojektowanie procesów biznesowych w modelu zdarzeniowym przy wsparciu FaaS.Migracja do chmury uczy nas myślenia nie tylko w kategoriach kodu, ale też optymalizacji kosztów i szybkości wdrażania zmian. Przykładowo, jeśli tworzysz aplikację bota integrującego się z platformą Slack, możesz uruchomić webhook w oparciu o PaaS, który zleca dalsze operacje do kolejki zdarzeń obsługiwanych przez infrastrukturę FaaS. W takim podejściu usługa PaaS zapewnia nam stałe działanie usługi nieobarczonej kosztem zimnego startu, a FaaS zapewnia infrastrukturę przetwarzania operacji, za którą płacimy tylko w trakcie obsługi. W takim podejściu poniesiemy minimalny koszt zawsze czekającej usługi (co jest wymaganiem od strony UI użytkownika) i zerowy koszt funkcji działających tylko wtedy, gdy jest dla nich zadanie do wykonania. Dzięki takiej analizie możemy optymalizować koszt działania systemu przeznaczając środki w miejscach, w których niosą realną wartość (szybki czas odpowiedzi usługi do użytkownika) i obniżając je tam, gdzie możliwe są oszczędności (przetwarzanie zadań w tle).Podsumowując -  przy wyborze modelu wdrożenia w chmurze ważne jest określenie rozwiązywanych problemów i celów, które chcesz osiągnąć. Na tej podstawie będziesz mógł świadomie wybrać najlepsze narzędzia do zadanego problemu. Nie bój się eksperymentować i mieszać podejść, aby uzyskać infrastrukturę skrojoną idealnie pod Twoje potrzeby.",
"url": "/2020/10/21/modele-wdrozenia-w-chmurze.html",
"author": "Grzegorz Lipecki",
"authorUrl": "/authors/glipecki.html",
"image": "glipecki.jpg",
"highlight": "/assets/img/posts/2020-10-22-modele-wdrozenia-w-chmurze/cloud.jpeg",
"date": "21-10-2020",
"path": "pl/2020-10-22-modele-wdrozenia-w-chmurze"
}
,


"2020-08-27-axon-kompleksowe-testowanie-aplikacji-html": {
"title": "Axon - Kompleksowe testowanie aplikacji",
"lang": "pl",
"tags": "java microservices axon axon-server event-sourcing spring-boot tests",
"content": "Powszechnie wiadomo, że kod dobrze pokryty testami jest dużo bardziej podatny na rozwój - wszak nie musimy obawiać się, że nasza zmiana spowoduje np. powrót znanego wcześniej błędu.W poprzednim wpisie opisałem swoje zmagania z migracją monolitu do mikroserwisów na Axonie, umyślnie pomijając kwestię związaną z testami.Niniejszy artykuł jest poświęcony w pełni temu tematowi.Przedstawię w nim kilka elementów składających się na kompleksowo przetestowaną aplikację opartą o Axona.Testy domenoweZacznijmy od przetestowania domeny, czyli logiki biznesowej zawartej w obiektach domenowych.W Axonie jest to ułatwione, poprzez gotowe narzędzia, które dostajemy w pakiecie z frameworkiem.Zaleta tych testów jest taka, że nie podnoszą one żadnego kontekstu, a więc wykonują się bardzo szybko.AgregatyZostając przy aplikacji z poprzedniego wpisu, weźmy jako przykład oznaczanie filmu jako obejrzany/nieobejrzany.Niezmiennik agregatu mówi, że gdy film jest już oznaczony jako obejrzany, to nie możemy tego zrobić ponownie - tak samo nieobejrzanego filmu nie możemy “odzobaczyć” ;).Wystąpienie takiej anomalii odnotowywane jest w logu, a ToggleWatchedEvent nie zostaje wyemitowany.@Aggregatepublic class MovieAggregate {    ...    @CommandHandler    public void handle(ToggleWatchedCommand command) {        if (this.watched.isWatched() == command.getWatched().isWatched()) {            log.info(&quot;Cannot toggle to the same state, skipping..&quot;);            return;        }        apply(new ToggleWatchedEvent(command.getMovieId(), command.getWatched()));    }    ...}Rozważmy tzw. happy path w testach dla tego przypadku biznesowego. Będziemy potrzebować obiektu FixtureConfiguration, który zasymuluje nam sytuację, w której może znaleźć się nasz agregat.public class MovieAggregateTest {    ...    private FixtureConfiguration&amp;lt;MovieAggregate&amp;gt; fixture;    @BeforeEach    public void setup() {        this.fixture = new AggregateTestFixture&amp;lt;&amp;gt;(MovieAggregate.class);        ...    }}Test piszemy w następujący sposób:  Określamy “punkt startowy” dla naszego agregatu, czyli jakie eventy zostały zastosowane w agregacie do tej pory (jest to given w podejściu behawioralnym).  Wpisujemy command, który chcemy poddać testom (odpowiednio - when). W naszym przypadku to ToggleWatchedCommand  Definiujemy stan oczekiwany.public class MovieAggregateTest {    ...    @Test    public void shouldToggleWatchedEventAppear() {        fixture.given(                                                              // 1                    new MovieCreatedEvent(movieId, searchPhrase),                    new MovieSavedEvent(movieId, externalMovie))                .when(new ToggleWatchedCommand(movieId, new Watched(true)))         // 2                .expectEvents(new ToggleWatchedEvent(movieId, new Watched(true)))   // 3                .expectState(                                                                           state -&amp;gt; assertThat(state.getWatched().isWatched()).isTrue());    }}Druga ścieżka do sprawdzenia to brak emisji zdarzenia w momencie zmiany stanu na ten sam.Dorzućmy więc do given wystąpienie eventu ToggleWatchedEvent - wtedy agregat nie powinien wyemitować nic nowego:public class MovieAggregateTest {    ...    @Test    public void shouldNotToggleWatchedEventAppear() {        fixture.given(                    new MovieCreatedEvent(movieId, searchPhrase),                    new MovieSavedEvent(movieId, externalMovie),                    new ToggleWatchedEvent(movieId, new Watched(true)))                .when(new ToggleWatchedCommand(movieId, new Watched(true)))                .expectNoEvents()                .expectState(                                                                           state -&amp;gt; assertThat(state.getWatched().isWatched()).isTrue());    }}SagiDrugim obiektem domenowym, który poddam testom, jest saga (zob. DDD).W mojej aplikacji zdarzeniem otwierającym sagę dla filmu jest MovieCreatedEvent wyemitowany przez MovieAggrate - po jego pojawieniu się, wysyłam command, który zostanie obsłużony w mikroserwisie (proxy-service) odpowiedzialnym za pobieranie szczegółów filmu z zewnętrznego źródła:public class MovieSaga {    ...    @StartSaga    @SagaEventHandler(associationProperty = MOVIE_ID)    public void handle(MovieCreatedEvent event) {        movieId = event.getMovieId();        String proxyId = PROXY_PREFIX.concat(movieId);        associateWith(&quot;proxyId&quot;, proxyId);        commandGateway.send(new FetchMovieDetailsCommand(proxyId, event.getSearchPhrase()));    }    ...}Proces ten może trochę potrwać (niedostępność zewnętrznego źródła, timeouty, problemy z łączem), dlatego też zdecydowałem się na sagę, która jest rozwiązaniem nieblokującym.Gdy proxy-service wyemituje zdarzenie MovieDetailsEvent, to w zależności od zawartości payloadu, śle command z uzupełnionymi szczegółami dla filmu lub nie i bez względu na rezultat kończę sagę:public class MovieSaga {    ...    @SagaEventHandler(associationProperty = PROXY_ID)    @EndSaga    public void handle(MovieDetailsEvent event) {        var movie = event.getExternalMovie();        if (MovieState.NOT_FOUND_IN_EXTERNAL_SERVICE == movie.getMovieState()) {            // handle when movie not found        } else {            commandGateway.send(new SaveMovieCommand(movieId, event.getExternalMovie()));        }    }    ...}Do przetestowania tego przypadku znów będziemy potrzebować fixture, tyle że tym razem skrojony pod sagi:public class MovieSagaTest {    ...    private SagaTestFixture&amp;lt;MovieSaga&amp;gt; fixture;    @BeforeEach    public void setup() {        fixture = new SagaTestFixture&amp;lt;&amp;gt;(MovieSaga.class);        ...    }}Testy mają sprawdzić czy odpowiednie commandy zostaną wyemitowane, oraz czy saga “wystartowała”, bądź zakończyła się w odpowiednim momencie.Szkielet testu wygląda następująco:  Mając agregat X z ustalonym identyfikatorem.  Wiedząc, że X nie wyemitował żadnego eventu (lub wyemitował konkretny event).  To gdy agregat X.  Wyemituje konkretny event.  Oczekujemy aktywnej (lub nieaktywnej) sagi oraz opublikowany command (lub nieopublikowanie niczego).public class MovieSagaTest {    ...    @Test    public void shouldDispatchFetchMovieDetailsCommand() {        fixture.givenAggregate(movieId)                                      // 1                .published()                                                 // 2                .whenAggregate(movieId)                                      // 3                .publishes(new MovieCreatedEvent(movieId, searchPhrase))     // 4                .expectActiveSagas(1)                                        // 5                .expectDispatchedCommands(                                                       new FetchMovieDetailsCommand(proxyId, searchPhrase));    }    @Test    public void shouldDispatchSaveCastCommand() {        fixture.givenAggregate(movieId)                                       // 1                .published(new MovieCreatedEvent(movieId, searchPhrase))      // 2                .whenAggregate(proxyId)                                       // 3                .publishes(new MovieDetailsEvent(proxyId, externalMovie))     // 4                .expectActiveSagas(0)                                         // 5                .expectDispatchedCommands(                                                        new SaveMovieCommand(movieId, externalMovie));    }}Jak widać zastosowanie fixture również w przypadku sagi, okazuje się proste i intuicyjne.Testy integracyjne - możliwe z Axonem?Po testach domenowych, gdy wiemy już, że nasza logika biznesowa jest poprawna (i mamy na to dowody w postaci testów!), można zabrać się za weryfikację trochę większego fragmentu aplikacji.KonfiguracjaTesty integracyjne z użyciem prawdziwego Event Store’a (w naszym przypadku AxonServera) wymagają więcej konfiguracji - twórcy w tym aspekcie akurat nie przygotowali nam gotowego rozwiązania.Trochę się naszukałem, zanim znalazłem informację o tym, że rekomendowanym przez AxonIQ rozwiązaniem problemu jest skorzystanie z obrazu dockerowego.Konkretnie wspominają o uruchomieniu AxonServer z terminala i podłączeniu się testami do tej instancji.Wręcz idealna rola dla testcontainers - pomyślałem (po przygotowaniu konfiguracji pod testy, trafiłem na podobne rozwiązanie na stacku).Stworzyłem klasę umożliwiającą podniesienie potrzebnych kontenerów, aby skorzystać z nich podczas testów:@ActiveProfiles(&quot;test&quot;)public class TestContainers {    private static final int MONGO_PORT = 29019;    private static final int AXON_HTTP_PORT = 8024;    private static final int AXON_GRPC_PORT = 8124;    public static void startAxonServer() {        GenericContainer axonServer = new GenericContainer(&quot;axoniq/axonserver:latest&quot;)                .withExposedPorts(AXON_HTTP_PORT, AXON_GRPC_PORT)                .waitingFor(                        Wait.forLogMessage(&quot;.*Started AxonServer.*&quot;, 1)                );        axonServer.start();        System.setProperty(                &quot;ENV_AXON_GRPC_PORT&quot;,                 String.valueOf(axonServer.getMappedPort(AXON_GRPC_PORT)));    }    public static void startMongo() {        GenericContainer mongo = new GenericContainer(&quot;mongo:latest&quot;)                .withExposedPorts(MONGO_PORT)                .withEnv(&quot;MONGO_INITDB_DATABASE&quot;, &quot;moviekeeper&quot;)                .withCommand(String.format(&quot;mongod --port %d&quot;, MONGO_PORT))                .waitingFor(                        Wait.forLogMessage(&quot;.*waiting for connections.*&quot;, 1)                );        mongo.start();        System.setProperty(                &quot;ENV_MONGO_PORT&quot;,                 String.valueOf(mongo.getMappedPort(MONGO_PORT)));    }}Należy tu jednak pamiętać o tym, że withExposedPorts wystawia porty tylko wewnątrz kontenera, potrzebny więc był sposób na pozyskanie portów, do których testy będą mogły się połączyć.Testcontainers przy każdym restarcie kontenera wystawia go na losowym wolnym porcie z danego zakresu, istnieje jednak metoda na pobranie tych portów w runtime.Robię to w ostatniej instrukcji każdej z metod, jednocześnie wrzucając znalezione wartości do zmiennych środowiskowych ENV_AXON_GRPC_PORT oraz ENV_MONGO_PORT.Zmienne te używam w yamlu konfiguracyjnym pod testy:spring:  data:        mongodb:            uri: mongodb://localhost:${ENV_MONGO_PORT}/moviekeeperaxon:  axonserver:        servers: localhost:${ENV_AXON_GRPC_PORT}Metody startMongo i startAxonServer wykorzystałem w klasie, z której dziedziczą wszystkie testy integracyjne:@ExtendWith(SpringExtension.class)@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)@ActiveProfiles(&quot;test&quot;)public class CommonIntegrationSetup {    ...    @BeforeAll    public static void beforeAll() {        TestContainers.startAxonServer();        TestContainers.startMongo();    }    ...}Po wszystkim nie musimy zatrzymywać kontenerów, zadzieje się to automatycznie (GenericContainer implementuje AutoCloseable).Należy jednak pamiętać o ustawieniu profilu i pliku konfiguracyjnym pod ten profil - zdarzyło mi się puścić testy (bez tych dwóch rzeczy skonfigurowanych), podczas gdy aplikacja chodziła “produkcyjnie” - można łatwo zgadnąć, do jakiego AxonServera owe testy się podłączyły. :)Skonfigurowane. Do dzieła!W mojej aplikacji w momencie, gdy znaleziony zostanie film o żądanym tytule, mają miejsce następujące kroki:  Zwracane są szczegóły znalezionego filmu.  Wysyłany jest command, który mówi znajdź trailery i obsadę dla tego filmu.Parę kroków później efektem tego commanda, są kolejne commandy, już skierowane do konkretnego mikroserwisu (odpowiedzialnego za pobranie obsady i trailerów).Żeby nie było za łatwo, wyszukiwanie czegokolwiek w TMDb zlecam osobnemu mikroserwisowi (zachęcam do spojrzenia na diagram komponentów z poprzedniego wpisu).Oczywiście nie chcę, aby test był zależny od jakiegoś serwisu, więc konieczne jest stworzenie mocka:@Profile(&quot;test&quot;)@Slf4j@Component@RequiredArgsConstructorpublic class MockProxyCommandHandler {    private final EventGateway eventGateway;    @CommandHandler    public void handle(FetchTrailersCommand command) {        log.info(&quot;MOCK fetching...&quot;);        eventGateway.publish(new TrailersDetailsEvent(command.getProxyId(), TRAILERS));    }}W teście chcę zasymulować sytuację, w której pojawia się command CreateTrailersCommand.W efekcie aplikacja powinna pobrać trailery dla wskazanego filmu i umieścić je w bazie:public class TrailersIntegrationTest extends CommonIntegrationSetup {    @Autowired    private CommandGateway commandGateway;    ...    @BeforeEach    public void beforeEach() {        this.movieId = &quot;123&quot;;        this.trailers = generateTrailers(movieId);    }    @Test    public void shouldRetrieveTrailers() {        // given        commandGateway.send(new CreateTrailersCommand(                    trailers.getAggregateId(),                     trailers.getExternalMovieId(),                     trailers.getMovieId()));        await()            .atMost(FIVE_SECONDS)            .with()            .pollInterval(ONE_HUNDRED_MILLISECONDS)            .until(() -&amp;gt; trailerRepository.findByMovieId(movieId).isPresent());        // when        ResponseEntity&amp;lt;TrailerDTO[]&amp;gt; trailerResponse = testRestTemplate                .getForEntity(                        String.format(GET_TRAILERS_URL, randomServerPort, movieId),                        TrailerDTO[].class);        // then        assertThat(trailerResponse.getStatusCode()).isEqualTo(HttpStatus.OK);        assertThat(trailerResponse.getBody()).isNotNull();        List&amp;lt;TrailerDTO&amp;gt; trailerDTOS = Arrays.asList(trailerResponse.getBody());        assertThat(trailerDTOS.size()).isEqualTo(2);        assertThat(trailerDTOS).isEqualTo(trailers.getTrailers());    }}Skorzystałem z awaitility, żeby poczekać chwilę na odpowiedź w razie małej czkawki.Testy E2ENa samym szczycie piramidy testów są testy end-to-end, czyli sprawdzenie aplikacji w ten sposób, w jaki klient z niej korzysta.W moim przypadku klientem jest aplikacja frontendowa, która uderzając na konkretny endpoint, oczekuje konkretnej odpowiedzi.Przetestujmy wyszukanie filmu po tytule.Taki test powinien:  uderzyć na endpoint, za którym kryje się dana funkcjonalność,  sprawdzić status odpowiedzi od serwera,  sprawdzić zawartość odpowiedzi,  upewnić się, że film został umieszczony w bazie, a jeśli tak to czy jest on równy temu, który dostaliśmy w ciele odpowiedzi.public class MovieE2ETest {    ...    @Test    public void shouldStoreMovie() {        // when        ResponseEntity&amp;lt;MovieDTO&amp;gt; storedMovieResponse = testRestTemplate.postForEntity(                           String.format(GET_OR_POST_MOVIES, randomServerPort),                            new TitleBody(SUPER_MOVIE),                            MovieDTO.class);        assertThat(storedMovieResponse.getStatusCode()).isEqualTo(HttpStatus.CREATED);        MovieDTO body = storedMovieResponse.getBody();        await()                .atMost(FIVE_SECONDS)                .with()                .pollInterval(ONE_HUNDRED_MILLISECONDS)                .until(() -&amp;gt; movieRepository.findById(body.getAggregateId()).isPresent());        // then        assertThat(body).isNotNull();        assertThat(body.getAggregateId()).isNotEmpty();        assertThat(body.getTitle()).isEqualTo(SUPER_MOVIE);        Optional&amp;lt;MovieDTO&amp;gt; persistedMovie = movieRepository                    .findByExternalMovieId(body.getExternalMovieId());        persistedMovie.ifPresent(movie -&amp;gt; {            assertThat(movie).isEqualTo(body);            assertThat(movie.getCreationDate()).isEqualTo(NOW);            assertThat(movie.isWatched()).isFalse();        });    }    ...}W tym podejściu również wykorzystałem kontenery testowe, tak samo, jak przy testach integracyjnych, konfiguracja jest identyczna.Nie chcę uzależniać żadnych testów od połączenia z zewnętrznym serwisem, więc i w tym przypadku posłużyłem się mockiem, symulującym działanie proxy-service.PodsumowanieJak widać korzystanie z Axonowych fixture bardzo ułatwia testowanie kodu, a i w przypadku testów wymagających szerszego kontekstu również istnieją rozwiązania.Uruchamianie takich testów można w łatwy sposób zautomatyzować, np. używając Travisa, dzięki czemu będziemy znali na bieżąco stan naszej aplikacji.Źródła  https://github.com/matty-matt/movie-keeper-core",
"url": "/2020/08/27/axon-kompleksowe-testowanie-aplikacji.html",
"author": "Mateusz Kociszewski",
"authorUrl": "/authors/mkociszewski.html",
"image": "mkociszewski.webp",
"highlight": "/assets/img/posts/2020-07-30-axon-kompleksowe-testowanie-aplikacji/axontest.jpg",
"date": "27-08-2020",
"path": "pl/2020-07-30-axon-kompleksowe-testowanie-aplikacji"
}
,


"2020-07-08-ansible-jak-uporzadkowac-chaos-html": {
"title": "Ansible - jak uporządkować chaos?",
"lang": "pl",
"tags": "ansible automation devops",
"content": "Czy istnieje możliwość usprawnienia istniejących procesów instalacji aplikacji lub nawet całych systemów mimo, że te sprawdzają się już od wielu lat? Okazuje się, że tak. Można dzięki temu zaoszczędzić trochę czasu na analizę występujących problemów, a także zwiększyć czytelność skryptów wykorzystywanych do instalacji. Ponadto można z administratora zdjąć obowiązek orientowania się w konfiguracji dziesiątek serwerów.Wystarczy w tym celu zastosować oprogramowanie do automatyzacji procesów. Takim narzędziem może być Ansible od Red Hat’a. W poniższym artykule opisano przykłady wykorzystania Ansible’a do instalacji platformy Eximee oraz kilku mikroserwisów wykorzystywanych przez klientów firmy. Opisane zostały podstawowe elementy oprogramowania i ich wykorzystanie. Wspomniano o sytuacjach, które można było zrobić lepiej podczas implementacji. Na koniec wymieniono główne zalety migracji ze skryptów bashowych na Ansible.Jak było kiedyś?Zręczność tworzenia skryptów w bashu od dawna jest ważną umiejętnością administratorów systemów informatycznych i osób działających w obszarze DevOps. Pozwala automatyzować dowolne zadania od tworzenia plików i nadawania im odpowiednich uprawnień, po wdrażanie i konfigurowanie gotowych aplikacji na docelowych serwerach. Skoro więc istnieją już sprawdzone i stosowane od wielu lat sposoby automatyzacji procesów, to czy warto je modyfikować lub usprawniać? Innymi słowy, czy jest sens zmieniać coś, co działa?Platforma Eximee, którą rozwijamy, instalowana była właśnie przy użyciu skryptów bashowych. Wszystkie wykorzystywane tam funkcje przez lata zostały doszlifowane, aby spałniały konkretne wymagania klientów i były na tyle uniwersalne, aby ograniczyć liczbę wprowadzanych w nich zmian. Napisany kod spełniał wszystkie założenia – tworzył odpowiednią strukturę katalogów, nadawał uprawnienia, zastępował placeholdery odpowiednimi wartościami z pliku konfiguracyjnego, startował poszczególne aplikacje osadzone wcześniej w katalogach docelowych, itp.Choć proces wydaje się prosty, to nie każda instalacja platformy kończyła się sukcesem. Przyczyn bywało kilka – niechęć instalującego do dokładnego zapoznania się z dokumentacją, czy choćby brak wcześniejszego doświadczenia we wdrażaniu platformy. Zawsze mogła zdarzyć się sytuacja, że nie wszystkie parametry instalacyjne zostały ustawione. Także użytkownik, z którego uruchamiano skrypt był nie bez znaczenia. Musiał on posiadać wszystkie uprawnienia do wykonania zawartych w kodzie instrukcji. Każda z wymienionych przyczyn wymagała analizy. Aby ta była możliwa konieczne było logowanie poszczególnych etapów wykonania skryptu i wartości, które miały wpływ na nieudaną instalację. Fakt ten wymagał przygotowania dodatkowych instrukcji tworzących plik z logiem i wypisujących stan instalacji na ekran konsoli użytkownika, tym samym zwiększajac złożoność i rozmiar skryptu.Sam start instalacji był dość prosty. Wystarczyło rozpakować archiwum ZIP, zawierające piki instalacyjne platformy Eximee i wykonać znajdujący się tam skrypt, przekazując plik z ustawieniami dostosowanymi dla danego serwera. Plik z ustawieniami tworzony był po stronie klienta i rozwijany w oparciu o załączane instrukcje.Uruchomienie skryptu instalacyjnego:$ ./update.sh ~/config/settings.shCo działo się po uruchomieniu skryptu update.sh? Zasadniczo za każdym razem to samo. Zatrzymywane były kluczowe procesy (Tomcat, Servicemix, itd.), usuwane były „ślady” po poprzedniej instalacji, rozpakowywana była zawartość archiwum, a wszystkie pliki trafiały do ustalonej struktury katalogów na serwerze klienta. Na koniec ponownie uruchamiane były kluczowe aplikacje. Przykładowa funkcja usuwająca dotychczas wyglądała tak:function cleanOldVersion {     \tlog &quot;Czyszczenie poprzedniej instalacji&quot;    \t   \tCONFIGS=&quot;${installSettings[&#39;ROOT&#39;]}/${installSettings[&#39;CONFIG_FOLDER&#39;]}&quot;    \tBINS=&quot;${installSettings[&#39;ROOT&#39;]}/${installSettings[&#39;BINARIES_FOLDER&#39;]}&quot;       \tlog &quot;Usuwanie: $CONFIGS/eximee/bundles_config&quot;    \trm -rf $CONFIGS/eximee/bundles_config || fail &quot;Blad przy czyszczeniu poprzedniej instalacji&quot;       \tlog &quot;Usuwanie: $CONFIGS/eximee/logback&quot;    \trm -rf $CONFIGS/eximee/logback || fail &quot;Blad przy czyszczeniu poprzedniej instalacji&quot;       \tlog &quot;Usuwanie: find $CONFIGS/eximee/ -maxdepth 1 -type f -name \\*.xml&quot;    \tfind $CONFIGS/eximee/ -maxdepth 1 -type f -name *.xml | grep -v &quot;konfiguracja-partnera&quot; |    \tgrep -v \t&quot;knowledge-base&quot; | xargs rm -rf || fail &quot;Blad przy czyszczeniu \tpoprzedniej instalacji&quot;       \tlog &quot;Czyszczenie tomcata.\\nUsuwanie plików i katalogów bez webapps&quot;    \tfind $BINS/tomcat/ -maxdepth 1 -mindepth 1 -not -name &quot;webapps&quot; | xargs rm -rf       \tlog &quot;Usuwanie aplikacji bez knowledgebase&quot;    \tfind $BINS/tomcat/webapps/ -maxdepth 1 -mindepth 1 -not -name &quot;knowledgebase*&quot; | \txargs rm -rf      \tlog &quot;Czyszczenie tomcata 9 na JVM 11.\\nUsuwanie plików i katalogów bez webapps&quot;    \tfind $BINS/tomcat-jvm11/ -maxdepth 1 -mindepth 1 -not -name &quot;webapps&quot; | xargs rm -rf       \tlog &quot;Usuwanie aplikacji bez knowledgebase&quot;    \tfind $BINS/tomcat-jvm11/webapps/ -maxdepth 1 -mindepth 1 -not -name &quot;knowledgebase*&quot; | \txargs rm -rf       \tlog &quot;Usuwanie: $BINS/eximee/bundles/\\*&quot;    \trm -rf $BINS/eximee/bundles/* || fail &quot;Blad przy czyszczeniu poprzedniej instalacji&quot;       \tlog &quot;Usuwanie: $BINS/eximee/bundles-jvm11/\\*&quot;    \trm -rf $BINS/eximee/bundles-jvm11/* || fail &quot;Blad przy czyszczeniu poprzednieji instalacji&quot;       \tif [ -d $BINS/servicemix/data/cache/bundle212 ]; then     \t\tlog &quot;Usuwanie: $BINS/servicemix/data/cache/bundle212&quot;     \t\trm -rf $BINS/servicemix/data/cache/bundle212 || fail &quot;Blad przy czyszczeniu \t\t\t\tpoprzedniej instalacji&quot;    \tfi       \tlog $PHASE_SEPARATOR       \tlog &quot;Czyszczenie zalezne od platformy&quot;    \tcleanSystemBased || fail &quot;Blad czyszczenia czesci zaleznej od platformy&quot;    \tlog $PHASE_SEPARATOR   }Niby nic nadzwyczajnego, a jednak na samo logowanie postępu i usunięcie plików, które i tak zostaną ponownie wgrane na środowisko wraz z nową paczką, przeznaczono jakieś 30 linii kodu. Choć logowanie niekoniecznie było niezbędne, to jednak pozwalało śledzić kolejne etapy procesu instalacji, dając większą wiedzę osobom analizującym ewentualne przyczyny nieudanej instalacji.W takim razie, co miał zrobić administrator systemu, który chciałby skrócić proces i zainstalować tylko te komponenty, które uległy zmianie? No cóż, musiał on wykazać się nie lada cierpliwością i pewnością siebie, modyfikując dołączony do paczki skrypt instalacyjny lub po prostu pogodzić się z faktem, że nie da się tego zrobić „na skróty”.Jakby tego było mało, chcąc zainstalować kompletną platformę na „świeżym” środowisku konieczna była praca z kilkoma archiwami zip, które zawierały poszczególne składowe systemu. Osobno bowiem dostarczano strukturę bazy danych, frontend, backend czy aplikacje do zarządzania systemem plików. W skrócie, aby administrator zainstalował platformę Eximee musiał co najmniej 4 razy powtórzyć podobny proces dla każdej części platformy.Era Ansible Już od wczesnych lat studiów programistom wpaja się, aby dążyć do utrzymania eleganckiej i czytelnej struktury swoich aplikacji (KISS – Keep it simple, stupid). Doskonale w ten trend wpasowuje się Ansible. To kupione przez firmę Red Hat opensource’owe oprogramowanie m.in. do automatyzacji procesu wdrażania aplikacji i zarządzania konfiguracją. Za pomocą języka YAML, w prosty sposób pozwala opisać wzajemne relacje między systemami.Czytając dokumentację dostajemy obietnicę ujednolicenia konfiguracji, organizacji złożonych procesów i jednocześnie łatwą do zarządzania architekturę. Ponadto czytamy, że Ansible pozwala osiągnąć wzrost wydajności i nie nakłada dodatkowych wymagań na otoczenie, w którym działa.Do pracy z Ansible wymagane jest tylko, aby na maszynie sterującej zainstalowane były:  Python 2.7 lub Python 3.5+  Ansible (np. przy użyciu Python Package Manager’a)Ansible wymaga hasła lub klucza SSH w celu rozpoczęcia zarządzania systemami i może rozpocząć zarządzanie nimi bez instalowania oprogramowania agenta, unikając problemu „zarządzania zarządzaniem” powszechnego w wielu systemach do automatyzacji.Ansible łączy się z systemami poprzez mechanizmy transportowe – SSH (Unix) lub PowerShell (Windows). Moduły, które są małymi programami, zawierające uzupełnione argumenty, przenoszone są do tymczasowego katalogu przez wspomniane mechanizmy na zarządzane maszyny. Tam są wykonywane, a następnie usuwane w ramach jednej akcji. Moduły zwracają obiekty JSON na standardowe wyjście, a te z kolei przetwarzane są przez program Ansible na maszynie sterującej. Mogą one zarządzać zasobami w sposób indempotentny. Oznacza to, że moduł działa w sposób deklaratywny, czyli może zdecydować np. czy dany pakiet powinien zostać zainstalowany w określonej wersji lub nie wykonać żadnej akcji, gdy system jest już w pożądanym stanie. Można je też uruchamiać pojedynczo (imperatywnie).Do zarządzania administrowanymi systemami służą pliki inventory. Pozwalają one na grupowanie serwerów i definiowanie zmiennych, które później wykorzystywane będą w tzw. playbookach. Plik inventory może być użyty globalnie, w ramach instalacji Ansible, lub wykorzystać lokalne pliki inventory dedykowane dla konkretnego projektu. Przykładowy fragment pliku inventory wygląda następująco:# eximee-ansible/settings/settings-template.yml### Ogólne parametry instalacjivalidation_protocol: httpvalidation_host: eximee-validationvalidation_port: 8080services_protocol: httpservices_host: eximee-servicesservices_port: 8080servicemix_host: eximee-servicemixvalidation_url: &quot;{{ validation_protocol }}://{{ validation_host }}:{{ validation_port }}/osgi-bridge/validation/&quot;services_url: &quot;{{ services_protocol }}://{{ services_host }}:{{ services_port }}/osgi-bridge/serviceproxy/&quot;eximee_status_url: &quot;{{ eximee_status_protocol }}://{{ eximee_status_host }}:{{ eximee_status_port }}/eximee-status/&quot;repository_url: &quot;{{ repository_protocol }}://{{ repository_host }}:{{ repository_port }}/repository/&quot;repository_username: UZUPELNIJrepository_password: UZUPELNIJmongodb_host: eximee-mongomongo_db_auth: MONGODB-CR...W pliku inventory znajdują się zatem dane dotyczące hostów czy używanych portów. W inventory warto umieścić również takie zmienne, które są częściej wykorzystywane w playbookach lub zależne są np. od serwerów, na których instalujemy aplikację. Nie powinno się bowiem modyfikować playbooków przy okazji każdej instalacji, a utrzymywać dla danego serwera odpowiednią konfigurację. W przypadku platformy Eximee takimi zmiennymi są np. adresy URL różnych aplikacji. Warto też zwrócić uwagę, że adresy w powyższym przykładzie nie są zdefiniowane „na sztywno”. Zawierają one odwołania do kilku innych zmiennych tworząc nową wartość, na co również pozwala Ansible.Do zapamiętania haseł i innych danych uwierzytelniających pomocne okaże się Ansible Vault. Właściwość ta pozwala na przechowywanie poufnych i wrażliwych danych (np. haseł) w inny sposób, niż w postaci zwykłego tekstu w plikach inventory, rolach lub playbookach. Wykorzystując poniższe polecenie i podając hasło, przy użyciu którego konfiguracja zostanie zaszyfrowana, można w prosty sposób zabezpieczyć wskazany plik.$ ansible-vault encrypt /path/to/fileNew Vault password: Confirm New Vault password: Encryption successfulZawartość takiego zasobu, który przed wykonaniem operacji wyglądał następująco:---repository_username: repoUserrepository_password: ^$asdY$4-(56aspo użyciu Ansible Vault wygląda znacznie bezpieczniej:$ANSIBLE_VAULT;1.1;AES256393038396565633461386537343838613639376439343235633565333664373336333833393237393135386136336365316535346630333261316232393238360a353437383762386665363662316138636361393731623061653962313862653332646365376164636339353736373661646237623531353263663338323163350a38623038303465353836353830353737643535616634633738663762366238613839356538336530636136333065343935393034656434666464623033633330656162643838643761656633303664633131653362373438636364376664373039643133623139353663323463623331343838326238393364333266353937313937663762653739343932363336623165353435646531366662633837353562Uruchomienie playbooka, który wykorzystuje zaszyfrowane pliki możliwe jest poprzez użycie dyrektywy --ask-vault-pass, np.:$ ansible-playbook --ask-vault-pass --inventory inventory_file sample_playbook.ymlWiedząc już czym jest moduł oraz gdzie należy umieścić konfigurację, potrzebna jest jeszcze wiedza, jak definiować pewien stan systemu, który chcemy osiągnąć. Do tego właśnie służą playbooki. Definicja takiego stanu dzieli się na taski. Zwiększa to nie tylko czytelność kodu playbooka, ale też oddziela od siebie niezależne etapy instalacji.# eximee-ansible/platform.yml---- name: Create installation_tmp dir  hosts: localhost  become: no  connection: local  tasks:    - file:        path: &quot;/installation_tmp&quot;        state: directory- import_playbook: check-settings.yml- import_playbook: eximee-common.yml- import_playbook: dmzzew.yml- import_playbook: dmz.yml- import_playbook: formstore.yml- name: Remove installation_tmp dir  hosts: localhost  become: no  gather_facts: false  connection: local  tasks:    - file:        path: &quot;/installation_tmp&quot;        state: absentW powyższym przykładzie widoczne są dwa proste zadania. Pierwsze, o nazwie Create installation_tmp dir tworzy tymczasowy katalog instalacyjny installation_tmp w lokalizacji, która zdefiniowana została pod zmienną eximee_platform_dest. Drugie (Remove installation_tmp dir) ten katalog usuwa. Pomiędzy tymi zadaniami zaimportowane zostały odseparowane logicznie playbooki. Każdy z nich instaluje odrębną część platformy, a ich rozdzielenie jeszcze bardziej zwiększa czytelność kodu. Co ciekawe, również importowane playbooki zostały odpowiednio podzielone na role, które, na podstawie określonej struktury plików, pozwalają na automatyczne ładowanie zmiennych czy tasków. Ponadto, grupowanie zadań w role pozwala na ich łatwe reużywanie.Jak uruchomić taki skrypt instalacyjny? Nic prostszego – wystarczy poniższa instrukcja:$ ansible-playbook nazwa_playbooka.ymlDocelowo więc pierwotna struktura paczki wdrożeniowej, składającej się ze skryptów bashowych, domyślnych parametrów konfiguracyjnych i plików z platformą, nabrała nowego charakteru. Wprowdzone modyfikacje pozwoliły na wydzielenie wielu plików z odseperowanymi logicznie zadaniami, które nie musiały już być rozpakowywane na maszynie docelowej, a jedynie uruchomione z odpowiednim użytkownikiem z poziomu maszyny kontrolnej:eximee-ansible|──common|──roles                                   # katalog agregujący role|   |──java8                               # rola java8|   |   |──tasks                           # taski zdefiniowane w ramach roli java8|   |   |   |  ensure-dir-exists.yml     |   |   |   |  install.yml|   |   |   |  main.yml|   |   |   |  unarchive.yml|   |──java11|   |   |──tasks                           # taski zdefiniowane w ramach roli java11|   |   |   |  ...|   |──tomcat-server                       # rola tomcat-server|   |   |──tasks                           # taski zdefiniowane w ramach roli tomcat-server|   |   |   |  main.yml|   |   |   |  server.yml|   |   |   |  setup.yml|   ...|──settings                                # ogólne parametry instalacji |   |   settings-template.yml             |   check-settings.yml                     # playbook weryfikujący czy wszystkie wymagane parametry instalacyjne zostały zdefiniowane|   formstore.yml                          # playbook instalacyjny dla roli formstore|   platform.yml                           # główny playbook instalacyjny|   ...Jakie korzyści niosła za sobą taka zmiana? Tych było kilka:  idempotenetność stanu docelowego – nie instalujemy czegoś, co już jest w pożądanym stanie;  możliwość zarządzania konfiguracjami poszczególnych komponentów platformy Eximee;  platforma dostarczana jest jedną paczką, zawierającą wszystkie komponenty;  instalacja odbywać się będzie bardzo podobnie, jak miało to miejsce w przypadku skryptów bashowych – uruchamiany będzie główny playbook wraz ze wskazanaiem pliku konfiguracyjnego;  możliwość uruchomienia skryptów z poziomu maszyny sterującej – system będzie wiedział na jakim serwerze ma zostać zainstalowana platforma;  instalowane będą tylko te aplikacje, których konfiguracja się zmieniła;  wbudowany mechanizm logowania postępu instalacji;  znaczna poprawa czytelności;  łatwa możliwość rozszerzerzania skryptów – np. o tagi, pozwalające na wykonanie skryptów tylko dla danego komponentu lub grupy komponentów.Ansible, a instalacja mikroserwisówPrzejście na nowy sposób instalacji wykonane zostało nie tylko dla komponentów platformy, ale też dla mikroserwisów dostarczanych dla jednego z klientów. W każdym takim mikroserwisie, będącym klasyczną aplikacją Spring Bootową, wydzielono dodatkowy moduł, w którym zdefiniowano taski odpowiedzialne za:  kopiowanie wykonywalnego jara na docelowe środowisko, pod określoną w konfiguracji lokalizację;  utworzenie katalogu z konfiguracją;  kopiowanie konfiguracji do utworzonego katalogu;  utworzenie katalogu z logami;  kopiowanie konfiguracji logback’a na środowisko docelowe;  restart aplikacji, jeżeli ta jest już uruchomiona na środowisku;  weryfikację działania aplikacji.Instalacja takiego mikroserwisu sprowadzała się do uzupełnienia parametrów konfiguracyjnych w pliku hosts-template.yml oraz wykonania polecenia:$ ansible-playbook --inventory hosts-template.yml microservice-x.yml -vvvvPlik microservice-x.yml to nic innego, jak główny playbook grupujący w rolach wszystkie powyższe zadania. Playbook jest bardzo prosty i wygląda następująco:---- name: deploy microservice-x  hosts: lan  become: &#39;&#39;  become_user: &#39;&#39;  roles:    - microservice-xJuż na pierwszy rzut oka widać, że wspomniane taski mogły zostać z powodzeniem wydzielone do osobnego projektu. Ten odrębny byt mógłby stać się wspólnym elementem dla wszystkich mikroserwisów po to, by nie powielać kodu i zmniejszyć złożoność dodatkowego modułu. W takich kwestiach bardzo pomocny jest rozdział poświęcony dobrym praktykom w samej dokumentacji Ansible, dostępny pod adresem: dobre praktyki.Podczas pierwszych wdrożeń na środowisku klienckim okazało się, że konieczne jest wprowadzenie dodatkowej konfiguracji. Powyższa definicja playbooka jest już w nią zaopatrzona. Chodzi bowiem o dyrektywy become oraz become_user. Pierwsze instalacje aplikacji kończyły się niepowodzeniem z powodu braku odpowiednich uprawnień. Dyrektywa become pozwala na ich tzw. eskalację czyli wykonanie zadania z uprawnieniami innego użytkownika, niż obecnie zalogowany (zdalny). Innymi słowy zastosowanie dyrektywy become pozwala na wykonanie poleceń przy użyciu takich narzędzi jak sudo, su, runas, itd. Jeżeli zdefiniowana zostanie jedynie dyrektywa become, użytkownik domyślnie otrzymuje uprawnienia root’a. Problem w tym, że nie zawsze to właśnie ten użytkownik był tym pożądanym. Tu z pomocą przyszła dyrektywa become_user, która pozwala na zdefiniowanie nazwy użytkownika, z którym mamy np. wykonać odpowiednie zadanie. Wykorzystanie w powyższym przykładzie playbooka następujących wartości:switch_user: yes            # włączenie eskalacji uprawnieńswitched_user: tomcat       # przełączenie na użytkownika o nazwie tomcatpozwoli na wykonanie zadań z poziomu użytkownika o nazwie tomcat, a nie tego, który jest obecnie zalogowany na serwerze.Śledzenie postępów instalacjiAnsible dostarcza wbudowany mechanizm logowania, pozwalający na śledzenie postępów instalacji oraz uzyskanie informacji, które zadania zostały wykonane, a które nie. Przykładowy log procesu instalacji mikroserwisu wygląda następująco:PLAY [deploy microservice-x] ********************************************************************************************************************************************************************************TASK [Gathering Facts] **********************************************************************************************************************************************************************************ok: [lan]TASK [microservice-x : copy microservice-x jar] *****************************************************************************************************************************************************************[WARNING]: Unable to find &#39;../files/bundles&#39; in expected paths (use -vvvvv to see paths)TASK [microservice-x : create config directory] *************************************************************************************************************************************************************ok: [lan]TASK [ microservice-x : copy application properties] *********************************************************************************************************************************************************ok: [lan] =&amp;gt; (item=/home/users/rmastalerek/Projects/Eximee/microservice-x/microservice-x-deploy/src/main/ansible/install/roles/microservice-x/files/../templates/application.properties.j2)TASK [microservice-x : make sure logs directory exists] *****************************************************************************************************************************************************ok: [lan]TASK [microservice-x : create logs directory] ***************************************************************************************************************************************************************skipping: [lan]TASK [microservice-x : make sure sensitive logs directory exists] *******************************************************************************************************************************************ok: [lan]TASK [microservice-x : create sensitive logs directory] *****************************************************************************************************************************************************skipping: [lan]TASK [microservice-x : copy logback conf] *******************************************************************************************************************************************************************ok: [lan]TASK [microservice-x : get pid of microservice-x] ***************************************************************************************************************************************************************changed: [lan]TASK [microservice-x : kill microservice-x processes] ***********************************************************************************************************************************************************changed: [lan] =&amp;gt; (item=14454)TASK [microservice-x : wait_for] ****************************************************************************************************************************************************************************ok: [lan] =&amp;gt; (item=14454)TASK [microservice-x : stop microservice-x] *********************************************************************************************************************************************************************TASK [microservice-x : start/restart microservice-x] ************************************************************************************************************************************************************changed: [lan]TASK [microservice-x : check if microservice-x process exists] **************************************************************************************************************************************************ok: [lan]TASK [microservice-x : report status of service] ************************************************************************************************************************************************************skipping: [lan]TASK [microservice-x : report status of service] ************************************************************************************************************************************************************ok: [lan] =&amp;gt; {    &quot;msg&quot;: &quot;Microservice-x works&quot;}PLAY RECAP **********************************************************************************************************************************************************************************************lan             : ok=12   changed=3    unreachable=0    failed=0    skipped=5    rescued=0    ignored=0 Sposób prezentowania informacji przez Ansible jest dość czytelny. Instalację rozpoczyna uruchomienie playbooka o nazwie deploy microservice-x. Następnie wykonywany jest domyślny, ansible’owy task Gathering Facts, który zbiera informację o hostach, na których wykonywane będą zmiany. Później już wykonywane są wszystkie zadania zdefiniowane w playbooku i wykorzystywanych rolach. Każde zadanie kończy się pewnym statusem. Na powyższym przykładzie widać, że zadania uzyskały takie statusy, jak: ok, changed czy skipping. Dzięki wspomnianej na początku idempotentności niektóre zadania zostały pominięte, ponieważ system był już w pożądanym stanie. Tym zadaniom system przypisał status ok. Taski, które wymagały wykonania jakiejś zmiany na serwerze oznaczone zostały statusem changed. Status skipping został przypisany zadaniom, których wykonanie zostało oparte o pewien warunek, a ten z kolei nie został spełniony.Każde wykonanie playbooka kończy się podsumowaniem. Zawiera ono zbiorczą informację dotyczącą wszystkich zadań oraz ich końcowych statusach. Gdyby okazało się, że jeden z kluczowych tasków nie może zostać wykonany, np. z powodu braku połączenia z serwerem docelowym, to skrypt zakończyłby swoje wykonanie na tym zadaniu, np.:PLAY [deploy microservice-x] ********************************************************************************************************************************************************************************TASK [Gathering Facts] **********************************************************************************************************************************************************************************fatal: [lan]: UNREACHABLE! =&amp;gt; {&quot;changed&quot;: false, &quot;msg&quot;: &quot;Failed to connect to the host via ssh: ssh: Could not resolve hostname some.host.consdata.local: Temporary failure in name resolution&quot;, &quot;unreachable&quot;: true}PLAY RECAP **********************************************************************************************************************************************************************************************lan                        : ok=0    changed=0    unreachable=1    failed=0    skipped=0    rescued=0    ignored=0Ansible daje możliwość wykonania playbooka z większą ilością informacji poprzez użycie przełącznika verbose (-vvv lub -vvvv z możliwością debugowania połączenia)PodsumowanieZgrabne zarządzanie konfiguracją, automatyzacja i uproszczenie procesu instalacji, do tego czytelność i większa kontrola nad procesem wdrażania aplikacji. To główne zalety tego, co udało się wprowadzić zastępując nieco archaiczne podejście oparte o bashowe skrypty.Wymienione korzyści wpływają przede wszystkim na skrócenie czasu wykonywania powtarzalnych czynności. Sama automatyzacja pozwala ograniczyć do minimum ryzyko popełnienia błędu ludzkiego podczas instalowania aplikacji. Zapis kroków instalacji aplikacji czy systemu w postaci prostych tasków oraz grupowanie ich w moduły a następnie playbooki pozwala na standaryzację procesu. To z kolei wyklucza niepotrzebne wykonywanie ręcznych zmian i późniejszą, często czasochłonną analizę problemu, gdy taki wystąpi. Dzięki plikom inventory nie jest konieczne utrzymywanie przez administratora konfiguracji do znacznej ilości serwerów i dokumentacji do nich. Niewątpliwą zaletą Ansible jest przewidywalność. Zapewnia ona efektywną pracę administratora czy DevOpsa i ogranicza wystąpienie nieoczekiwanych problemów.",
"url": "/2020/07/08/ansible-jak-uporzadkowac-chaos.html",
"author": "Robert Mastalerek",
"authorUrl": "/authors/rmastalerek.html",
"image": "rmastalerek.webp",
"highlight": "/assets/img/posts/2020-07-08-ansible-jak-uporzadkowac-chaos/chaos.jpg",
"date": "08-07-2020",
"path": "pl/2020-07-08-ansible-jak-uporzadkowac-chaos"
}
,


"2020-06-09-microservices-on-axon-html": {
"title": "Mikroserwisy na Axonie",
"lang": "pl",
"tags": "java microservices axon event-sourcing spring-boot",
"content": "Znając definicję Event Sourcingu oraz korzyści, jakie nam zapewnia (dla przypomnienia polecam wpis Marcina poświęcony częściowo tej tematyce) warto rozważyć zastosowanie tego wzorca w swoim projekcie (oczywiście nie wszędzie się on nada).Osoby zainteresowane tematem z pewnością zostaną postawione przed wyborem technologii, w której rozpoczną implementację. Niezależnie od języka programowania, można implementować CQRS oraz Event Sourcing samemu, od A-Z, jednakże byłoby to czasochłonne i może prowadzić do wielu błędów. Alternatywą będzie zatem skorzystanie z gotowego frameworku, który od początku tworzony był z myślą o wspomnianych wzorcach (włączając w to mikroserwisy) - mowa tutaj o AxonFramework.W tym wpisie przedstawię Axona i omówię wybory, przed którymi stałem w kontekście tego frameworka, oraz drogę migracji z monolitu do mikroserwisów wraz z problemami, na które się natknąłem.Krótko o AxonieAxon to framework, który czerpie garściami z Domain Driven Design (które jest poza zakresem tego wpisu), wykorzystując również nomenklaturę panującą w tym podejściu, którą także będę się posługiwał w tym wpisie.Axon bierze na barki zarządzanie przepływem wszystkich informacji między komponentami np. kierowanie commandów do odpowiednich agregatów, czy zapisywanie eventów w event store. Jeżeli chodzi o kwestie event store’a, to framework zostawia tu pełną dowolność, choć nie każda baza spełni się w tej roli.Dodatkowym plusem jest bezproblemowa integracja ze Spring Bootem, możliwość skalowania i gotowość produkcyjna, co moim zdaniem czyni Axona mocnym graczem.Event storeFundamentem projektu opartego o Event Sourcing jest oczywiście event store - źródło prawdy całego systemu, stąd wybór narzędzia pod tę funkcję jest kluczowy.Może Kafka?Kafka opiera się na eventach, których kolejność pojawiania się może zostać zachowana - co zapobiega sytuacji, w której wykonamy aktualizację krotki, zanim zostanie ona utworzona.Ponadto Kafka trzyma dane na topicach, zapamiętując offset (liczbę porządkową) dla każdego eventu. Znając offset istnieje możliwość odtworzenia topica od tego offsetu, aż do końca - umożliwia to odtwarzanie idealnego stanu aplikacji dosłownie na zawołanie (dopóki nie mamy do czynienia z paruset milionami eventów :wink:).Do tego Kafka bardzo łatwo się skaluje oraz uniemożliwia edycję nałożonych eventów, co niewątpliwie jest plusem. Jest jednak parę punktów, które brakują Kafce, do bycia idealnym kandydatem na event store:  Problem pojawia się w momencie, gdy chcielibyśmy odtworzyć agregat na podstawie eventów. Kafka w tym momencie musiałaby przeiterować cały topic od pewnego offsetu, aż do końca.W kolejnym kroku konieczne jest odfiltrowanie eventów nie związanych z agregatem, który próbujemy odtworzyć, co wymaga od nas dodatkowej logiki w kodzie, oraz nakłada niepotrzebny narzut na event store (odfiltrowane eventy nie są nam potrzebne).  Drugim problemem jest brak natywnego wsparcia dla mechanizmu snapshotów, bez którego odtwarzanie stanu przy dużym wolumenie może trwać wieki.  Kolejnym ograniczeniem równie ważnym co poprzednie, jest brak optimistic lockingu w Kafce (istnieje jedynie obejście w postaci jednego wątku piszącego na topic). Bez tego mechanizmu, w wielowątkowej aplikacji może pojawić się problem, gdy wpadną dwa eventy w tym samym czasie - wtedy jedno z tych zdarzeń zaaplikuje się na modelu zbudowanym z niekompletnego zestawu eventów.Potencjalnym rozwiązaniem pierwszego problemu mógłby być osobny topic dla każdego agregatu, wówczas odpada konieczność filtrowania eventów.To rozwiązanie jednak może nie sprawdzić się przy ogromnej ilości agregatów. Wynika to ze sposobu, w jaki Kafka przechowuje topici (a właściwie partycje) - dla każdej tworzony jest osobny katalog w systemie plików. Szczegółowe wyjaśnienie znajduje się w filmie przygotowanym przez AxonIQ (firma odpowiedzialna za rozwój Axona).AxonServerW kwestii event store AxonIQ wyszedł na przeciw potrzebom dając do dyspozycji swoje narzędzie, które idealnie spełnia się w tej roli - AxonServer:  Pozwala na dokładanie eventów (z jednoczesnym brakiem możliwości edycji już istniejących).  Zapewnia stałą wydajność niezależnie od ilości danych przetrzymywanych w event store.  Umożliwia konstruowanie snapshotów dla agregatów i nakładanie ich (w przypadku dużej ilości eventów rekonstrukcja agregatu bez funkcjonalności snapshotów może trochę trwać).Po uruchomieniu AxonServera mamy dostęp do dashboardu pokazującego, który mikroserwis jest podpięty pod event store wraz z jego liczbą instancji:Na samym dashboardzie, funkcjonalności panelu administracyjnego się nie kończą:  Podgląd konfiguracji wraz z przepustowością (commandy/eventy/query/snapshoty na sekundę).  Możliwość wyszukiwania eventu przy użyciu zapytań.  Tabelka ze wskazaniem, który command, ile razy i w jakim serwisie został obsłużony.  Zarządzanie dostępem do panelu.Oczywiście AxonFramework jest w pełni kompatybilny z AxonServerem i działa out-of-the-box, bez dodatkowej konfiguracji.Najpierw monolitZaczynając przygodę z Axonem, nie chciałem skakać na głęboką wodę, zacząłem więc od monolitu, mając jednak z tyłu głowy perspektywę zmigrowania na coś bardziej skalowalnego.Migracja z monolitu na mikroserwisy nierzadko sprawia wiele problemów, tak było również w moim przypadku z tą aplikacją.W skrócie pozwala ona na wyszukiwanie filmów po tytułach, wraz z ich obsadą oraz trailerami korzystając z API TMDb, zapisywanie wszystkiego w bazie, oznaczanie filmu jako przeczytany oraz sprawdzanie premiery cyfrowego wydania.Stworzyłem więc agregat filmu wraz z encjami zawierającymi trailery oraz obsadę:@Aggregatepublic class MovieAggregate {    @AggregateIdentifier    private MovieId movieId;    @AggregateMember    private TrailerEntity trailerEntity;    @AggregateMember    private CastEntity castEntity;    ...}Pobieranie danych z zewnętrznego serwisu działo się w event handlerze, poprzez zawołanie odpowiedniej metody z interfejsu ExternalService:@Servicepublic class MovieEventsHandler {    ...    @EventHandler    public void handle(MovieSearchDelegatedEvent event) {            ...                ExternalMovie externalMovie = externalService.searchMovie(event.getSearchPhrase());                commandGateway.send(new SaveMovieCommand(event.getMovieId(), externalMovie));            ...    }    ...}Agregat obsługiwał SaveMovieCommand, wysyłając MovieSavedEvent, który z kolei powoduje zapis do bazy:    @EventHandler    public void handle(MovieSavedEvent event) {        ...        movieRepository.findByExternalMovieId(event.getExternalMovie().getExternalMovieId().getId()).ifPresentOrElse(                movie -&amp;gt; handleMovieDuplicate(),                () -&amp;gt; persistMovie(event));    }Szczegółowy diagram przepływu dla tego przypadku użycia (już dla mikroserwisów) znajduje się w kolejnym rozdziale.Projekt w tym momencie spełniał moje wymagania i składał się z trzech elementów:  Aplikacja-monolit  Event store - AxonServer  Storage, read model - MongoDBUwidoczniły się poszczególne funkcjonalności, które mogłyby być odrębnymi serwisami - mowa tu o zarządzaniu: filmami, trailerami, obsadą oraz serwis odpowiedzialny za odświeżanie dat cyfrowej premiery wraz z ocenami i liczbą głosów.MikroserwisyPrzyszła pora na przekucie teorii w praktykę wykorzystując wypracowany wcześniej podział odpowiedzialności.Aplikacja podzielona na mniejsze fragmenty (realizujące skończone funkcjonalności) wyglądałaby w ten sposób:  proxy-service, odpowiedzialny za pobieranie danych z zewnętrznego serwisu.  trailer-service, obsługujący zapis/odczyt trailerów, serwujący endpointy do pobierania trailerów.  cast-service, robiący to samo dla obsady.  movie-service, odpowiadający za szczegóły dot. filmu wraz z funkcjonalnością cyfrowych premier, serwujący wszystkie endpointy związane z filmem.Przejście na mikroserwisy wiązało się również ze stworzeniem API Gateway kierującym ruch do odpowiedniego serwisu w zależności od endpointu.Na diagramie prezentuje się to następująco:A tak prezentuje się przepływ Axonowych zdarzeń w przypadku wyszukania filmu z automatycznym zwróceniem wyniku.Obsada i trailery pobierane są z zewnętrznego serwisu od razu i zapisywane do bazy. Użytkownik dopiero później może je pobrać wchodząc w szczegóły wyszukanego filmu. ProblemyMigracja okazała się bezbolesna dla Axon Servera, który bez problemu zaczął wykrywać nowe instancje. Pierwsze problemy zaczęły pojawiać się w momencie, gdy chciałem wysłać command do innego mikroserwisu.Z jakiegoś powodu aplikacja nie potrafiła skorelować commanda o tych samych polach i nazwie w jednym serwisie z identycznym commandem w drugim serwisie.Okazało się, że problem tkwi w serializacji - commandy były w pakietach o innych nazwach, przez co nie były interpretowane jako ten sam byt.Nie chcąc tracić czasu, uspójniłem pakiety między commandami i przepływ zaczął działać.UsprawnieniaW międzyczasie zastąpiłem EventHandler odpowiadający za pobieranie danych z zewnętrznego serwisu Sagami, które wysyłają commandy do proxy-service, aby wyszukał podany tytuł.Ten asynchronizm uodpornił aplikację na niedostępność zewnętrznego serwisu lub długi czas odpowiedzi:  movie-service    @Sagapublic class MovieSaga {  ...  @StartSaga  @SagaEventHandler(associationProperty = &quot;movieId&quot;)  public void handle(MovieSearchDelegatedEvent event) {      ...      commandGateway.send(new FetchMovieDetailsCommand(proxyId, event.getSearchPhrase()));  }  @SagaEventHandler(associationProperty = &quot;proxyId&quot;)  @EndSaga  public void handle(MovieDetailsFetchedEvent event) {      ...      var movie = event.getExternalMovie();      if (MovieState.NOT_FOUND_IN_EXTERNAL_SERVICE == movie.getMovieState()) {          queryUpdateEmitter.emit(GetMovieQuery.class, query -&amp;gt; true, new MovieDTO(MovieState.NOT_FOUND_IN_EXTERNAL_SERVICE));          end();      } else {          commandGateway.send(new SaveMovieCommand(movieId, event.getExternalMovie()));      }  }  ...}        proxy-service    @Componentpublic class ProxyCommandHandler {  ...  @CommandHandler  public void handle(FetchMovieDetailsCommand command) {      ExternalMovie externalMovie;      try {          externalMovie = tmdbService.searchMovie(command.getSearchPhrase());      } catch (NotFoundInExternalServiceException e) {          externalMovie = ExternalMovie.builder().movieState(MovieState.NOT_FOUND_IN_EXTERNAL_SERVICE).build();      }      eventGateway.publish(new MovieDetailsFetchedEvent(command.getProxyId(), externalMovie));  }  ...}      Jako że trailers i cast dostały swój własny serwis i nie były już powiązane z agregatem filmu, musiałem przekonwertować je na samodzielne agregaty:@Aggregatepublic class TrailerAggregate {    @AggregateIdentifier    private String trailersId;    private List&amp;lt;Trailer&amp;gt; trailers;    ...}PodsumowaniePrzejście na architekturę mikroserwisów niewątpliwie daje wiele korzyści, jednak bez wyklarowanego dobrego podziału jest to mocno utrudnione.Axon sam w sobie sprzyja tej architekturze, a korzystając z gotowych narzędzi, można taką migrację przeprowadzić w relatywnie krótkim czasie.Cały kod znajduje się w moim repozytorium tutaj.Źródła  https://github.com/matty-matt/movie-keeper-core  https://axoniq.io/  https://youtu.be/zUSWsJteRfw?t=2179",
"url": "/2020/06/09/microservices-on-axon.html",
"author": "Mateusz Kociszewski",
"authorUrl": "/authors/mkociszewski.html",
"image": "mkociszewski.webp",
"highlight": "/assets/img/posts/2020-06-08-microservices-on-axon/axon.jpg",
"date": "09-06-2020",
"path": "pl/2020-06-08-microservices-on-axon"
}
,


"2020-06-01-na-co-nam-ta-chmura-html": {
"title": "Na co nam ta chmura?",
"lang": "pl",
"tags": "cloud google cloud platform gcp serverless",
"content": "Chmury obliczeniowe prężnie się rozwijają i zyskują coraz większą popularność. Wiele firm decyduje się na skorzystanie z produktów oferowanych przez Google, Amazon czy Microsoft.Z inicjatywy PKO Banku Polskiego powstaje Chmura Krajowa, która nawiązuje współpracę z Google. Google Cloud otwiera nowy region w Warszawie. 🔗W ostatnich latach bardzo dużo się dzieje w świecie Cloud Computing. Coraz częściej rezygnujemy z rozwiązań on-premise na rzecz platform chmurowych, migrujemy swoje infrastruktury do takich gigantów jak AWS, Microsoft Azure, czy Google Cloud. Ale dlaczego?Na co nam ta chmura?W tym wpisie przedstawię kilka powodów, dla których warto skorzystać z Google Cloud Platform, jednak wiele elementów pokrywa się również z innymi usługodawcami, czy ogólną ideą chmury.WydajnośćInfrastruktura Google Cloud Platform zapewnia nam wydajność kilkudziesięciu centrów danych oraz własnej sieci internetowej.Sieć Google jest odpowiedzialna za 40% ruchu całego internetu i jest największa siecią tego typu na świecie. Komunikacja między usługami Google Cloud odbywa się przez wewnętrzną infrastrukturę sieciową, co zapewnia bardzo wysoką przepustowość oraz niską latencję.Google Networkźródło: https://cloud.google.com - dostęp: 2020-06-01NiezawodnośćUsługi Google Cloud oferują wysoką dostępność na poziomie SLA powyżej 99%.Przykładowo dla Cloud Storage jest to (w zależności od klasy) od 99,0% do 99,95% SLA, a dla Compute Engine od 99,5% do 99,99% SLA.Aby zminimalizować przerwy w świadczeniu usług z powodu awarii sprzętu, klęsk żywiołowych lub innych incydentów, Google zbudował wysoce redundantną infrastrukturę. Jeśli awarii ulegnie nawet całe centrum danych, nasze dane i usługi będą nadal dostępne, ponieważ Google posiada centra danych rozmieszczone na całym świecie, a nasze dane są replikowane.Korzystając z chmury nie musimy więc przejmować się awariami sprzętu (np. wymianą dysków), zanikami energii elektrycznej, pożarami oraz fizycznymi włamaniami do serwerowni.BezpieczeństwoGoogle Cloud oferuje bezpieczeństwo, o które trudno w przypadku on-premise. Składa się na to wiele elementów, między innymi:      Bieżące aktualizacje bezpieczeństwa, które odbywają się bez przerw w dostępie do usługi;        Ponad 500 ekspertów bezpieczeństwa Google, w tym czołowych ekspertów na świecie, którzy pracują przez całą dobę, aby wcześniej wykryć zagrożenia i zareagować;        Szyfrowanie danych przesyłanych między Google, a klientami oraz między centrami danych, a także szyfrowanie danych zapisanych w chmurze;        Automatyczna rotacja kluczy (Cloud KMS);        Zarządzanie rolami i uprawnieniami (Cloud Identity and Access Management);        Zabezpieczenie przed atakami DDoS (Cloud Armor);        Bezpieczeństwo fizyczne centrów danych Google, które odwiedzić może jednie niewielka część pracowników firmy;        Wieloletnie doświadczenie zdobyte podczas ataków na usługi wyszukiwarki Google, Gmail oraz YouTube;        Wykorzystanie niestandardowego sprzętu oraz podpisów kryptograficznych na wszystkich niskopoziomowych komponentach (takich jak BIOS, Bootloader, Kernel).  Google posiada rozbudowaną warstwę bezpieczeństwa, która zabezpiecza infrastrukturę od sprzętu aż po system operacyjny. Nie wszystko jest jednak dostępne out-of-the-box.Musimy pamiętać, że niektóre elementy muszą zostać przez nas poprawnie skonfigurowane. Mam na myśli na przykład uprawnienia/role czy reguły firewall. Jeśli tego nie zrobimy, to może się okazać, że nieupoważnione osoby mogą mieć dostęp do naszego systemu przez internet.ElastycznośćJednym z największych atutów chmury jest jej elastyczność.Nie jesteśmy ograniczeni przez sprzęt, który posiadamy. Zasoby są dostępne na żądanie, więc jeśli przyjdzie potrzeba zwiększenia pamięci RAM czy wielkości dysku, to możemy to zrobić w każdej chwili.Wykorzystujemy tylko tyle zasobów, ile potrzebujemy w danej chwili. Jest to szczególnie ważne, jeśli nasza infrastruktura ma być gotowa na chwilowy, zwiększony ruch. W przeciwieństwie do on-premise, nasze zasoby nie marnują się w przypadku nie korzystania z nich.SkalowalnośćW przypadku usług w modelu SaaS, takich jak np. Cloud SQL czy Cloud Function, nie musimy się przejmować skalowaniem czy wydajnością. Google zapewni nam dostępność oraz wydajność usługi niezależnie od obciążenia. Nie musimy więc zajmować się klastrowaniem własnej bazy danych.Jeśli uruchomimy naszą aplikację w modelu PaaS, np. korzystając z usługi App Engine (w wersji standard), Google automatycznie będzie skalował ją w zależności od obciążenia. Większą kontrolę nad skalowaniem naszej aplikacji zapewni nam Kubernetes Engine, który jest hybrydą między IaaS a PaaS.Również w przypadku modelu IaaS, korzystając z maszyn wirtualnych Compute Engine, mamy możliwość automatycznego skalowania. W tym wypadku musimy jednak skonfigurować grupę instancji i ustalić minimalną/maksymalną liczbę maszyn wirtualnych oraz na jakiej podstawie usługa ma być skalowana (np. obciążenie CPU). W zamian otrzymujemy największą kontrolę nad tym, w jaki sposób odbywa się skalowanie.Obniżenie kosztówW przypadku modelu IaaS, jeśli zmigrujemy się do chmury strategią “Lift and Shift”, czyli mówiąc prościej, przeniesiemy nasze serwery na maszyny wirtualne Compute Engine, może okazać się, że miesięczny koszt infrastruktury będzie wyższy niż w przypadku on-premise.Sytuacja zmienia się w przypadku modeli PaaS czy SaaS, a także hybrydy w postaci Google Kubernetes Engine. Ponieważ zasoby w chmurze są dostępne na żądanie, to płacimy tylko za te, które faktycznie wykorzystujemy.Jeśli nasz system przyjmuje wzmożony ruch np. przez kilka dni w każdym miesiącu, to w tych dniach zostaną przydzielone dodatkowe zasoby. Gdy ruch spadnie, to zasoby zostaną zredukowane, dzięki czemu nie będziemy płacić za to, czego w danej chwili nie potrzebujemy.Google Cloud oferuje również zniżki:  Sustained use discounts - zniżki za uruchamianie określonych zasobów przez znaczną część miesiąca;  Committed use discounts - zniżki w ramach umowy z Google, jeśli zobowiążemy się do korzystania z zasobów przez określony czas.a także darmowe limity. 🔗Zaoszczędzimy również na utrzymaniu infrastruktury, ponieważ nie potrzebujemy już administratorów opiekujących się naszymi serwerami.Dzięki automatyzacji, jaką daje nam chmura, możemy być mniej DevOps, a bardziej NoOps.",
"url": "/2020/06/01/na-co-nam-ta-chmura.html",
"author": "Michał Hoja",
"authorUrl": "/authors/mhoja.html",
"image": "mhoja.jpg",
"highlight": "/assets/img/posts/2020-06-01-na-co-nam-ta-chmura/cloud.jpg",
"date": "01-06-2020",
"path": "pl/2020-06-01-na-co-nam-ta-chmura"
}
,


"2020-05-05-tworzenie-i-usuwanie-indeksow-w-bazie-mongodb-na-produkcji-html": {
"title": "Tworzenie i usuwanie indeksów w bazie MongoDB na produkcji",
"lang": "pl",
"tags": "tech mongodb index",
"content": "Rankingi baz danych pokazują, że już dobrych kilka lat wśród baz typu NoSQL króluje MongoDB i nic nie wskazuje na to, aby miało się to szybko zmienić. Trudno się temu dziwić - MongoDB jest dokumentową bazą danych, ale oferuje dużo więcej poza funkcjonalnościami dodawania i pobierania dokumentów.Dysponujemy bogatym zestawem narzędzi do zapytań w postaci mechanizmu agregacji. Mamy też możliwość nasłuchiwania na zmiany danych w bazie poprzez strumień zmian (change stream). Możemy również walidować schemat kolekcji poprzez dostarczony przez nas zestaw reguł. Wersja 4.0 MongoDB dostarcza transakcje obejmujące więcej niż jeden dokument. Już wcześniejszy mechanizm transakcji na poziomie pojedyneczego dokumentu pozwala na tworzenie produkcyjnych aplikacji przy wykorzystaniu funkcji typu znajdź i usuń, podmień oraz zaktualizuj (findOneAndDelete, findOneAndReplace, findOneAndUpdate). W tym przypadku musimy zaprojektować odpowiedni schemat bazy.Dzięki shardingowi MongoDB pozwala przechowywać i przetwarzać ogromne ilości danych bez znacznego spadku wydajności. Poza tym dzięki redundancji w klastrze (replica set) dane są niemalże zawsze dostępne. Co więcej, dokumentacja MongoDB wprost mówi, że klaster z redundacją danych powinien być podstawą każdej instalacji produkcyjnej.Każdy system jest rozwijany i zmienia się w odpowiedzi na nowe wymagania biznesowe. Z tego powodu może się zdarzyć, że konieczne będą zmiany w zapytaniach do bazy i co za tym idzie również mogą być wymagane zmiany indeksów. Staniemy wtedy przed wyzwaniem zmiany indeksów w systemie produkcyjnym. Może się zdarzyć, że jeden okaże się niepotrzebny i będziemy chcieli go usunąć, aby zwolnić miejsce. Drugi indeks trzeba będzie stworzyć, aby przyspieszyć wykonywanie nowych zapytań. Tworzenie indeksu może trwać bardzo długo. Nie chcemy na ten czas wyłączać produkcji. Poniższy tekst wyjaśnia, jak stworzyć nowy i usunąć stary indeks bez większych zawirowań na produkcji.Aby cała operacja odbyła się bez problemów, powinniśmy wykonać ją w następującej kolejności:  utworzenie indeksu pod nowe zapytania w systemie;  aktualizacja części biznesowej systemu;  usunięcie starego indeksu, o ile nie jest już używany.Tworzenie indeksu w MongoDB w sposób rollingTworzenie indeksu w sposób rolling dotyczy tylko i wyłącznie bazy z redundacją danych w klastrze. Ta metoda pozwala na tworzenie nieunikalnych indeksów. Przeprowadzana jest dla każdego węzła w klastrze:  węzeł jest odłączany od klastra i uruchamiany w trybie standalone;  tworzony jest indeks na tym węźle;  węzeł jest dołączany do klastra.W czasie tworzenia indeksu pozostałe węzły klastra cały czas działają produkcyjnie i co za tym idzie dane się zmieniają. Węzeł, który był uruchomiony w trybie standalone, i na którym tworzony był indeks, po włączeniu do klastra będzie musiał się zsynchronizować i pobrać wszytkie zmiany, które zaszły w czasie, kiedy był odłączony. Aby cała procedura odbyła się bez problemu, należy zapewnić, aby oplog miał odpowiedni rozmiar. Oplog przechowuje ostatnie zmiany danych, które zaszły w węźle primary i jeżeli nie będzie odpowiednio duży, to węzeł który jest dołączany do klastra, nie będzie w stanie się zsynchronizować. Więcej informacji o tym, jak dobrać rozmiar oplog jest w dokumentacji MongoDB.Procedurę zaczynamy od węzłów secondary, a na końcu przeprowadzamy ją dla węzła primary. Należy zwrócić uwagę na write concern. Przykładowo jeżeli mamy klaster złożony z 3 węzłów i write concern równe 3, to z pukntu widzenia aplikacji baza będzie niedostępna po odłączeniu węzła. Wszystkie zapisy będą się kończyć błędem.Odłączenie węzła od klastraOdłączenie węzła od klastra wymaga 2 kroków:  zmiany w konfiguracji, aby węzeł po restarcie nie podłączył się do klastra,  restartu węzła.W konfiguracji musimy zmienić port, wykomentować konfigurację repliki oraz wyłączyć odświeżanie sesji. Przykładowo fragment pliku z konfiguracją może wyglądać tak:net:  bindIp: 127.0.0.1  port: 28017#   port: 27017#replication:#   replSetName: rs0# Wyłącz odświeżanie sesjisetParameter:   disableLogicalSessionCacheRefresh: trueUwaga! Jeżeli węzeł jest primary, to musimy wykonać operację step down przed restartem, aby stał się secondary, a klaster wybrał inny węzeł jako primary. Służy temu polecenie rs.stepDown().Po zmianach w konfiguracji restartujemy węzeł. W tym momencie węzeł jest odłączony od klastra i działa w trybie standalone. Teraz możemy utworzyć indeks.Tworzenie indeksuPo restarcie węzła i uruchomieniu w trybie standalone, podłączamy się do niego i wykonujemy polecenie tworzące indeks zgodnie z dokumentacją. Tworzenie indeksu może okazać się czasochłonne. Aktualnie wykonywane operacje na bazie sprawdzimy, uruchamiając polecenie db.currentOp() w powłoce MongoDB. To pozwoli dowiedzieć się, czy indeks nadal się buduje.Dołączenie węzła do klastraPo zbudowaniu indeksu dołączamy węzeł do klastra. Polega to na wycofaniu zmian w konfiguracji oraz restarcie węzła. W tym momencie węzeł będzie się aktualizował względem klastra (węzła primary), czyli pobierał zmiany, które zaszły w czasie kiedy był odłączony.Informacje o klastrze są zwracane przez polecenia:  rs.status() - zwraca status klastra względem węzła, na którym polecenie zostało uruchomione;  rs.printSlaveReplicationInfo() - zwraca informacje o węzłach secondary łącznie z opóźnieniem względem węzła primary.Usuwanie indeksuProcedura usunięcia indeksu zależy od wersji MongoDB. Od wersji 4.2 wystarczy usunąć indeks na węźle primary. Całą resztą, czyli usunięciem indeksu z węzłów secondary, zajmie się baza. Jeżeli pracujemy z wcześniejszą wersją, to indeks należy usunąć podobnie jak tworzymy indeks w sposób rolling. Jedyną różnicą jest to, że indeks usuwamy, a nie tworzymy.W jednym i drugim przypadku musimy jednak się upewnić, że żadne zapytanie nie korzysta z danego indeksu. Inaczej zapytanie zakończy się błędem.Zarządzanie indeksami a automatyzacjaZazwyczaj chcemy ułatwić sobie życie i wiele rzeczy automatyzujemy. Można również się pokusić o automatyczne usuwanie i tworzenie indeksów podczas wdrażania kolejnej wersji systemu. Doświadczenie pokazuje, że można pochopnie umieścić w skryptach dwa polecenia usunięcia i stworzenia indeksu na węźle primary. Najprawdopodobniej skończy się to poważnym błędem i jeżeli operację przeprowadzamy na produkcji, to produkcja będzie niedostępna do czasu naprawy. Czas przestoju będzie znaczący, jeżeli kolekcja będzie znaczących rozmiarów.Zmiany w indeksach powinniśmy przeprowadzać manualnie z planem działania w ręku.PodsumowanieMongoDB jest dokumentową bazą danych oferującą bogatą funkcjonalność i jednocześnie na tyle elastyczną, że pozwala przeprowadzać zadania administracyjne bez przerwy w działaniu. To się tyczy również tworzenia oraz usuwania indeksów, co nie jest operacją trudną.",
"url": "/2020/05/05/tworzenie-i-usuwanie-indeksow-w-bazie-mongodb-na-produkcji.html",
"author": "Mikołaj Gucki",
"authorUrl": "/authors/mgucki.html",
"image": "mgucki.jpg",
"highlight": "/assets/img/posts/2020-05-05-tworzenie-i-usuwanie-indeksow-w-bazie-mongodb-na-produkcji/indeksy_mongo.jpg",
"date": "05-05-2020",
"path": "pl/2020-05-05-tworzenie-i-usuwanie-indeksow-w-bazie-mongodb-na-produkcji"
}
,


"2020-03-18-tomcat-model-przetwarzania-zadan-html": {
"title": "Tomcat - model przetwarzania żądań",
"lang": "pl",
"tags": "java tomcat nio connector",
"content": "Tomcat jest jednym z najpopularniejszych serwerów webowych dla aplikacji pisanych w Javie. Jest podstawowym kontenerem aplikacji springbootowych. Tworząc nowy projekt często polegamy na jego domyślnej konfiguracji. Kiedy projekt dojrzewa do wdrożenia produkcyjnego i musi zmierzyć się z obsługą dużego ruchu, często konieczne okazuje się dostrojenie tej konfiguracji. W tym artykule skupię się na konfiguracji connectorów na przykładzie pewnego problemu produkcyjnego. Opis konfiguracji Tomcata bazuje na wersji 7.x, ale jest w zasadzie aktualny również dla wyższych wersji.Problem z wyczerpaną pulą wątków w TomcacieW jednym z systemów, które współtworzyłem, wystąpił problem podczas działania produkcyjnego. Problem objawiał się brakiem możliwości połączenia z endpointem wystawionym na Tomcacie. Monitoring i analiza logów jednoznacznie pokazały, że wyczerpała się pula wątków obsługujących żądania http. Dalsza analiza ujawniła, że praprzyczyną problemu był w tym przypadku system autoryzacji, z którym łączył się nasz system. W systemie autoryzacji znacząco wzrosły czasy odpowiedzi, co powodowało, że wątki Tomcata były bardzo długo zajęte. Tomcat powoływał nowe wątki, ale ostatecznie osiągnął limit 1000 wątków (taki mieliśmy ustawiony na connectorze) i przestał obsługiwać nowe połączenia - również takie, które nie wymagały wywołania systemu autoryzacji.Co więcej, zauważyliśmy i potwierdziliśmy to później w testach, że po całkowitym wysyceniu puli wątków, Tomcat nie jest w stanie bez restartu powrócić do prawidłowego działania nawet wtedy, kiedy problem z długimi czasami odpowiedzi zostanie wyeliminowany.Opisana sytuacja skłoniła nas do sprawdzenia czy jesteśmy w stanie zapobiec takiemu zachowaniu Tomcata poprzez modyfikację konfiguracji connectorów. Pojawił się między innymi pomysł zmiany implementacji connectora blokującego (bio) na nieblokujący (nio). W systemie, którego dotyczył problem używany był tomcat 7.x, w którym domyślnie używana jest implementacja blokująca.Konfiguracja connectorówPodstawowa konfiguracja Tomcata znajduje się w pliku conf/server.xml i zawiera konfigurację następujących elementów:  Server - konfiguracja całego kontenera, zawiera konfiguracje poszczególnych serwisów.  Service - zawiera konfigurację poszczególnych connectorów łącząc je silnikiem przetwarzania żądań.  Connector - nasłuchuje na wybranym porcie http i obsługuje połączenia przekazując żądania do silnika zdefiniowanego w elemencie Engine.  Engine - przetwarza żądania pochodzące ze wszystkich connectorów zdefiniowanych w elemencie Service.Z punktu widzenia przetwarzania żądań, kluczowe znaczenie ma konfiguracja connectora i parametry:  protocol - Pozwala określić wybraną implementacje connectora (to tutaj możemy zdecydować czy chcemy użyć connectora blokującego czy nieblokującego).  acceptCount - Długość kolejki oczekujących połączeń. Kolejka napełnia się jeżeli wszystkie wątki w puli wątków connectora są zajęte. W przypadku osiągnięcia limitu tej kolejki (domyślnie 100) połączenia klientów będą odrzucane.  maxConnections - Maksymalna liczba połączeń, które mogą być przetwarzane. Po osiągnięciu maksymalnej liczby połączeń, połączenia nadal są przyjmowane i kolejkowane do czasu osiągnięcia limitu wynikającego z parametru acceptCount. Domyślnie maxConnections jest równe maxThreads dla connectora blokującego i 10000 dla nieblokującego.  maxThreads - Maksymalna liczba wątków obsługujących żądania. Parametr ten oznacza, ile żądań może być symultanicznie przetwarzanych przez serwer. Domyślna wartość tego parametru to 200. Jest to najważniejszy parametr, zważywszy na problem, który rozwiązujemy w tym artykule.Monitorowanie puli wątkówAktualna liczba wątków przetwarzających żądania jest, obok rozmiaru sterty, jednym z najważniejszych parametrów, które powinny być objęte monitoringiem. W tym przypadku monitorujemy atrybut currentThreadsBusy mbean’aCatalina:name=&quot;http-RODZAJ_CONNECTORA-PORT&quot;,type=ThreadPool(przy czym RODZAJ_CONNECTORA to “bio” dla connectora blokującego i “nio” dla nieblokującego, PORT to numer portu, na którym nasłuchuje connector).Do monitoringu można użyć narzędzia jolokia, które udostępnia JMX za pomocą protokołu http. W takiej konfiguracji wystarczy regularnie odpytywać o stan puli wątków poprzez wywołanie http GET:http://HOST:PORT/jolokia/read/Catalina:name=&quot;http-RODZAJ_CONNECTORA-PORT&quot;,type=ThreadPool/currentThreadsBusyW odpowiedzi dostajemy jsona, który w polu value zawiera aktualny rozmiar puli wątków{  &quot;request&quot;: {    &quot;mbean&quot;: &quot;Catalina:name=\\&quot;http-RODZAJ_CONNECTORA-PORT\\&quot;,type=ThreadPool&quot;,    &quot;attribute&quot;: &quot;currentThreadsBusy&quot;,    &quot;type&quot;: &quot;read&quot;  },  &quot;value&quot;: 1,  &quot;timestamp&quot;: 1564763927,  &quot;status&quot;: 200}Warto skonfigurować alerting, który będzie ostrzegał o wysycaniu się puli połączeń i pozwoli szybciej zdiagnozować problem.Porównanie connectorów blokujących (bio) i nieblokujących (nio)Różnica w działaniuW Tomcacie 7.x domyślną implementacją connectora jest implementacja blokująca. W Tomcacie 8 i wyższych domyślny jest connector nieblokujący. Podstawowa różnica w działaniu polega na tym, że w connectorze blokującym wątek przypisywany jest do połączenia, a w connectorze nieblokującym do pojedynczego żądania. Może to mieć duże znaczenie przy zastosowaniu połączeń keep-alive, kiedy to jedno połączenie jest wykorzystywane do przesłania wielu żądań i odpowiedzi. W takim modelu wątek na serwerze jest zajęty na cały czas trwania pojedynczego połączenia mimo, że faktyczne przetwarzanie na serwerze zachodzi tylko w obrębie pojedynczego żądania.TestDla porównania można wykonać prosty test na dwóch rodzajach connectorów. W teście klient raz na sekundę wysyła żądanie do serwera. Łącznie jeden klient wysyła 30 żądań. Uruchamiamy 20 klientów w ciągu 2 sekund. Jednocześnie monitorujemy rozmiar puli wątków.Tak wygląda wykres czasów odpowiedzi dla connectora blokującego:a tak dla connectora nieblokującego:Nie widać tutaj jakiejś specjalnej różnicy między działaniem connectora bio i nio. Zupełnie inaczej wyglądają natomiast wykresy liczby zajętych wątków w puli. Dla connectora blokującego liczba zajętych wątków podczas trwania testu przekracza 20. Czyli jest zgodna z liczbą klientów wysyłających żądania do serwera. Połączenia keep-alive mają timeout równy 5 sekund więc klient utrzymuje cały czas jedno połączenie do wszystkich żądań:Dla connectora nieblokującego żądania są obsługiwane w większości przez jeden wątek:Test pokazuje, że przy takim modelu przetwarzania żądań za pomocą connectora nieblokującego możemy uzyskać sporą oszczędność zasobów i lepiej zutylizować serwer.Użycie mod_proxy zmienia sytuacjęWarto zwrócić uwagę na to, że takie wyniki osiągamy w przypadku bezpośredniego połączenia pomiędzy klientem, a serwerem Tomcat. Zdarza się jednak, że mamy jeszcze warstwę pośrednią np. w postaci serwera Apache httpd i modułu mod_proxy. Domyślnie mod_proxy nie ma włączonej opcji keep-alive więc wszystkie połączenia do backendu są zamykane po obsłużeniu pojedynczego żądania.Test został przeprowadzony za pomocą narzędzia Gatling. Więcej o samym narzędziu można przeczytać w osobnym artykule na naszym blogu.Próba rozwiązania problemuPodsumowując: Wstępem do rozważań nad konfiguracją connectorów był problem z długimi czasami jakie wątki Tomcata spędzały na komunikacji z systemem autoryzacji. Czy zmiana connectora na nieblokujący pomogłaby w tej sytuacji? Niestety nie. Connector nieblokujący nie wiąże wątku z połączeniem, ale nadal wiąże go z pojedynczym żądaniem. W tym przypadku nadal wątki będą blokowane na długim wywołaniu systemu autoryzacji. Choć zastosowanie connectora nio z pewnością zaoszczędziłoby część zasobów maszyny, to w tym przypadku należałoby raczej rozważyć użycie jakiejś implementacji circuit breaker’a lub też zmienić model obsługi żądań na bardziej reaktywny (np. webflux).",
"url": "/2020/03/18/tomcat-model-przetwarzania-zadan.html",
"author": "Jakub Wilczewski",
"authorUrl": "/authors/jwilczewski.html",
"image": "jwilczewski.jpg",
"highlight": "/assets/img/posts/2020-03-18-tomcat-model-przetwarzania-zadan/Tomcat.jpeg",
"date": "18-03-2020",
"path": "pl/2020-03-18-tomcat-model-przetwarzania-zadan"
}
,


"2020-03-09-o-openid-connect-slow-kilka-html": {
"title": "O OpenID Connect słów kilka",
"lang": "pl",
"tags": "openid connect oauth 2 keycloak uwierzytelnianie użytkownika",
"content": "W nawiązaniu do mojego poprzedniego wpisu pt.:“Keycloak - uwierzytelnianie i autoryzacja użytkownika w aplikacji Angular/Spring Boot” 🔗chciałbym krótko opisać standard OpenID Connect, który został wykorzystany podczas logowania do aplikacji przy użyciu serwera uwierzytelniania Keycloak.WstępNajpopularniejszymi standardami wykorzystywanymi do uwierzytelniania/autoryzacji są OAuth 2.0, OpenID Connect oraz SAML.O OAuth 2.0 zostało już napisanych wiele artykułów, których nie ma sensu powielać. Jednak aby przedstawić OpenID Connect, musiałbym opisać OAuth 2.0 oraz JWT.W celu zapoznania się ze standardem OAuth 2.0 odeślę do artykułu pt.:“OAuth 2.0 – jak działa / jak testować / problemy bezpieczeństwa” 🔗autorstwa Marcina Pioska z portalu Sekurak, w którym została opisana terminologia, sposoby pozyskiwania tokenu oraz zasada działania standardu.O JWT natomiast możemy przeczytać w dokumencie RFC7519.Dlaczego więc do integracji naszej aplikacji z serwerem uwierzytelniania Keycloak użyliśmy OpenID Connect?Uwierzytelnianie a autoryzacjaPoruszając temat zabezpieczania zasobów i dostępu do nich, mówimy o takich pojęciach jak uwierzytelnianie (ang. authentication) oraz autoryzacja (ang. authorization).  uwierzytelnianie to proces polegający na potwierdzeniu tożsamości, czyli w skrócie - kim jestem?;  autoryzacja to proces nadawania uprawnień (dostępu do zasobu), czyli w skrócie - co mogę zrobić?.OAuth 2.0, według oficjalnej dokumentacji, nie powinien służyć do uwierzytelniania, a jedynie do autoryzacji (źródło 🔗):  OAuth 2.0 is not an authentication protocol.Jeśli potrzebujemy mechanizmu pozwalającego na poprawne zaimplementowanie uwierzytelniania, z pomocą przychodzi OpenID Connect.OpenID ConnectOpenID Connect jest prostą warstwą tożsamości opartą na OAuth 2.0.Umożliwia klientom weryfikację tożsamości użytkownika końcowego na podstawie uwierzytelnienia przeprowadzonego przez serwer autoryzacji, a także uzyskanie podstawowych informacji o jego profilu. Rozszerza OAuth 2.0, umożliwiając uwierzytelnianie stowarzyszone (ang. federated authentication):  Federated Authentication - użytkownik loguje się do serwisu Spotify przy użyciu konta na portalu Facebook (OpenID Connect);  Delegated Authorization - Spotify próbuje uzyskać dostęp do listy znajomych na Facebooku, aby zaimportować ją do swojej bazy danych (OAuth 2.0).Przepływ procesu jest podobny do OAuth 2.0, ale dodatkowo w procesie bierze udział ID Token.ID TokenID Token przypomina koncepcję dowodu osobistego w formacie JWT, podpisanego przez dostawcę OpenID (OP). Spełnia założenia standardu JWT, więc składa się z nagłówka, zawartości orazsygnatury. Jego zawartość może więc wyglądać następująco:{  &quot;jti&quot;: &quot;e78b5f41-e769-4353-8874-44302f4a17c3&quot;,  &quot;exp&quot;: 1583205992,  &quot;nbf&quot;: 0,  &quot;iat&quot;: 1583169992,  &quot;iss&quot;: &quot;http://localhost:8180/auth/realms/SpringBootAngular&quot;,  &quot;aud&quot;: &quot;developer&quot;,  &quot;sub&quot;: &quot;c3c3755a-e499-4782-b119-a19bede0ace8&quot;,  &quot;typ&quot;: &quot;Serialized-ID&quot;,  &quot;nonce&quot;: &quot;02c0b033-bfac-4030-a317-c18aec3cb2db&quot;,  &quot;auth_time&quot;: 1583169991,  &quot;acr&quot; : &quot;1&quot;,  &quot;session_state&quot;: &quot;8b66127b-4474-41bd-8e36-5d18286df73f&quot;,  &quot;state_checker&quot;: &quot;HZZmC4no-TEqiCf31Mk1MtONDqyxkk81ZXZCwANQb9Y&quot;,  &quot;name&quot;: &quot;Jan Nowak&quot;,  &quot;email&quot;: &quot;jan.nowak@example.pl&quot;}W skrócie, ID Token m.in.:  potwierdza tożsamość użytkownika, zwanego podmiotem w OpenID (sub);  określa organ wydający (iss);  jest generowany dla określonej grupy odbiorców - klienta (aud);  może zawierać losowy ciąg służący do identyfikowania pochodzenia żądania (nonce);  może określać kiedy (auth time) i jak, pod względem siły (acr) użytkownik został uwierzytelniony;  posiada znacznik czasu wydania (iat) oraz czas ważności (exp);  może zawierać dodatkowe informacje takie jak imię, nazwisko (name) oraz adres email (email);  jest podpisany cyfrowo, dzięki czemu może zostać zweryfikowany;  opcjonalnie może zostać zaszyfrowany w celu zapewnienia poufności danych.Więcej informacji na ten temat znajdziemy w oficjalnej dokumentacji 🔗.PodsumowanieDzięki wykorzystaniu ID Token, OpenID Connect nadaje się do uwierzytelniania użytkownika, w przeciwieństwie do OAuth 2.0, który najlepiej sprawdzi się podczas autoryzacji dwóch aplikacji komunikujących się między sobą przez API.Wiemy już, że Federated Authentication ma zastosowanie, kiedy użytkownik loguje się do serwisu przy użyciu wspólnego konta.OpenID Connect wydaje się więc być naturalnym kandydatem do uwierzytelniania użytkowników w różnego rodzaju serwisach społecznościowych czy aplikacjach internetowych (jak w przykładzie logowania do aplikacji za pośrednictwem Keycloaka).Delegated Authorization natomiast ma zastosowanie, kiedy klient (aplikacja) próbuje uzyskać dostęp do zasobów innej aplikacji. Użytkownik musi jedynie zaakceptować uprawnienia przyznawane aplikacji klienckiej, czyli np. odczyt listy znajomych na Facebooku, o który ubiega się Spotify.Tutaj do autoryzacji Spotify w Facebooku najlepiej spisze się OAuth 2.0.Standardy wymienione we wstępie można podsumować następująco:  OAuth 2.0 - stworzony w 2006 roku przy współpracy Twittera i Google, otwarty standard autoryzacji oparty o format JSON. Główne zastosowanie to autoryzacja API między dwoma aplikacjami;  OpenID Connect 1.0 - stworzony w 2014 roku przez OpenID Foundation, otwarty standard uwierzytelniania oparty o format JSON. Główne zastosowanie to SSO dla aplikacji konsumenckich;  SAML 2.0 - stworzony w 2001 roku przez OASIS, otwarty standard autoryzacji i uwierzytelniania oparty o format XML. Główne zastosowanie to SSO dla aplikacji enterprise.",
"url": "/2020/03/09/o-openid-connect-slow-kilka.html",
"author": "Michał Hoja",
"authorUrl": "/authors/mhoja.html",
"image": "mhoja.jpg",
"highlight": "/assets/img/posts/2020-03-09-o-openid-connect-slow-kilka/oauth.jpg",
"date": "09-03-2020",
"path": "pl/2020-03-09-o-openid-connect-slow-kilka"
}
,


"2020-02-14-angular-app-initializer-html": {
"title": "Angular APP_INITIALIZER",
"lang": "pl",
"tags": "Angular APP_INITIALIZER Application Initializer InjectionToken DI Dependency Injection ApplicationInitStatus",
"content": "APP_INITIALIZER to wbudowany w Angulara InjectionToken.Pod InjectionToken można zarejestrować wartość, funkcję albo serwis. Token ten można wstrzyknąć do komponentu lub serwisu.Przykład zdefiniowania MY_TOKEN:export const MY_TOKEN = new InjectionToken&amp;lt;string&amp;gt;(&#39;MY_TOKEN&#39;);Zarejestrowanie wartości Hello pod MY_TOKEN:@NgModule({// (...)providers: [{  provide: MY_TOKEN,  useValue: &#39;Hello&#39;,}]// (...)})export class AppModule { }Wstrzyknięcie wartości MY_TOKEN do serwisu oraz wyświetlenie w konsoli przeglądarki Hello:@Injectable()export class MyService {  constructor(@Inject(MY_TOKEN) public value: string) {    console.log(value);  }}Natomiast, dzięki tokenowi APP_INITIALIZER, możliwe jest wykonanie funkcji, lub zestawu funkcji, które zostaną wykonane przed uruchomieniem aplikacji (bootstraping).PrzykładProsty przykład wywołania dwóch funkcji przed startem aplikacji:export function appInit1() {  return () =&amp;gt; console.log(&#39;Hello from appInit1!&#39;);}export function appInit2() {  return () =&amp;gt; console.log(&#39;Hello from appInit2!&#39;);}Parametr multi pozwala na rejestrację dwóch lub więcej funkcji pod APP_INITIALIZER:@NgModule({// (...)providers: [{  provide: APP_INITIALIZER,  useFactory: appInit1,  multi: true},{  provide: APP_INITIALIZER,  useFactory: appInit2,  multi: true}],// (...)})export class AppModule { }W konsoli przeglądarki pojawią się poniższe komunikaty:Hello from appInit1!Hello from appInit2!Przykład z PromiseDo APP_INITIALIZER można także przekazać funkcję, która zwróci Promise! Angular poczeka, aż wszystkie zwrócone Promise’y zostaną rozwiązane (resolved).export function appInit() {  return () =&amp;gt; new Promise((resolve, reject) =&amp;gt; {    setTimeout(() =&amp;gt; {      console.log(&#39;Hello from appInit&#39;);      resolve();    }, 2000);  })}@NgModule({// (...)providers: [{  provide: APP_INITIALIZER,  useFactory: appInit,  multi: true}],// (...)})export class AppModule { }W rezultacie, po 2 sekundach od wywołania funkcji appInit, w konsoli zostanie wyświetlona wiadomość: Hello from appInit.Zaawansowany przykładDo funkcji uruchamianej przed bootstrapem aplikacji możliwe jest wstrzyknięcie serwisu.W poniższym przykładzie aplikacja frontendowa ściąga konfigurację wymaganą do poprawnego działania.Na backendzie wystawiony jest plik conf.json, serwowany przez http-server.Interface Configuration jest modelem danych z pliku conf.json, zawiera tylko pole name.Serwis AppInitService wywołuje żądanie typu GET na /api/conf.json.export interface Configuration {  name;}@Injectable()export class AppInitService {  constructor(private httpClient: HttpClient) {  }  init(): Promise&amp;lt;Configuration&amp;gt; {    return this.httpClient.get&amp;lt;Configuration&amp;gt;(&#39;api/conf.json&#39;)           .toPromise();  }}export function appInit(appInitService: AppInitService) {  return () =&amp;gt; appInitService.init().then(configuration =&amp;gt; console.log(configuration));}@NgModule({// (...)providers: [  AppInitService,  {    provide: APP_INITIALIZER,    useFactory: appInit,    deps: [AppInitService],    multi: true  }]// (...)})export class AppModule { }Powyższy kod wyświetli w konsoli konfigurację z pliku conf.json.{name: &quot;Test App name&quot;}Implementacja w AngularzePrzyjrzyjmy się teraz, w jaki sposób APP_INITIALIZER został zaimplementowany w samym Angularze.W pliku application_init.ts znajduje się definicja InjectionToken.export const APP_INITIALIZER = new InjectionToken&amp;lt;Array&amp;lt;() =&amp;gt; void&amp;gt;&amp;gt;(&#39;Application Initializer&#39;);Początek bootstrapowania aplikacji w Angular wygląda następująco:platformRef#bootstrapModuleFactory()return _callAndReportToErrorHandler(exceptionHandler, ngZone !, () =&amp;gt; {       const initStatus: ApplicationInitStatus = moduleRef.injector.get(ApplicationInitStatus);       initStatus.runInitializers(); // (1)       return initStatus.donePromise.then(() =&amp;gt; {         this._moduleDoBootstrap(moduleRef); // (2)         return moduleRef;       });W punkcie (1) w serwisie ApplicationInitStatus wywołana jest funkcja runInitializers. Po zakończeniu ApplicationInitStatus, Angular przeprowadza bootstrap komponentu.ApplicationInitStatus#runInitializers()  runInitializers() {    // (...)    const asyncInitPromises: Promise&amp;lt;any&amp;gt;[] = [];    if (this.appInits) {      for (let i = 0; i &amp;lt; this.appInits.length; i++) {        const initResult = this.appInits[i]();        if (isPromise(initResult)) {          asyncInitPromises.push(initResult);        }      }    }    Promise.all(asyncInitPromises).then(() =&amp;gt; { complete(); }).catch(e =&amp;gt; { this.reject(e); });    // (...)  }Metoda runInitializers sprawdza wywołania, które zwróciły Promise i czeka, aż wszystkie funkcje zostaną zakończone (resolve).ZastosowaniaDo czego można zastosować APP_INITIALIZER?  obsługa powiadomień push z serwera (comety),  pobranie konfiguracji np. CSRF token,  monitorowanie aktywności użytkownika,  keep alive,  keycloak - polecam świetny wpis Michała Hoji na ten temat źródło.Nawet, jeżeli w swojej aplikacji nie używamy APP_INITIALIZER, sam Angular wykorzystuje go do poprawnego działania.Przykłady użycia w Angularze:  RouterModule, używany jest do poprawnej pracy Guardów;  WorkerAppModule;  ServiceWorkerModule;  NgProbe. Angular 9 wprowadza nowy renderer Ivy tym samym mechanizm NgProbe przestanie działać.",
"url": "/2020/02/14/angular-app-initializer.html",
"author": "Dorian Mejer",
"authorUrl": "/authors/dmejer.html",
"image": "dmejer.jpg",
"highlight": "/assets/img/posts/2020-02-14-angular-app-initializer/app_initializer.png",
"date": "14-02-2020",
"path": "pl/2020-02-14-angular-app-initializer"
}
,


"2020-02-01-keycloak-user-authentication-authorization-springboot-angular-html": {
"title": "Keycloak - uwierzytelnianie i autoryzacja użytkownika w aplikacji Angular/Spring Boot",
"lang": "pl",
"tags": "angular spring boot keycloak keycloak-spring-boot-2-starter keycloak-spring-boot-2-adapter keycloak-angular uwierzytelnianie użytkownika",
"content": "Jak wykorzystać serwer Keycloak do logowania w aplikacji?Omówimy to na przykładzie gotowego projektu, który umożliwia użytkownikowi zalogowanie się do aplikacji z poziomu przeglądarki internetowej.Czym jest Keycloak?Keycloak to serwer uwierzytelniania i autoryzacji na licencji open-source. Może zostać podłączony do LDAP/AD lub uwierzytelniać użytkowników przy użyciu Google, Facebooka itd.Posiada również konsolę administracyjną, w której możemy łatwo skonfigurować chociażby uprawnienia użytkowników.Więcej informacji na stronie www.keycloak.org.Projekt demoProjekt aplikacji znajduję się na GitHubie - link do repozytorium.Składa się on z aplikacji backendowej napisanej w Spring Boot (Kotlin) oraz frontendowej napisanej w Angular 8.Aby przedstawić sposób zaimplementowania uwierzytelniania użytkownika za pomocą serwera Keycloak, zabezpieczymy endpointy na backendzie oraz osobno część frontendową.Serwer KeycloakWykorzystamy serwer Keycloak uruchomiony na Dockerze. Wykonując w głównym folderze repozytorium polecenie:docker run --rm --name keycloak-server -p 8180:8080 \\    -e KEYCLOAK_USER=admin \\    -e KEYCLOAK_PASSWORD=admin \\    -e KEYCLOAK_IMPORT=/realm/realm-export.json \\    -v $(pwd)/realm/:/realm/ \\    jboss/keycloakuruchomimy serwer Keycloak na porcie 8180.Konsola administracyjna będzie dostępna pod adresem: 127.0.0.1:8180/auth/adminZalogujemy się do niej przy użyciu loginu i hasła zdefiniowanego w KEYCLOAK_USER oraz KEYCLOAK_PASSWORD, w naszym przypadku admin:admin.Konfiguracja serweraSama konfiguracja serwera Keycloak, to temat na osobny wpis, dlatego wykorzystamy przygotowaną wcześniej konfigurację.Eksport konfiguracji serwera został umieszczony w repozytorium projektu demo (realm/realm-export.json) i zostanie załadowany podczas uruchomienia serwera.Skonfigurowane zostały:  realm SpringBootAngular  client SpringBootAngularClient          Access Type: confidential - podczas logowania musimy przekazać secret skonfigurowany w SpringBootAngularClient                  dostępnę są jeszcze opcje bearer-only oraz public - więcej na ten temat w dokumentacji (link)                    Valid Redirect URIs: * - dla ułatwienia bez ograniczeń      Web Origins: * - dla ułatwienia bez ograniczeń        rola user_role - tylko użytkownik z tą rolą będzie mógł się autoryzować w aplikacjach  użytkownik user z rolą user_role (user:password)  użytkownik user2 bez roli user_role (user2:password2)Aplikacja backendowaAplikacja backendowa składa się z dwóch kontrolerów:  MainController - wystawia zabezpieczone endpointy          /api/hello - zwraca string Hello from the Backend!      /api/logout - wylogowuje nas po stronie aplikacji backendowej        KeycloakController - wystawia publiczny endpoint          api/keycloak/config - udostępnia wspólną konfigurację dla frontendu      Aplikacja frontendowaAplikacja frontendowa składa się z trzech komponentów:  PublicComponent - dostępny dla wszystkich bez logowania  ProtectedComponent - zabezpieczony przed nieautoryzowanym użytkownikiem  ToolbarComponent - menu z przyciskami dla wygodyUwierzytelnianie na backendzieZależnościNa backendzie wykorzystamy zależności:  org.keycloak:keycloak-adapter-core  org.keycloak:keycloak-spring-boot-2-adapter  org.keycloak:keycloak-tomcat-adapterMoglibyśmy wykorzystać jedną zależność, zawierającą powyższe adaptery:  org.keycloak:keycloak-spring-boot-2-starterjednak w najnowszej dostępnej wersji 4.0.0.Final używa starszych wersji adapterów. Jedną ze zmian w nowszych wersjach adapterów, jest poprawiona walidacja tokenów. Jeśli w przyszłości pojawi się nowa wersja zależności, z poprawionymi adapterami, to nic nie będzie stało na przeszkodzie żeby z niej skorzystać.KonfiguracjaOprócz konfiguracji portu aplikacji oraz poziomu logowania adapterów (dzięki czemu zobaczymy w logach co się dokładnie dzieje), musimy skonfigurować keycloak-adapter-core.Dzięki keycloak-spring-boot-2-adapter możemy wszystko skonfigurować w application.yml:keycloakRequiredUserRole: user_rolekeycloak:  enabled: true  auth-server-url: http://localhost:8180/auth  realm: SpringBootAngular  resource: SpringBootAngularClient  security-constraints:    - authRoles:        - ${keycloakRequiredUserRole}      securityCollections:        - name: protected resource          patterns:            - /api/*    - securityCollections:        - name: public resource          patterns:            - /api/keycloak/config  credentials:    secret: &quot;&amp;lt;SECRET&amp;gt;&quot;  realm-key: &quot;&amp;lt;PUBLIC_KEY&amp;gt;&quot;  keycloakRequiredUserRole - dla ułatwienia, ponieważ wykorzystamy tylko jedną rolę, będzie nam łatwiej udostępnić ją przez api (jeśli chcemy wykorzystać więcej ról, to musimy je wyciągąć z listy authRoles  keycloak.security-constraints - tutaj definiujemy ograniczenia endpointówMożemy zdefiniować ścieżki dostępne publicznie:np. /api/keycloak/configoraz ścieżki które będą wymagały uprawnieńnp. /api/*które będzie wymagać od użytkownika roli ${keycloakRequiredUserRole} (czyli user_role).Konfiguracja Keycloak:  keycloak.enabled - umożliwi nam łatwe wyłączenie uwierzytelniania;  keycloak.auth-server-url - adres serwera Keycloak;  keycloak.realm - nazwa realmu;  keycloak.resource - nazwa klienta skonfigurowanego dla podanego realmu;  keycloak.credentials.secret - secret wygenerowany w SpringBootAngularClient, możemy go znaleźć w konsoli administracyjnej (Clients &amp;gt; SpringBootAngularClient &amp;gt; Credentials &amp;gt; Secret);  keycloak.realm-key - klucz publiczny realmu, możemy go znaleźć w konsoli administracyjnej (Realm Settings &amp;gt; Keys &amp;gt; Active &amp;gt; RSA &amp;gt; Public Key).Wystawienie konfiguracji dla frontenduPonieważ zabezpieczamy osobno aplikację backendową jak i frontendową, a konfiguracja Keycloak jest taka sama, to możemy wystawić konfigurację z application.yml przez api. Dzięki temu unikniemy duplikowania konfiguracji w aplikacji frontendowej.Konfigurację wystawimy w KeycloakController pod adresem api/keycloak/config. Będzie ona miała postać:{    &quot;enabled&quot;: true,    &quot;authServerUrl&quot;: &quot;http://localhost:8180/auth&quot;,    &quot;realm&quot;: &quot;SpringBootAngular&quot;,    &quot;resource&quot;: &quot;SpringBootAngularClient&quot;,    &quot;requiredUserRole&quot;: &quot;user_role&quot;,    &quot;credentials&quot;: {        &quot;secret&quot;: &quot;&amp;lt;SECRET&amp;gt;&quot;    }}Uwierzytelnianie na frontendzieZależnościPo stronie aplikacji frontendowej wykorzystamy bibliotekę keycloak-angular (link).W package.json dodamy zależności:&quot;keycloak-angular&quot;: &quot;^7.0.1&quot;,&quot;keycloak-js&quot;: &quot;^6.0.1&quot;Pobranie konfiguracji z backenduKonfigurację pobierzemy uderzając na endpoint backendu.Przy pierwszym pobraniu konfiguracji z KeycloakConfigService zostanie wykonany request, a wynik zostanie zapisany. Kolejne pobrania konfiguracji będą już zwracać zapisaną konfigurację.@Injectable({providedIn: &#39;root&#39;})export class KeycloakConfigService {    private config: KeycloakConfig;    constructor(private http: HttpClient) {    }    getConfig(): Observable&amp;lt;KeycloakConfig&amp;gt; {        if (this.config) {            return of(this.config);        } else {            const configObservable = this.http.get&amp;lt;KeycloakConfig&amp;gt;(&#39;/api/keycloak/config&#39;);            configObservable.subscribe(config =&amp;gt; this.config = config);            return configObservable;        }    }}KonfiguracjaPobraną konfigurację wykorzystamy w injection tokenie, definiując w app.module.ts provider dla tokenu APP_INITIALIZER:{    provide: APP_INITIALIZER,    useFactory: initializer,    multi: true,    deps: [KeycloakService, KeycloakConfigService]}Jako initializer utworzymy funkcję wykorzystującą nasz KeycloakConfigService oraz inicjalizującą KeycloakService z biblioteki keycloak-angular:export function initializer(keycloakService: KeycloakService, keycloakConfigService: KeycloakConfigService): () =&amp;gt; Promise&amp;lt;boolean&amp;gt; {    return (): Promise&amp;lt;boolean&amp;gt; =&amp;gt; keycloakConfigService.getConfig()        .pipe(            filter(config =&amp;gt; config.enabled),            flatMap(config =&amp;gt; {                return keycloakService.init({                    config: {                        url: config.authServerUrl,                        realm: config.realm,                        clientId: config.resource,                        credentials: {                            secret: config.credentials.secret                        }                    },                    initOptions: {                        onLoad: &#39;check-sso&#39;,                        checkLoginIframe: false                    }                });            })).toPromise();}Zabezpieczenie route’ówAby zabezpieczyć routy, wykorzystamy Angular Route Guard.Stworzymy AppAuthGuard rozszerzając KeycloakAuthGuard oraz implementując canActivate.W konstruktorze pobierzemy konfigurację z KeycloakConfigService, aby zweryfikować czy uwierzytelnianie jest włączone (parametr keycloak.enabled w application.yml), oraz czy użytkownik posiada wymaganą rolę:@Injectable({    providedIn: &#39;root&#39;})export class AppAuthGuard extends KeycloakAuthGuard {    isAuthEnabled: boolean = true;    requiredUserRole: string;    constructor(protected router: Router, protected keycloakService: KeycloakService, private keycloakConfigService: KeycloakConfigService) {        super(router, keycloakService);        this.keycloakConfigService.getConfig().subscribe(config =&amp;gt; {            this.isAuthEnabled = config.enabled;            this.requiredUserRole = config.requiredUserRole;        });    }    canActivate(route: ActivatedRouteSnapshot, state: RouterStateSnapshot): Promise&amp;lt;boolean&amp;gt; {        if (!this.isAuthEnabled) {            return Promise.resolve(true);        } else {            return super.canActivate(route, state) as Promise&amp;lt;boolean&amp;gt;;        }    }    isAccessAllowed(route: ActivatedRouteSnapshot, state: RouterStateSnapshot): Promise&amp;lt;boolean&amp;gt; {        return new Promise(async (resolve) =&amp;gt; {            if (!this.authenticated) {                return this.keycloakService.login();            }            resolve(this.roles.includes(this.requiredUserRole));        });    }}a następnie wykorzystamy naszego guarda w app-routing.module.ts do zabezpieczenia routa protected:const routes: Routes = [    {        path: &#39;&#39;,        component: PublicComponent    },    {        path: &#39;protected&#39;,        component: ProtectedComponent,        canActivate: [AppAuthGuard]    }];TestyUruchomienieJeśli mamy już uruchomiony serwer Keycloak, możemy zbudować aplikacje i obraz dockerowy, a następnie uruchomić kontener.W tym celu w głównym folderze repozytorium najpierw budujemy aplikacje:./gradlew buildpo zbudowaniu tworzymy obraz dockerowy:docker build -t michuu93/spring-angular-keycloak-demo .a na koniec uruchamiamy kontener:docker run --rm --name spring-angular-keycloak-demo --network host michuu93/spring-angular-keycloak-demoPonieważ zarówno aplikacje z wnętrza kontenera, jak i my z przeglądarki internetowej musimy mieć dostęp do serwera Keycloak pod tym samym adresem, nie wnikając w zasadę działania sieci w dockerze uruchomiliśmy kontener z opcją --network host.Dzięki temu nie musimy wystawiać dodatkowo żadnego proxy ani podmieniać hostów w naszym systemie. Jest to jednak ułatwienie na potrzeby demo i nie powinniśmy go wykorzystywać na produkcji.Weryfikacja działania uwierzytelnianiaPo uruchomieniu możemy sprawdzić aplikacje w działaniu!Przechodzimy na stronę localhost:9082 i powinniśmy zobaczyć aplikację frontendową, a dokładniej ToolbarComponent oraz PublicComponent:Do dyspozycji mamy menu, z którego możemy przejść do:  Public - niezabezpieczonego route z PublicComponent;  Protected - zabezpieczonego route z ProtectedComponent;  Keycloak Configuration - niezabezpieczonego endpointu udostępniającego konfiguację Keycloak z backendu;  Backend Hello - zabezpieczonego endpointu zwracającego tekst Hello from the Backend!;  Logout (backend) - wylogowania z backendu;  Logout (frontend) - przycisk widoczny tylko po zalogowaniu na frontendzie, który wylogowuje nas z frontendu.Sprawdźmy więc czy mamy dostęp do Keycloak Configuration bez zalogowania:Wygląda na to, że publicznie dostępny route i endpoint api, działają poprawnie.Jeśli teraz przejdziemy do Protected, zostaniemy przekierowani na stronę logowania Keycloak:http://localhost:8180/auth/realms/SpringBootAngular/protocol/openid-connect/auth?client_id=SpringBootAngularClient&amp;amp;redirect_uri=http%3A%2F%2Flocalhost%3A9082%2F%23%2F&amp;amp;state=3e007783-4772-48dd-8b31-4bfe3cc9c42c&amp;amp;response_mode=fragment&amp;amp;response_type=code&amp;amp;scope=openid&amp;amp;nonce=023ab545-32dc-4bdc-9cbf-12cad1d0c944Jak widzimy, adres zawiera informacje, takie jak nazwa protokołu, realm, client czy adres, na który mamy zostać przekierowani po zalogowaniu.Jeśli zalogujemy się użytkownikiem posiadającym rolę user_role (user:password), to zostaniemy przekierowani z powrotem do naszej aplikacji:Dodatkowo mamy jeszcze tylko przycisk Logout (frontend). Jesteśmy teraz zalogowani, więc możemy przejść do Protected:Zadziała również Backend Hello:Jeśli wylogujemy się teraz z frontendu - Logout (frontend), to zniknie przycisk Logout (frontend) i nie będziemy mieli już dostępu do Protected.Cały czas jednak będziemy mieli dostęp do Backend Hello. Dopiero kiedy wylogujemy się z backendu - Logout (backend), to stracimy dostęp do Backend Hello.Jednak jeśli będąc zalogowanym użytkownikiem, wylogujemy się z backendu a nie frontendu, to automatycznie zostaniemy również wylogowani z frontendu.Dlaczego?O tym jak działa Keycloak i tokeny, którymi się posługujemy (a dokładniej standard OpenID Connect) nie jest tematem tego wpisu. Co do kwestii wylogowywania, to temat (ze względu na złożoność implementacji) nadaje się na osobny wpis, szczególnie gdy mówimy o rozproszonych systemach z wieloma instancjami aplikacji, stojącymi za loadbalancerem.Projekt demo posiada bardzo prosty mechanizm wylogowywania.Wylogowanie z frontendu wywoła metodę logout na serwisie biblioteki keycloak-angular:logout = async (): Promise&amp;lt;void&amp;gt; =&amp;gt; await this.keycloakService.logout();Natomiast wylogowanie z backendu uderzy na /api/logout, co spowoduje wykonanie na backendzie:fun logout(request: HttpServletRequest, response: HttpServletResponse) {    request.logout()    response.sendRedirect(&quot;/&quot;)}Na koniec sprawdźmy jeszcze użytkownika bez roli user_role (user2:password2).Po zalogowaniu, w aplikacji powinniśmy zobaczyć te same przyciski w menu, co na użytkowniku posiadającym wymaganą rolę.Jest tak, ponieważ ukrywanie przycisków uzależniliśmy tylko od tego, czy użytkownik jest zalogowany, a nie czy ma uprawnienia do routa:ngOnInit(): void {    this.keycloakService.isLoggedIn().then(isLogged =&amp;gt; this.isLogged = isLogged);}Uderzenie na Backend Hello również nie zadziała.Uwierzytelnianie możemy bardzo szybko wyłączyć w obu aplikacjach, ustawiając keycloak.enabled w application.yml na false.Może być to przydatne np. na środowiskach testowych.Na zakończenieMam nadzieję, że udało mi się w przystępny sposób przedstawić integrację aplikacji z serwerem Keycloak.Nie jest to jedyny sposób implementacji, jeśli nasza aplikacja ma być bardziej elastyczna, możemy wykorzystać implementację standardu OAuth2 przy użyciu Spring Security. Dzięki temu nie uzależnimy się od serwera Keycloak i w przyszłości będziemy mogli łatwiej zamienić go na inny serwer uwierzytelniania/autoryzacji.Keycloak wystawia endpointy pod którymi udostępnia konfigurację (link).Znajdziemy je w Realm Settings &amp;gt; General &amp;gt; Endpoints, np. dla standardu OpenID Connect będzie to w naszym przypadku:http://127.0.0.1:8180/auth/realms/SpringBootAngular/.well-known/openid-configurationMogą być one przydatne, jeśli aplikacja nie wykorzystuje adapterów Keycloak do połączenia z serwerem.Nie musimy też wykorzystywać logowania do aplikacji przez przeglądarkę - możemy wykorzystać Keycloaka do uwierzytelniania aplikacji między sobą, np. kiedy integrujemy ze sobą różne moduły. Wtedy przydatna będzie opcja bearer-only w konfiguracji clienta.",
"url": "/2020/02/01/keycloak-user-authentication-authorization-springboot-angular.html",
"author": "Michał Hoja",
"authorUrl": "/authors/mhoja.html",
"image": "mhoja.jpg",
"highlight": "/assets/img/posts/2020-02-01-keycloak-uwierzytelnianie-autoryzacja-springboot-angular/Keycloak.png",
"date": "01-02-2020",
"path": "pl/2020-02-01-keycloak-user-authentication-authorization-springboot-angular"
}
,


"2020-02-01-keycloak-user-authentication-and-authorization-springboot-angular-html": {
"title": "Keycloak - user authentication and authorization in Angular/Spring Boot application",
"lang": "en",
"tags": "angular spring boot keycloak keycloak-spring-boot-2-starter keycloak-spring-boot-2-adapter keycloak-angular uwierzytelnianie użytkownika",
"content": "Keycloak - user authentication and authorization in Angular/Spring Boot applicationIn this article I will demonstrate using a Keycloak server to log into an application on the example of a ready-made project which enables logging in from the browser level.What is Keycloak?Keycloak is an open source authentication and authorization server. It can connect to the LDAP/AD or authenticate users via Google, Facebook, etc. It also features an admin console where you can easily configure user permissions and other parameters.Find out more at www.keycloak.org.Demo projectThe application is available on GitHub.It consists of a back-end part written in Spring Boot (Kotlin) and a front-end application part in Angular 8.For the purpose of demonstrating the implementation of user authentication with a Keycloak server, let’s secure the endpoints on the backend and the front-end part separately.Keycloak serverThe Keycloak server will be run in Docker. Execute the following command in the main repo directory to run an instance of a Keycloak server on port 8180:docker run --rm --name keycloak-server -p 8180:8080 \\  -e KEYCLOAK_USER=admin \\  -e KEYCLOAK_PASSWORD=admin \\  -e KEYCLOAK_IMPORT=/realm/realm-export.json \\  -v $(pwd)/realm/:/realm/ \\  jboss/keycloakThe admin console will be available at: 127.0.0.1:8180/auth/adminLog in to the admin console using the login and password defined in KEYCLOAK_USER and KEYCLOAK_PASSWORD, in this case admin:admin.Server configurationKeycloak server configuration deserves a separate article so let’s use a pre-made configuration here.The server configuration export has been located in the demo project’s repository (realm/realm-export.json) and will be loaded at server start.** The following parameters have been configured: **  realm SpringBootAngular  client SpringBootAngularClient          Access Type: confidential - when logging in you need to pass the secret configured in SpringBootAngularClient                  also available are bearer-only and public options - more about that can be found in the  documentation                    Valid Redirect URIs: * - no limitations for convenience      Web Origins: * - no limitations for convenience        user_role - only a user with this role will be able to authorize themselves in the applications  user with user_role (user:password)  user2 without user_role (user2:password2)Backend applicationThe backend application consists of two controllers:  MainController - exposes the secured endpoints          /api/hello - returns the Hello from the Backend! string      /api/logout - logs the user out of the backend application        KeycloakController - exposes a public endpoint          api/keycloak/config - exposes a shared configuration to the frontend      Frontend applicationThe frontend application consist of three components:  PublicComponent - accessible to everyone without logging in  ProtectedComponent - protected from unauthorized users  ToolbarComponent - menu with buttons for convenienceAuthentication on backendDependenciesThe following dependencies will be used on the backend:  org.keycloak:keycloak-adapter-cores  org.keycloak:keycloak-spring-boot-2-adapter  org.keycloak:keycloak-tomcat-adapterOne dependency containing the following adapters could be used:  org.keycloak:keycloak-spring-boot-2-starter couldHowever, the latest available version, 4.0.0.Final, uses older adapter versions. One of the changes in newer adapter versions is improved token validation. If a new version of the dependencies with improved adapters comes out in the future, it will be possible to useit as well.ConfigurationBesides configuring the application port and adapter login level (which lets you see what exactly is going on in the logs), you need to configure keycloak-adapter-core.Thanks to keycloak-spring-boot-2-adapter everything can be configured in application.yml:keycloakRequiredUserRole: user_rolekeycloak:  enabled: true  auth-server-url: http://localhost:8180/auth  realm: SpringBootAngular  resource: SpringBootAngularClient  security-constraints:    - authRoles:        - ${keycloakRequiredUserRole}      securityCollections:        - name: protected resource          patterns:            - /api/*    - securityCollections:        - name: public resource          patterns:            - /api/keycloak/config  credentials:    secret: &quot;&amp;lt;SECRET&amp;gt;&quot;  realm-key: &quot;&amp;lt;PUBLIC_KEY&amp;gt;&quot;  keycloakRequiredUserRole - since only one role will be used, it will be easier to make it accessible via API (if you want to use more roles, you need to extract them from the authRoles list)  keycloak.security-constraints - you define endpoint limitations hereYou can define paths that are public:e.g. /api/keycloak/configand paths which will require permissionse.g. /api/*which will require ${keycloakRequiredUserRole} from the user (that is user_role).Keycloak configuration:  keycloak.enabled - allows you to easily switch off authentication;  keycloak.auth-server-url - Keycloak server address;  keycloak.realm - realm name;  keycloak.resource - name of the client configured for the given realm;  keycloak.credentials.secret - secret generated in SpringBootAngularClient, you can find it in admin console (Clients &amp;gt; SpringBootAngularClient &amp;gt; Credentials &amp;gt; Secret);  keycloak.realm-key - realm’s public key, you can find it in the admin console (Realm Settings &amp;gt; Keys &amp;gt; Active &amp;gt; RSA &amp;gt; Public Key).Exposing configuration to frontendSince the backend and frontend applications are secured separately, and the Keycloak configuration remains the same, the configuration can be exposed from application.yml via API. This way you can avoid duplicating the configuration in the frontend application.Expose the configuration in KeycloakController at the following address: api/keycloak/config. It will look like that:{    &quot;enabled&quot;: true,    &quot;authServerUrl&quot;: &quot;http://localhost:8180/auth&quot;,    &quot;realm&quot;: &quot;SpringBootAngular&quot;,    &quot;resource&quot;: &quot;SpringBootAngularClient&quot;,    &quot;requiredUserRole&quot;: &quot;user_role&quot;,    &quot;credentials&quot;: {        &quot;secret&quot;: &quot;&amp;lt;SECRET&amp;gt;&quot;    }}Authentication on the frontendDependenciesThe keycloak-angular library will be used on the frontend link.Add dependencies in package.json:&quot;keycloak-angular&quot;: &quot;^7.0.1&quot;,&quot;keycloak-js&quot;: &quot;^6.0.1&quot;Downloading configuration from backendThe configuration can be downloaded by calling the backend endpoint.The first configuration download from KeycloakConfigService will execute a request and the result will be saved. Subsequent downloads will return the saved configuration.@Injectable({providedIn: &#39;root&#39;})export class KeycloakConfigService {    private config: KeycloakConfig;    constructor(private http: HttpClient) {    }    getConfig(): Observable&amp;lt;KeycloakConfig&amp;gt; {        if (this.config) {            return of(this.config);        } else {            const configObservable = this.http.get&amp;lt;KeycloakConfig&amp;gt;(&#39;/api/keycloak/config&#39;);            configObservable.subscribe(config =&amp;gt; this.config = config);            return configObservable;        }    }}ConfigurationUse the downloaded configuration in the injection token, defining the provider for the APP_INITIALIZER token in app.module.ts:{    provide: APP_INITIALIZER,    useFactory: initializer,    multi: true,    deps: [KeycloakService, KeycloakConfigService]}Create a function using the KeycloakConfigService that initializes KeycloakService from the keycloak-angular library:export function initializer(keycloakService: KeycloakService, keycloakConfigService: KeycloakConfigService): () =&amp;gt; Promise&amp;lt;boolean&amp;gt; {    return (): Promise&amp;lt;boolean&amp;gt; =&amp;gt; keycloakConfigService.getConfig()        .pipe(            filter(config =&amp;gt; config.enabled),            flatMap(config =&amp;gt; {                return keycloakService.init({                    config: {                        url: config.authServerUrl,                        realm: config.realm,                        clientId: config.resource,                        credentials: {                            secret: config.credentials.secret                        }                    },                    initOptions: {                        onLoad: &#39;check-sso&#39;,                        checkLoginIframe: false                    }                });            })).toPromise();}Securing routesIn order to secure routes, use Angular Route Guard.Create AppAuthGuard by extending KeycloakAuthGuard and implementing canActivate.Download the configuration from KeycloakConfigService in the constructor to verify if authentication is enabled (the keycloak.enabled parameter in application.yml), and check if the user has the required role:@Injectable({    providedIn: &#39;root&#39;})export class AppAuthGuard extends KeycloakAuthGuard {    isAuthEnabled: boolean = true;    requiredUserRole: string;    constructor(protected router: Router, protected keycloakService: KeycloakService, private keycloakConfigService: KeycloakConfigService) {        super(router, keycloakService);        this.keycloakConfigService.getConfig().subscribe(config =&amp;gt; {            this.isAuthEnabled = config.enabled;            this.requiredUserRole = config.requiredUserRole;        });    }    canActivate(route: ActivatedRouteSnapshot, state: RouterStateSnapshot): Promise&amp;lt;boolean&amp;gt; {        if (!this.isAuthEnabled) {            return Promise.resolve(true);        } else {            return super.canActivate(route, state) as Promise&amp;lt;boolean&amp;gt;;        }    }    isAccessAllowed(route: ActivatedRouteSnapshot, state: RouterStateSnapshot): Promise&amp;lt;boolean&amp;gt; {        return new Promise(async (resolve) =&amp;gt; {            if (!this.authenticated) {                return this.keycloakService.login();            }            resolve(this.roles.includes(this.requiredUserRole));        });    }}Next, use the guard in w app-routing.module.ts to secure the protected route:const routes: Routes = [    {        path: &#39;&#39;,        component: PublicComponent    },    {        path: &#39;protected&#39;,        component: ProtectedComponent,        canActivate: [AppAuthGuard]    }];TestsRunning a containerOnce you have a Keycloak server running, you can build the application and a Docker image and then run a container.To do so, first build the application in the main repo directory:./gradlew buildNext, create a docker image:docker build -t michuu93/spring-angular-keycloak-demo .Finally, run the container:docker run --rm --name spring-angular-keycloak-demo --network host michuu93/spring-angular-keycloak-demoSince the applications inside the container as well as you operating at the browser level need to have access to the Keycloak server at the same address, a container with the --network host option is run in Docker. Let’s skip the details on how the network works for now.Thanks to that you don’t have to expose any proxy or replace hosts in the system. This is, however, a simplification made for the purpose of the demo and shouldn’t be used in a production environment.Verifying authentication procedureAfter running the application you can test it in action.Go to localhost:9082 where you should see the frontend application, specifically ToolbarComponent and PublicComponent:You can use the menu to go to:  Public - unsecured route with PublicComponent;  Protected - secured route with ProtectedComponent;  Keycloak Configuration - unsecured endpoint exposing Keycloak configuration from the backend;  Backend Hello - secure endpoint that returns Hello from the Backend!;  Logout (backend) - logging out of the backend;  Logout (frontend) - button which logs you out of the frontend, available only after logging into the frontend.To check if you have access to Keycloak Configuration without logging in, execute:It looks like the public route and API endpoint are working properly.If you now move to Protected, you will be redirected to Keycloak’s login page:http://localhost:8180/auth/realms/SpringBootAngular/protocol/openid-connect/auth?client_id=SpringBootAngularClient&amp;amp;redirect_uri=http%3A%2F%2Flocalhost%3A9082%2F%23%2F&amp;amp;state=3e007783-4772-48dd-8b31-4bfe3cc9c42c&amp;amp;response_mode=fragment&amp;amp;response_type=code&amp;amp;scope=openid&amp;amp;nonce=023ab545-32dc-4bdc-9cbf-12cad1d0c944As you can see, the address contains information such as protocol name, realm, client and the address you will be redirected to after logging in.If you log in with a user that has user_role (user:password), you will be redirected back to the application:Additionally, you can use the Logout (frontend) button. You’re logged in now, so you can go to Protected:Backend Hello will work as well:If you log out of the frontend now - Logout (frontend), the Logout (frontend) button will disappear and you will no longer have access to Protected.You will still have access to Backend Hello. Only when you log out of the backend - Logout (backend), will you lose access to Backend Hello.However, if you are a logged in user and you log out of the backend, not the frontend, you will be automatically logged out of the frontend as well.Why?The principles of how Keycloak and the tokens (the OpenID Connect standard specifically) work is a separate topic. Due to complex implementation, logging out deserves a separate article, especially when distributed systems with multiple application instances running behind a load balancer are involved.The demo has a very simple logging out mechanism.Logging out from the frontend will call the logout method on the keycloak-angular library service:logout = async (): Promise&amp;lt;void&amp;gt; =&amp;gt; await this.keycloakService.logout();whereas logging out from the backend will call /api/logout, which will result in executing:fun logout(request: HttpServletRequest, response: HttpServletResponse) {    request.logout()    response.sendRedirect(&quot;/&quot;)}on the backend.Finally, let’s also verify a user without the user_role (user2:password2).After logging in, you should see the same buttons in the menu as in the case of a user with the required role.This is so because displaying buttons is not dependent on whether the user has the permission to the route but on whether they are logged in:ngOnInit(): void {    this.keycloakService.isLoggedIn().then(isLogged =&amp;gt; this.isLogged = isLogged);}Calling Backend Hello will not be possible either.Authentication can be quickly turned off in both applications by setting keycloak.enabled in application.yml to false.It can be useful e.g. in test environments.Closing remarksI hope this article has been useful in demonstrating how to integrate an application with a Keycloak server.Still, this is not the only method of implementation. If the application is supposed to be more flexible, you can use the implementation of the OAuth2 standard using Spring Security. This way you will not be tied to a Keycloak server and in the future you will be able to easily replace it with a different authentication/authorization server.Keycloak exposes endpoints at which the configuration is available (link).You can find them in Realm Settings &amp;gt; General &amp;gt; Endpoints. For OpenID Connect in this case it is:http://127.0.0.1:8180/auth/realms/SpringBootAngular/.well-known/openid-configurationThey can be useful if the application does not use Keycloak adapters to connect to the server.Also, you don’t have to log in to the application via a browser but you can use Keycloak to authenticate applications between one another, e.g. when you are integrating various modules together. In such a case, the bearer-only option` in the client configuration will be useful.",
"url": "/2020/02/01/keycloak-user-authentication-and-authorization-springboot-angular.html",
"author": "Michał Hoja",
"authorUrl": "/authors/mhoja.html",
"image": "mhoja.jpg",
"highlight": "/assets/img/posts/2020-02-01-keycloak-uwierzytelnianie-autoryzacja-springboot-angular/Keycloak.png",
"date": "01-02-2020",
"path": "en/2020-02-01-keycloak-user-authentication-and-authorization-springboot-angular"
}
,


"2020-01-19-zarzadzanie-zadaniami-html": {
"title": "Moje własne zadania i jak się w nich odnaleźć",
"lang": "pl",
"tags": "Personal Tasks Time Management Productivity To-Do Lists Planning Your Day",
"content": "Czy warto świadomie zarządzać zadaniami?Większość z nas ma coś do zrobienia. Na początku owe coś jest dość ogólnie zdefiniowane, np.: “napisać wpis na bloga” czy “zaprojektować mechanizm ładowania modułów aplikacji”. Dość szybko zaczynamy rozbijać temat na mniejsze elementy. Te mniejsze elementy zostają doprecyzowane lub dalej podzielone, aby w końcu stać się  zadaniami:  wymyślić tytuł i datę publikacji wpisu  zaproponować agendę  napisać tekst.Bierzemy pierwsze zadanie od góry i zaczynamy realizować nasz projekt. Dość szybko pojawiają się nowe zadania:  skoordynować datę publikacji z innymi wpisami w kolejce  zorganizować wsparcie merytoryczne          doprecyzować agendę, aby móc wybrać osobę wspierającą        zweryfikować dane statystyczne  sporządzić listę źródeł  wybrać grafiki.Jeszcze nie zaczęliśmy pisać samego tekstu bloga a już mamy 9 zadań. W miarę postępu prac pojawiają się następne konkretne zadania:  znaleźć wyniki badań potwierdzających tezę  przeredagować punkt 5  skrócić podsumowanie  uzgodnić przesunięcie daty publikacji  poprawić literówki po przeglądzie.Opierając się na własnym doświadczeniu zaryzykuję poniższe stwierdzenia:  nawet przy pozornie prostym temacie zadań potrafi być sporo  nowe zadania pojawiają się ciągle, w trakcje realizacji innych zadań.Większość z nas zarządza własnymi zadaniami. Świadomie lub nie. Część z nas robi to w pamięci - pamiętam o kupieniu kwiatów dla żony na rocznicę ślubu. Niestety nasz mózg ma pewne ograniczenia utrudniające zarządzanie zadaniami w pamięci.Zadania przechowywane w pamięci trudno się priorytetyzuje. Ciekawe zadania automatycznie stają się ważniejsze. Podobnie zresztą zadania nowe. Duży wpływ na priorytety zadań mają też czynniki zewnętrzne co nie zawsze jest pożądane. Cytując nasz biurowy humor: “kto głośniej krzyczy ten ma większy priorytet”.Zadania nudne lub odległe w czasie dość łatwo mogą się zagubić w mózgu, przynajmniej w moim. Szczególnie gdy lista zadań i różnych wątków, które należy kontrolować ciągle rośnie. Paradoksalnie niektóre zadania trudno usunąć z mózgu. Mam w głowie mnóstwo rzeczy, o których nie muszę już pamiętać i z przyjemnością zwolniłbym tę część pamięci.Najważniejszym argumentem przeciwko przechowywaniu listy zadań w głowie jest skalowanie tego rozwiązania. Owszem, mózg można ćwiczyć i konsekwentnie poprawiać jego działanie, jednakże większość z nas ma swój limit po przekroczeniu którego zaczyna gubić informacje. W miarę jak lista zadań w naszej głowie rośnie, prędzej czy później o którymś zapomnimy.Mam nadzieję, że na na tym etapie tekstu jest jasne, że warto odciążyć głowy przenosząc gdzieś listę zadań. Pytanie zatem gdzie?Co chcę osiągnąć?Wybierając mechanizm zarządzania zadaniami mam dwa podstawowe cele:  zapisując zadanie chcę mieć pewność, że zajmę się nim w odpowiednim czasie          nie muszę o nim pamiętać        realizując zadanie chcę mieć pewność, że jest to najlepsze wykorzystanie mojego czasu w danej chwili          mogę się skupić tylko na tym zadaniu.      Zestaw narzędzi umożliwiających zrealizowanie powyższych celów pozwoli mi ze spokojem pracować nad meritum i uwolni mnie od nerwowego “przeczesywania” pamięci: “co powinienem teraz zrobić?”, “czy wysłałem ofertę na XYZ?”, “czy mogę iść do domu?”.Teoria i metodykiNie będzie dużym zaskoczeniem jeżeli napiszę, że ktoś już wpadł na pomysł usprawnienia zarządzania zadaniami. W internecie i drukowanych materiałach znaleźć można mnóstwo informacji na temat wielu metodyk zarządzania zadaniami, np:  Getting Things Done (GTD)  Getting Sh-t Done (GSD)  Autofocus  Strikethru.Część metodyk poparta jest badaniami i opiera się na naukowych podstawach, aby maksymalizować efekty naszej pracy. Żadna z poznanych przeze mnie metryk nie jest jednak dla mnie idealna. W codziennej pracy wykorzystuję parę zaleceń i wskazówek z różnych metodyk i źródeł.3 elementy zarządzania zadaniamiPracę z zadaniami dzielę na trzy podstawowe etapy:  zapisywanie zadań  przegląd zadań  wykonanie zadania.Zapisywanie zadańZadanie powinniśmy zapisać w momencie, gdy je dostrzeżemy. Chcemy minimalizować czas, w którym zadanie jest tylko w naszej pamięci. Oznacza to, że chcemy zapisywać zadania w trakcie wykonywania innych czynności, np. będąc na spotkaniu, pracując nad innym zdaniem, rozmawiając na korytarzu czy patrząc przez okno kontemplując poranną kawkę.Narzędzie wspomagające nas w zarządzaniu zadaniami musi zatem dostarczać prosty sposób zapisywania zadania. Jak najmniej danych obowiązkowych, jak najmniej decyzji do podjęcia w momencie zapisywania. Jeżeli do stworzenia zadania muszę się przygotować i wiedzieć kiedy, jak, gdzie i z kim to zadanie będę wykonywał, to często tego zadania nie zapiszę.Jeżeli od początku wiem coś więcej o zadaniu np. “żeby wysłać tego maila będę potrzebował dostępu do poczty” to mogę od razu to zapisać np. dodając etykietę “email”. Ale nie muszę tego robić natychmiast. Mogę te informacje uzupełnić później. Najlepiej podczas przeglądania zadań.Aby móc zapisać zadanie musimy najpierw je dostrzec. To samo w sobie często jest bardzo trudne, a czasem wręcz najtrudniejsze. Realizując zadanie łatwo jest “zatracić się” i rozwiązywać kolejne wyzwania bez zastanawiania się czy nie odwodzi nas to zbytnio od aktualnego celu.A zatem co może być moim zadaniem? Wszystko co nie jest wprost zaadresowane do osoby obecnej przy rozmowie. Aby w ciągu słów i myśli rozpoznać potencjalne zadanie warto zwracać uwagę na wypowiedziane czasowniki:  w pierwszej osobie liczby pojedynczej i mnogiej, np:          “zrobię/zrobimy”      “sprawdzę/sprawdzimy”      “zapytam”      “przypilnuję/dopilnuję”        w drugiej osobie liczby mnogiej          “zrobicie”      “sprawdzicie”        w trzeciej osobie liczby pojedynczej          “on to zrobi”.      Zdania z takimi czasownikami opisują zadanie, które należy wykonać albo upewnić się, że ktoś je wykona.Przegląd zadańJeżeli mamy już listę zadań dodanych jak najszybciej, “na gorąco”, to koniecznie musimy tę listę oczyścić. Nieoczyszczana lista szybko robi się długa i przerażająca. Jej pielęgnowanie jest bardzo ważne. Na szczęście mamy dość proste narzędzie do pielęgnacji listy: przegląd.Przegląd polega na przejściu wszystkich zadań na liście.  Procedura powtarzana dla każdego zadania może być różna. W moim przypadku sprawdza się poniższa:  jeżeli zadanie jest nieaktualne lub bez sensu: usuwam to zadanie  jeżeli zadanie jest “małe” (dla mnie najczęściej to znaczy “zajmie mniej niż 3 minuty”): realizuję natychmiast  jeżeli jest jakaś wątpliwość w zadaniu i mogę ją wyjaśnić: uzupełniam opis zadania  jeżeli jest jakaś wątpliwość w zadaniu i nie mogę jej od razu wyjaśnić: dodaję zadanie na wyjaśnienie  upewniam się, że zadanie ma pełen opis wedle obecnego stanu wiedzy (treść, priorytet, termin, etykiety, komentarze…), w razie potrzeby: uzupełniam lub poprawiam opis.Koniec, w ramach przeglądu nie robię nic więcej. Taki przegląd powinien zająć nie więcej niż 15 minut i powinien być robiony co najmniej raz dziennie. Dla mnie optymalne są dwa przeglądy: jeden rano “do porannej kawki” podczas którego staram się ułożyć plan dnia, i drugi pod koniec pracy w biurze podczas którego upewniam się że nie przeoczyłem czegoś ważnego.Część materiałów dostępnych online poleca wieczorny przegląd zadań “na koniec dnia”, aby następnego dnia obudzić się z gotowym planem na dzień w głowie. Cytując klasyka:  “najlepszy sposób na zakończenie dnia to zaplanowanie dnia następnego”Pomysł jest pociągający, idea szczytna. Prawda jest taka, że nie udało mi się jak dotąd wcielić tego w życie. Może kiedyś będę mógł uzupełnić ten wpis doświadczeniami w tym zakresie.W ramach przeglądu można też przypisać zadaniom kategorie/projekty, priorytety, etykiety, itp. Możliwości są mocno zależne od wybranego narzędzia wpierającego zarządzanie zadaniami, jednak cel jest wspólny dla wszystkich: podzielić duży zbiór zadań na mniejsze, które łatwiej ułożyć w odpowiedniej kolejności.Wykonanie zadańPo dostrzeżeniu zadań, zapisaniu ich poza głową, przejrzeniu i odpowiednim uszeregowaniu przychodzi wreszcie czas na ich wykonanie. Jeżeli poprzednie elementy wykonaliśmy poprawnie, możemy w tym momencie skupić się w 100% na meritum zadania. Jesteśmy pewni, że zadanie na górze listy jest najważniejsze i w tej chwili powinniśmy się nim zająć. Nie rozprasza nas zastanawianie się nad tym, jak lepiej wykorzystać czas.Dostępne narzędziaWybór narzędzia najczęściej jest najciekawszym etapem różnych przedsięwzięć. Wiele osób od tego zaczyna przygodę z “zarządzaniem zadaniami”. Niestety wielu też na tym etapie kończy. Pamiętajmy, że narzędzie ma nam pomóc osiągnąć cel. Ma nam ułatwić. Nie wskaże nam natomiast celów i nie wykona zadań za nas. Korzystanie z danego narzędzia nie powinno też być celem samym w sobie.Skoro chcemy jednak “gdzieś” te zadania zapisywać zobaczmy, jakie mamy możliwości.  kartka papieru          łatwe do wdrożenia, praktycznie zerowy czas wejścia      trudne zarządzanie (sortowanie? priorytetyzacja? zadania cykliczne?)      trudne wyszukiwanie      słabe skalowanie        plik tekstowy          łatwe do uruchomienia      łatwe wyszukiwanie      konieczność wypracowania własnej notacji i oznaczeń      brak obsługi przypomnień      synchronizacja pomiędzy urządzeniami?        WIKI (np. lista zadań w prywatnej przestrzeni)          łatwe do uruchomienia      łatwe wyszukiwanie      konieczność wypracowania własnej notacji i oznaczeń      brak obsługi przypomnień      łatwa synchronizacja pomiędzy urządzeniami        JIRA (np. lista zadań w prywatnym projekcie)          proste zarządzanie z wykorzystaniem rankingu, etykiet, epików itp.      możliwość skonfigurowania przypomnień      skomplikowana konfiguracja przepływu i struktury zadania      słabe wsparcie dla urządzeń mobilnych bez dodatkowych aplikacji        “JIRA” w chmurze (np. Trello)          lista zadań w prywatnej “chmurze”      proste zarządzanie z wykorzystaniem rankingu, etykiet, epików itp.      możliwość skonfigurowania przypomnień      dostępność z internetu (bez vpn/certyfikatów)      aplikacja mobilna      konieczność dostosowania workflow        dedykowana aplikacja          lista zadań w prywatnej “chmurze”      możliwość wprowadzenia struktury/etykiet      dostępność z internetu      aplikacja mobilna      brak narzuconego/wymaganego workflow.      Powyższa lista żadną miarą nie jest kompletna. Jestem przekonany, że istnieje jeszcze sporo ciekawych rozwiązań. Korzystanie z dedykowanej aplikacji jest jednak tak wygodne, że przestałem szukać dalej.Moje narzędzieJa do zarządzania zadaniami wykorzystuję aplikację Todoist. Mój wybór ułatwiły następujące właściwości aplikacji:  łatwość dostępu          WWW      dedykowana aplikacja mobilna      aplikacja Chrome (np. uruchamianie w Ubuntu jako osobne okno które mogę przykleić do pulpitu)      rozszerzenie Chrome (np. do szybkiego dodawania zadań)        brak narzuconego sposobu zarządzania          nie muszę definiować statusów/workflow zadań      mogę dowolnie interpretować priorytety/etykiety/projekty itp.        dodatkowe wbudowane funkcjonalności          etykiety      projekty      filtry      przypomnienia czasowe i geograficzne      integracje (email, Gmail, Chrome, Asystent Google, Slack, Kalendarz Google, IFFT…)        aspekt społecznościowy          sporo materiałów opisujących podejścia różnych osób do zarzadzania zadaniami w Todoist      Youtube, Twitter, blog.      Dodawanie zadań jest proste. Wymaga minimalnej wiedzy o zadaniu jednocześnie umożliwia ustawienie wszystkich aspektów natychmiast. Dzięki integracjom mogę dodawać zadanie w najwygodniejszy aktualnie sposów, np.  gdy prowadzę samochód mogę powiedzieć do Asystenta Google: “notatka dla siebie sprawdź jutro czy poszła faktura za X”  podczas korzystania z telefonu mogę dodać zadnie dotykając jeden przycisk na ekranie i wpisać treść zadania          etykiety, terminy, priorytety są zaczytywane z tekstu      można też je łatwo wybrać przyciskami        mogę dodać zadanie wysyłając email na dedykowany adres          bardzo przydatne jako forward - stworzenie zadania z maila zajmuje 4 sekundy        mogę dodać zadanie z menu kontekstowego w przeglądarce  mogę dodać zadanie z poziomu rozmowy na Slack wpisująć “/todoist sprawdź kiedy grają Gwiezdne Wojny”  mogę dodać zadanie bezpośrednio w aplikacji  mogę automatycznie tworzyć zadania w odpowiedzi na zdarzenia IFFT.Przeglądanie zadań jest proste. Zadania bez wskazanego projektu wpadają do skrzynki odbiorczej. Jest to worek, na wszystko co wpadło (o ile podczas tworzenia zadania nie podałem wprost projektu). Przeglądając skrzynkę odbiorczą mogę:  uzupełnić opis zadania          notatki w formie tekstu i załączników        dodać podzadania  ustawić priorytet          5 poziomów      steruje kolejnością wyświetlania zadań (prosto, bez ręcznego sortowania i nadmiernego wyboru)        przypisać do projektu          pozwala zdefiniować osobne worki zadań      ułatwia “ogarnięcie” dużej ilości zadań        ustawić etykietę          ja wykorzystuję etykiety jako wskazówki czego potrzebuję do wykonania zadania      jedno zadanie może mieć wiele etykiet      przykłady moich etykiet: email, wiki, jira, biuro, cisza      bardzo przydatne przy wykorzystaniu filtrów        ustawić termin  ustawić przypomnienie          czasowe      geograficzne (np. “gdy wejdę do Castorama”) - brzmi fajnie, przydatność taka sobie w moim przypadku.      Wybór zadania do wykonania jest prosty. Opieram się o filtry wybierające zadania na dany moment. Dla przykładu:  filtr “K9” (od nazwy naszego biura):          etykiety: biuro lub jira lub wiki lub email lub telefon lub “nie cisza”      termin: dzisiaj lub brak terminu (jeżeli na jutro to dzisiaj nie wrzucaj mi tego na ekran)        filtr “pociąg”:          etykiety: jira lub wiki lub email lub cisza      termin: dzisiaj lub brak terminu        filtr “wieczór”:          etykiety: jira lub wiki lub email lub cisza lub “nie biuro”.      Zadania widoku filtra są posortowane po priorytetach. Najważniejsze na górze. Aplikacja Chrome dodana do autostartu zapewnia że lista zadań będzie zawsze widoczna na pulpicie komputera.Kiedy to wszystko robić?Dodajemy zadania ciągle, na bieżąco. Nie chcemy pamiętać o dodaniu zadania.Przegląd zadań - regularnie. U mnie sprawdza się metoda “pozytywnego skojarzenia”: poranny przegląd zadań zawsze przy porannej kawce. Należy pamiętać, że przegląd można robić też w miarę potrzeb, nawet jeśli niedawno zrobiło się planowany. Jeżeli np. zrealizowaliśmy już wszystkie zadania na dzisiaj (rzadko) albo sprawy bieżące przygniotły nas bardziej niż zakładaliśmy (często).Wykonywanie zadań - ciągle. Jeżeli siadam przy biurku, to nie zastanawiam się, co tu by zrobić. Patrzę na listę w odpowiednim filtrze i jadę od góry. Zadanie możemy również wykonywać w odpowiedzi na przypomnienie. Polecam jednak ostrożność w tym aspekcie. Przypomnienia potrafią skutecznie wybić z rytmu.Kiedy robię coś nie tak?Po czym mogę poznać, że coś robię nie tak? Mam skonfigurowane narzędzie. Jestem zmotywowany. Coś tam wychodzi, ale czy na pewno? Poniższe objawy powinny nas sprowokować do optymalizacji lub zmiany podejścia:  nie zapisuję zadań          nie mam przy sobie narzędzia      nie wiem, na kiedy to      nie wiem, czego będę potrzebował      nie wiem, czy warto        realizuję chaotycznie          nie wiem, za co się zabrać      “aaaa, tutaj to było!””      “dzisiaj już tego nie dokończę”        realizuję z pamięci i potem znajduję zadania już wykonane          “o to już zrobiłem dwa tygodnie temu”      “ten projekt zrobiliśmy w zeszłym roku”        realizuję zbyt duże zadania          “napisz bloga” (jedno zadanie)        realizuję zadania, których nie warto robić teraz.Gdzie szukać informacji?Przede wszystkim: Google. Temat nie jest nowy, jest bardzo często opisywany. Pamiętajmy jednak, aby wykorzystać opisy aby dostosować mechanizm do własnych potrzeb. Widziałem już (nie tylko u siebie) próby bezkrytycznego wdrożenia metodyki (np. GTD) kończące się totalną klapą z uwagi na drobnostki. Można wybrać sobie tylko te elementy, które nam pomagają.Pozostałe źródła informacji:  Youtube          Carl Pullein      Keep Productive      Doist        strona Todoist i blog firmy która go napisała  Twitter          @todoist      @carlpullein      #productivity.      ",
"url": "/2020/01/19/zarzadzanie-zadaniami.html",
"author": "Michał Stolarski",
"authorUrl": "/authors/mstolarski.html",
"image": "mstolarski.jpg",
"highlight": "/assets/img/posts/2020-01-19-zarzadzanie-zadaniami/zarzadzanie_zadaniami.jpg",
"date": "19-01-2020",
"path": "pl/2020-01-19-zarzadzanie-zadaniami"
}
,


"2020-01-09-same-site-lax-by-default-html": {
"title": "SameSite=Lax by default coraz bliżej - czy jesteś gotowy?",
"lang": "pl",
"tags": "cookies security same-site",
"content": "Same site cookies (First-Part-Only) to stworzony kilka lat temu mechanizm, który pozwala na zmniejszenie ryzyka ataków typu CSRF.Zapewnia on, że dane ciasteczko może być wysyłane wyłącznie z żądaniami zainicjowanymi z domeny, dla której zostało zarejestrowane. Dokładniejszy opis znajdziesz pod tym adresem: https://web.dev/samesite-cookies-explained/.Skoro atrybut SameSite istnieje tak długo dlaczego nagle stał się dla mnie ważny?W maju 2019 twórcy przegladarki Chrome zapowiedzieli, że od wersji 80 (planowana wersja wydania to 4 lutego 2020) dla wszystkich ciasteczek, które nie mają zdefiniowanego atrybutu SameSite domyślnie zostanie przyjęta wartość Lax. Mozilla i Microsoft również wyraziły chęć wprowadzenia tej zmiany we własnych przeglądarkach, więc możemy się spodziewać, że w nieodległej przyszłości takie zachowanie zostanie również zaimplementowane w Firefoxie i Edge’u.W praktyce oznacza to, że ciasteczka które nie będą miały jawnie zdefiniowanego atrybutu SameSite nie będą wysyłane przy próbach żądań typu cross-site. Wyjątkiem od tej zasady będzie przypadek gdy użyjemy bezpiecznej metody HTTP (GET, HEAD, OPTIONS, TRACE) a wykonanie żądania będzie skutkowało tzw. top-level navigation (zmiana adresu w pasku przeglądarki).Jak sprawdzić czy ta zmiana dotyczy mnie?Jednym z najprostszych sposobów jest zajrzenie do narzędzi developerskich przeglądarki. Od wersji 77 przeglądarka Chrome, w przypadku problematycznych żądań, wyświetla w konsoli ostrzeżenie o następującej treści (jeżeli zauważyłeś takie ostrzeżenie w swoim systemie, będziesz musiał podjąć jakąś akcję):A cookie associated with a cross-site resource at (cookie domain) was set without the SameSite attribute. A future release of Chrome will only deliver cookies with cross-site requests if they are set with SameSite=None and Secure. You can review cookies in developer tools under Application&amp;gt;Storage&amp;gt;Cookies and see more details at https://www.chromestatus.com/feature/5088147346030592 and https://www.chromestatus.com/feature/5633521622188032Dodatkowo, przegladarki Chrome oraz Firefox umożliwiają ręczne włączenie opisywanego wyżej zachowania. W Chrome’ie należy ustawić wartość Enabled następujacym flagom: SameSite by default cookies (chrome://flags/#same-site-by-default-cookies) oraz Cookies without SameSite must be secure (chrome://flags/#cookies-without-same-site-must-be-secure)W Firefoxie włączymy funkcjonalność wchodząc na adres about:config i ustawiając flagi network.cookie.sameSite.laxByDefault oraz network.cookie.sameSite.noneRequiresSecure na wartość true.Weryfikując swoje systemy warto zwrócić uwagę na następujące elementy:  treści umieszczone w tagu &amp;lt;iframe&amp;gt; (w taki sposób czesto zapewniana jest integracja z systemami zapewniajacymi mapy, video, kontrolki do płatności, kalendarze itp),  zewnętrzne zasoby używane w naszym systemie (np: obrazki, zewnętrzne skrypty czy arkusze styli),  wszelkie wejścia do twojego systemu (np: przekierowania z innych stron, publiczne usługi, formularze, które mogą być wysyłane z innych systemów),  skoki do innych systemów (np: formularze płatności),  wszelkie elementy wykorzystujące SSO (single sign-on).Jak się przygotować?Niestety nie istnieje jeden prosty sposób, który rozwiąże wszystkie problemy. Jednak twórcy implementacji atrybutu SameSite przewidzieli ewentualne problemyw istniejących systemach i dodali możliwość ustawienia mu wartości None. Takie ciasteczko będzie działało niemalże identycznie jak te przed opisywanymi zmianami -jedyną różnicą jest konieczność zastosowania atrybutu Secure (w przeciwnym razie ciasteczko bedzie zawsze ignorowana przy żadaniach typu cross-site). Należy jednak pamiętać, że domyślne ustawienie SameSite=Lax zostało wprowadzone ze względów bezpieczeństwa i ustawiajac wartość atrybutu na None narażamy się na potencjalne ataki, więc tamgdzie jest to możliwe powinniśmy unikać tego rozwiązania. Dodatkowo, wartość ta nie jest prawidłowo obsługiwana przez część przegladarek co może być problemem w niektórych systemach.O części usecase’ów oraz rozwiązań możesz poczytać tutaj: https://web.dev/samesite-cookie-recipes/.Przydatne linki  https://blog.chromium.org/2019/05/improving-privacy-and-security-on-web.html  https://blog.chromium.org/2019/10/developers-get-ready-for-new.html  https://www.chromium.org/updates/same-site  https://web.dev/samesite-cookies-explained/  https://web.dev/samesite-cookie-recipes/  https://tools.ietf.org/html/draft-west-first-party-cookies-07  https://www.chromestatus.com/feature/5088147346030592  https://www.chromestatus.com/feature/5633521622188032",
"url": "/2020/01/09/same-site-lax-by-default.html",
"author": "Mateusz Pogorzelski",
"authorUrl": "/authors/mpogorzelski.html",
"image": "mpogorzelski.jpg",
"highlight": "/assets/img/posts/2020-01-09-same-site-lax-by-default/samesite.jpg",
"date": "09-01-2020",
"path": "pl/2020-01-09-same-site-lax-by-default"
}
,


"2020-01-09-rxjs-introduction-html": {
"title": "RxJS z Angularem - programowanie reaktywne aplikacji frontendowej",
"lang": "pl",
"tags": "Angular rxjs asynchronous reactive reactive extensions rxjs operators",
"content": "Pisząc aplikacje z wykorzystaniem Angulara mamy styczność z obiektami typu Observable. Na pewno zdarzyło Ci się użyć serwisu HttpClient do pobierania danych z serwera albo EventEmittera do komunikacji komponentów rodzic-dziecko. W każdym z tych przypadków użycia masz do czynienia z obiektem Observable. Czy zastanawiałeś się nad tym, czym w zasadzie jest ten typ obiektu, dlaczego musisz go zasubskrybować, aby otrzymać dane? A może już to wiesz, ale chciałbyś dowiedzieć się jak efektywniej wykorzystać bibliotekę RxJS?Jeżeli tak, to ten artykuł jest dla Ciebie!Czym jest RxJS?Reactive Extensions for JavaScript (RxJS) jest biblioteką ułatwiającą programowanie reaktywne w języku JavaScript. Dzięki tej bibliotece i komponentom, jakie udostępnia tworzenie asynchronicznych programów jest intuicyjne i proste - zaraz przekonasz się jak bardzo!Strumień danychZacznijmy jednak od podstaw. Czym jest strumień danych w programowaniu reaktywnym? Według definicji, jest on sekwencją danych dostępnych w danym okresie. Taką sekwencję możemy obserwować oraz pobrać z niej potrzebne nam obiekty lub wartości. Dane natomiast mogą pojawić się w każdym momencie jego życia, a o ich pojawieniu się jesteśmy powiadamiani callbackiem (czyli funkcją odwrotną wywoływaną przez strumień). Istnieją dwa typy strumieni - zimny i ciepły.Strumień zimnyStrumień zimny nie będzie emitować (produkować) danych aż do momentu, gdy ktoś (obserwator) zacznie go obserwować. Wyemituje odrębną wartość dla każdego nowego obserwatora - te wartości nie są współdzielone. Przykładowo: wysłanie żądania GET do serwera.this.httpClient.get&amp;lt;ServerResponse&amp;gt;(&#39;someUrl&#39;)Strumień ciepłyStrumień ciepły, w przeciwieństwie do zimnego, emituje dane niezależnie od tego, czy ktoś na niego nasłuchuje. Każdy obserwator operuje na współdzielonym zasobie danych - dwóch obserwatorów otrzyma tą samą wartość w momencie wyemitowania danej przez strumień. Przykładowo: wydarzenia ‘click’import {fromEvent} from &#39;rxjs&#39;; fromEvent(document, &#39;click&#39;)Observable, wzorzec ObserwatorMając już wiedzę czym jest strumień i z jakimi rodzajami strumieni możemy się spotkać, warto przejść do opisania podstawowego konceptu RxJS: Observable.Observable jest obiektem reprezentującym strumień danych. Implementuje wzorzec projektowy Obserwator, który zakłada istnienie bytu przechowującego listę obiektów - obserwatorów, nasłuchujących na jakiekolwiek zmiany stanu danego bytu, powiadamiającego każdego z nich o zmianie poprzez wywołanie funkcji przez nich przekazanych (callback).Najprostszego Observable możemy stworzyć za pomocą funkcji statycznej of.// obiekt Observable, emitujący wartości liczbowe od 1 do 5const numbers$: Observable&amp;lt;number&amp;gt; = of(1,2,3,4,5);Obiekt numbers$ jest definicją strumienia danych typu number. Jest to tylko i wyłącznie szablon strumienia. W tym przypadku stworzyliśmy strumień zimny. Znamy zbiór danych (1,2,3,4,5), jednak dane zaczną być emitowane dopiero w momencie rozpoczęcia nasłuchiwania na dany strumień przez obserwatora. Pod strumień ‘podłączamy’ się za pomocą funkcji subscribe().subscribe() i unsubscribe()Funkcja subscribe jako parametr oczekuje obiektu, który definiuje trzy funkcje: next, error oraz complete.const subscription = numbers$.subscribe({    next(value) {},    error(err) {},    complete() {}});Każda z tych funkcji jest callbackiem, który jest wywoływany w poszczególnych momentach przepływu danych przez strumień. Funkcja next(value) wywoływana jest za każdym razem, gdy strumień emituje pojedynczą wartość - czyli w naszym przypadku funkcja next() zostanie wywołana 5-krotnie, po razie dla każdej z cyfr z zakresu 1-5. Callback error(err) zostanie wywołany w momencie, gdy strumień zostanie nienaturalnie zamknięty lub przerwany. Complete jest ostatnim callbackiem, który wywoływany jest po zamknięciu strumienia.Wywołanie funkcji subscribe() dopisuje nas do listy obserwatorów danego strumienia.W momencie podpięcia się jako obserwator do danego Observable zostaje przekazany nam obiekt typu Subscription, na którym możemy wywołać metodę unsubscribe(), która usunie nas z listy obserwatorów.Odsubskrybowanie się jest bardzo ważne w przypadku strumieni gorących, które w większości przypadków są nieskończone - emitują wartość przez potencjalnie nieskończoną ilość czasu. Jeżeli zapomnimy o usunięciu nas z listy obserwatorów danego strumienia, referencja do stworzonego przez nas obserwatora będzie istnieć przez cały cykl życia aplikacji - tworząc nieskończoną dziurę w pamięci, która w sytuacji ekstremalnej może doprowadzić do zabicia karty, w której działa nasza aplikacja.Subject - tworzenie własnych strumieniRxJS oferuje nam również możliwość tworzenia własnych strumieni. Możemy to zrobić za pomocą obiektów typu Subject. Subject, tłumacząc na język polski, oznacza dosłownie: temat. Taki strumień jest więc tematem, potencjalnie nieskończonym, który emituje nowe wartości w kluczowych dla nas miejscach aplikacji. Subject możemy stworzyć bardzo prosto - jak każdy inny obiekt:const subject$ = new Subject&amp;lt;number&amp;gt;();Do danego strumienia możemy teraz się subskrybować jako obserwatorzy:subject$.asObservable()    .subscribe((value) =&amp;gt; console.log(&#39;value from subject$: &#39;, value))Zmienna subject$ oferuje nam teraz metodę next(value: number), dzięki której możemy rozesłać wszystkim obserwatorom nową wartość liczbową, w taki sposób:subject$.next(5);// value from subject$: 5Strumień, jaki teraz stworzyliśmy jest strumieniem nieskończonym, więc musimy pamiętać o odsubskrybowaniu się po zakończeniu nasłuchiwania! Możemy również zamknąć strumień wywołując metodę complete().Wyemitowaliśmy w powyższym przykładzie wartość numeryczną ‘5’, którą otrzymał pojedynczy obserwator. Jeżeli w momencie emisji nowej wartości nie istniałby żaden obserwator, to wartość ta przepadła by na wieki. Jest to dosyć smutne. Co w przypadku, gdy ta wartość jest dla nas ważna i nie chcemy jej stracić? Na szczęście z pomocą przychodzą nam specyficzne obiekty rozszerzające typ Subject.ReplaySubjectReplaySubject jest strumieniem, który dla każdego nowego obserwatora odtwarza N ostatnio emitowanych danych. Wartość N możemy przekazać w konstruktorze tego obiektu, o tak:const replaySubject$ = new ReplaySubject&amp;lt;number&amp;gt;(5);Jeżeli wcześniej przez ten strumień emitowane były wartości, to przy subskrypcji, zostaną one odtworzone danemu obserwatorowi, ale nie więcej niż 5 ostatnich. Przykładowo:replaySubject$.next(1);replaySubject$.next(2);replaySubject$.next(3);replaySubject$.next(4);replaySubject$.next(5);replaySubject$.next(6);replaySubject$.asObservable()    .subscribe(replayedValue =&amp;gt; console.log(replayedValue));// 2// 3// 4// 5// 6BehaviorSubjectBehaviorSubject jest specyficznym rodzajem strumienia. Zawsze posiada on wartość, gdyż jest ona wymagana przy tworzeniu danego obiektu. Ponadto, strumień ten zawsze przechowuje ostatnio emitowaną wartość i podobnie jak w przypadku ReplaySubject, odtwarza ją każdemu nowemu obserwatorowi.Tworzymy go w równie prosty sposób:const behaviorSubject = new BehaviorSubject&amp;lt;boolean&amp;gt;(true);behaviorSubject.asObservable().subscribe(value =&amp;gt; console.log(value))// truebehaviorSubject.next(false);behaviorSubject.asObservable().subscribe(value =&amp;gt; console.log(&quot;drugi obserwator: &quot;, value))// false// drugi obserwator: falseTeraz, każdy nowy obserwator otrzyma obecnie przechowywaną przez dany strumień wartość - logiczną wartość ‘true’.AsyncSubjectAsyncSubject jest specyficznym strumieniem, ponieważ wyemituję on ostatnią wartość przekazaną w funkcji next() dopiero po zamknięciu tego strumienia, czyli po wywołaniu na nim funkcji complete(). Po zamknięciu przechowuje on wyemitowaną wartość i wyemituję ją każdemu nowemu obserwatorowi, który spóźnił się z subskrypcją przed zamknięciem strumienia.const asyncSubject = new AsyncSubject&amp;lt;number&amp;gt;();asyncSubject.asObservable().subscribe(value =&amp;gt; console.log(&quot;wartosc z async subject: &quot;, value))asyncSubject.next(1);asyncSubject.next(2);asyncSubject.complete();// wartosc z async subject: 2asyncSubject.asObservable().subscribe(value =&amp;gt; console.log(&quot;wartosc z async subject po zamknieciu strumienia: &quot;, value))// wartosc z async subject po zamknieciu strumienia: 2RxJS operatory - operacje na strumieniuPoznaliśmy różne sposoby tworzenia strumieni, co jeśli emitowane dane za każdym razem chcielibyśmy zmodyfikować pod nasz konkretny przypadek biznesowy? Z pomocą oczywiście przychodzi nam RxJS z szerokim wachlarzem operatorów, czyli funkcji operujących na naszym strumieniu. Poniżej przedstawię Ci parę z nich, które uważam za bardzo przydatne w codziennej pracy.Załóżmy, że operujemy na następującym strumieniu:const strumien$ = new BehaviorSubject&amp;lt;number&amp;gt;(15_000);const observable$ = strumien$.asObservable();Strumień domyślnie emituje wartość liczbową ‘15000’.Możemy operować na danych produkowanych przez nasz strumień, przekazując potrzebne nam operatory jako argumenty funkcji pipe() wywołanej na observable$.mapMap jest z pewnością znanym Ci operatorem, chociażby z API JS Arrays.map, którym RxJS się sugerował. Map transformuje dane zwracając nam nowy wynik dla każdej emitowanej danej. Przykładowo:observable$.pipe(map(numerek =&amp;gt; numerek*2))    .subscribe(value =&amp;gt; console.log(&#39;zmapowana wartosc: &#39;, value))// zmapowana wartosc: 30000firstInteresującym operatorem jest operator first(), który pobiera pierwszy element ze strumienia, następnie jako efekt uboczny - wypisuje obserwatora z listy obserwujących dany strumień! Jest to bardzo wygodne, jeżeli z danego gorącego strumienia potrzebujemy tylko jednej wartości - nie musimy przejmować się w tym momencie wywołaniem funkcji unsubscribe().Przykład:observable$.pipe(first())    .subscribe(value =&amp;gt; console.log(&#39;pierwsza otrzymana wartosc: &#39;, value))// pierwsza otrzymana wartosc: 15000strumien$.next(25000);// nic sie nie stalo, observable$ odpisal sie z listy obserwatorowwithLatestFromNastępnym operatorem wartym uwagi jest withLatestFrom, który umożliwia nam ‘skrzyżowanie strumieni’, czyli dołączenie do jednego strumienia ostatnio emitowanej wartości przez drugi strumień. Przykładowo:const otherStream$ = new BehaviourSubject&amp;lt;boolean&amp;gt;(true);observable$.pipe(withLatestFrom(otherStream$))    .subscribe(([value, otherStreamValue]) =&amp;gt; console.log(&#39;otrzymana wartosc z pierwszego strumienia: {}, z drugiego: {}&#39;, value, otherStreamValue))// otrzymana wartosc z pierwszego strumienia: 15000, z drugiego: truetakeUntilOperator takeUntil jest wygodnym operatorem, jeżeli chcemy w przejrzysty i prosty sposób wypisać się z listy obserwujących dany strumień.Rzućmy okiem na kod:@Component({  selector: &#39;app-some-component&#39;,  template: `        some template  `,  styleUrls: [&#39;some-styles.scss&#39;]})export class SomeComponent implements OnDestroy, OnInit {    private destroySubject$ = new Subject&amp;lt;void&amp;gt;();    private someValues$ = new BehaviourSubject&amp;lt;string&amp;gt;(&#39;initial value&#39;);        ngOnInit(): void {       this.someValues$.asObservable().pipe(takeUnitl(this.destroySubject$))            .subscribe(value =&amp;gt; ...)    }    ngOnDestroy(): void {       this.destroySubject.next();    }   }Powyższy fragment przedstawia nam SomeComponent, który nasłuchuje na wartości ze strumienia someValues$ aż do momentu, gdy zostanie wyemitowana wartość z drugiego strumienia: destroySubject$. W taki oto sposób nie musimy pamiętać o ręcznym odsubskrybowaniu się z pierwszego strumienia, gdyż zostanie on automatycznie zamknięty w momencie zniszczenia komponentu przez Angulara. :)switchMapKolejnym przydatnym operatorem może być switchMap(), którego zastosowanie pozwala nam zachować czystość i powstrzyma nas przed tworzeniem tzw. callback hell, czyli zagnieżdżania wywołań subscribe() tworzących łańcuch wywołań trudny w czytaniu i utrzymywaniu. Dlatego też zamiast pisać:observable$.subscribe(    value =&amp;gt; someService.processValue(value)        .subscribe(someServiceResponse =&amp;gt; andYetAnotherService.processAnotherValue(someServiceResponse)                .subscribe(yetAnotherResponse =&amp;gt; veryImportantService.processVeryImportantValue(yetAnotherResponse)                    .subscribe(veryImportantResponse =&amp;gt; ...))))Każdy z serwisów zwraca Observable do strumienia, z którego potrzebujemy wartości do wywołania kolejnego serwisu. Możemy zamiast tego napisać:observable$.pipe(    switchMap(value =&amp;gt; someService.processValue(value)),    switchMap(someServiceResponse =&amp;gt; andYetAnotherService.processAnotherValue(someServiceResponse)),    switchMap(yetAnotherResponse =&amp;gt; veryImportantService.processVeryImportantValue(yetAnotherResponse)))        .subscribe(veryImportantResponse =&amp;gt; ...)SwitchMap automatycznie subskrybuje nas do kolejnego strumienia, przez co kod staje się czytelniejszy.Oczywiście zagnieżdżanie strumieni i tak wprowadza narzut przeszkadzający w szybkim zrozumieniu, co dany kod produkuje, lecz jest to bardziej eleganckie podejście.Więcej operatorówAby poznać więcej operatorów polecam stronę learnrxjs, która świetnie opisuje interesujące operatory udostępnione w bibliotece RxJS.PodsumowanieRxJS jest potężnym narzędziem dającym wiele możliwości napisania kodu, który będzie reaktywny, asynchroniczny i intuicyjny dla czytającego go kolegi lub koleżanki z zespołu. Mam nadzieję, że będziesz kontynuować swoją przygodę z programowaniem reaktywnym i będziesz móc przekuć zebraną przy czytaniu tego artykułu wiedzę w kod asynchroniczny, napawający Cię dumą!",
"url": "/2020/01/09/rxjs-introduction.html",
"author": "Konrad Gabara",
"authorUrl": "/authors/kgabara.html",
"image": "kgabara.jpg",
"highlight": "/assets/img/posts/2020-01-09-rxjs-wstep/RxJS.png",
"date": "09-01-2020",
"path": "pl/2020-01-09-rxjs-introduction"
}
,


"2020-01-09-rxjs-with-angular-introduction-html": {
"title": "RxJS with Angular - reactive programming of a front-end application",
"lang": "en",
"tags": "Angular rxjs asynchronous reactive reactive extensions rxjs operators",
"content": "When writing  applications using Angular you eventually come into contact with Observables. You have surely dealt with them if you have used the HttpClient service to download data from a server or used EventEmitter for the communication between parent-child components. But have you ever wondered what exactly an object of this type is and why you have to subscribe to it to obtain data? Or maybe you know that already but want to find out how to use the RxJS library to its full potential?If this is the case, then this article is for you.What is RxJS?Reactive Extensions for JavaScript (RxJS) is a library that simplifies reactive programming in JavaScript. In this text I will try to demonstrate how the library and the components that it provides make creating asynchronous programs intuitive and straightforward.Data streamLet’s start with the basics. According to its definition, a data stream is a sequence of data available within a given period, which you can observe or download objects and data from. Data can appear at any point during its lifespan, and you are notified about it with a callback, that is, a reverse function called by the stream. There are two types of streams: cold and hot.Cold streamA cold stream will not be emitting data until it is observed. It will emit a separate value for every new observer, and these values will not be shared, for example: sending a GET request to the server.this.httpClient.get&amp;lt;ServerResponse&amp;gt;(&#39;someUrl&#39;)Hot streamUnlike a cold stream, a hot stream emits data irrespective of whether it is being listened to or not. All observers work on a shared set of data - two observers will receive the same value at the time of it being emitted by the stream. A click event is an example of such a scenario:import {fromEvent} from &#39;rxjs&#39;; fromEvent(document, &#39;click&#39;)Observable, observer patternNow, having defined a stream and distinguished its types, let’s describe a basic RxJS concept: an observable. An observable is an object that represents a data stream. It implements the observer design pattern which assumes the existence of an entity that stores the list of objects - observers listening for any changes of state of this entity. It also informs all observers about a change by calling functions that they have passed (callback).A simple observable can be created with the static of function.// Observable emitting numerical values from 1 to 5const numbers$: Observable&amp;lt;number&amp;gt; = of(1,2,3,4,5);Object numbers$ is the definition of the number-type of data stream. This is only a stream pattern. In this case a cold stream has been created. We know what the data set is (1, 2, 3, 4, 5), however data will be emitted only when an observer starts listening to a given data stream. To ‘connect’ to the stream, the subscribe() function should be used.subscribe() and unsubscribe()As a parameter, the subscribe function expects an object that defines three functions: next, error and complete.const subscription = numbers$.subscribe({    next(value) {},    error(err) {},    complete() {}});Each of these functions is a callback that is called at given moments of data flowing through the stream. The next(value) function is called every time the stream emits a single value, which in this case means that the next() function will be called 5 times, 1 time for every digit from 1 to 5. The error(err) callback will be called when the stream is unnaturally closed or disrupted. Complete is the last callback that is called once the stream has been closed.Calling the subscribe() function adds you to the list of observers of a given stream.When subscribing as an observer to a given observable, you are given a subscription-type  object on which you can call the unsubscribe() method to remove yourself from the list of observers.Unsubscribing is crucial in the case of hot streams, as they are mostly infinite, meaning they emit the value for a potentially infinite amount of time. If you forget to unsubscribe from the list of observers of a such a stream, the reference to the observer you’ve created will exist throughout the whole lifecycle of the application, creating an infinite hole in the memory, which in extreme cases might lead to the death of the tab that the application is running in.Subject - creating your own streamsRxJS enables you to create your own streams and to do so you can use a subject-type object. Such a stream is potentially infinite, emitting new values at what you consider important points of the application. A subject is created in the same way as other objects:const subject$ = new Subject&amp;lt;number&amp;gt;();Now you can subscribe to the stream as an observer:subject$.asObservable()    .subscribe((value) =&amp;gt; console.log(&#39;value from subject$: &#39;, value))The subject$ variable now provides you with the next(value: number) method, which allows you to send out a new numeric value to all observers, like this:subject$.next(5);// value from subject$: 5The stream you’ve just created is infinite, so you need to remember to unsubscribe once you finish listening to it. You can also close the stream with the complete() command.In this example, a numerical value ‘5’ was emitted and received by a single observer. If no observers exist at the time of emitting a new value, it is lost. However, if this value is important to you and you don’t want to lose, you can use specific subject-type extending objects.ReplaySubjectReplaySubject is a stream which replays a defined value of the last emitted data for every new observer. The value can be passed in the constructor in this way:const replaySubject$ = new ReplaySubject&amp;lt;number&amp;gt;(5);If some values have been emitted by this stream before, a maximum of 5 most recent ones will be generated to a given observer. For example:replaySubject$.next(1);replaySubject$.next(2);replaySubject$.next(3);replaySubject$.next(4);replaySubject$.next(5);replaySubject$.next(6);replaySubject$.asObservable()    .subscribe(replayedValue =&amp;gt; console.log(replayedValue));// 2// 3// 4// 5// 6BehaviorSubjectBehaviorSubject is a unique type of stream. It always has a value because it is necessary for creating a given object. Also, this stream always keeps the last emitted value and, similarly to ReplaySubject, replays it to every new observer. It is created just as easily:const behaviorSubject = new BehaviorSubject&amp;lt;boolean&amp;gt;(true);behaviorSubject.asObservable().subscribe(value =&amp;gt; console.log(value))// truebehaviorSubject.next(false);behaviorSubject.asObservable().subscribe(value =&amp;gt; console.log(&quot;secondobserver: &quot;, value))// false// second observer: falseNow, every new observer will receive the value - logical value ‘true’ -  that is currently being stored by the stream.AsyncSubjectAsyncSubject is a unique kind of stream because it emits the last value passed in the next() function only after closing the stream, that is, after calling the complete() function on it. After closing, it stores the emitted value and emits it to each new observer that failed to subscribe before the stream closed.const asyncSubject = new AsyncSubject&amp;lt;number&amp;gt;();asyncSubject.asObservable().subscribe(value =&amp;gt; console.log(&quot;value from async subject: &quot;, value))asyncSubject.next(1);asyncSubject.next(2);asyncSubject.complete();// value from async subject: 2asyncSubject.asObservable().subscribe(value =&amp;gt; console.log(&quot;value from async subject after closing stream: &quot;, value))// value from async subject after closing stream: 2RxJS operators - operations on the streamI’ve demonstrated a few ways of creating streams but you might also want to modify the emitted data to fit a particular business case. RxJS features a wide range of operators, that is, functions performing operations on the stream which can be used to modify the data. Below I will present a few of them that I consider very useful in everyday’s work.Let’s assume you are working on the following stream:const stream$ = new BehaviorSubject&amp;lt;number&amp;gt;(15_000);const observable$ = stream$.asObservable();By default the stream emits a numerical value ‘15000’. You can perform operations on the data generated by the stream by passing the necessary operators as arguments of the pipe() function called on observable$.mapYou are probably familiar with the map operator, e.g., from API JS Arrays.map, which RxJS was based on. Map transforms data, returning a new result for every emitted piece of data. For example:observable$.pipe(map(number =&amp;gt; number*2))    .subscribe(value =&amp;gt; console.log(&#39;mapped value: &#39;, value))// mapped value: 30000firstFirst is an interesting operator that downloads the first element from the stream, and then - as a side effect - it unsubscribes the observer from the stream’s observers list. This is very convenient when you need only one value from a given hot stream, as you don’t have to call the unsubscribe() function. For example:observable$.pipe(first())    .subscribe(value =&amp;gt; console.log(&#39;first received value: &#39;, value))// first received value: 15000stream$.next(25000);// don’t worry, observable$ has unsubscribed from the observers listwithLatestFromAnother noteworthy operator is withLatestForm, which enables you to ‘cross streams’, that is, to add the last emitted value of one stream to another stream. For instance:const otherStream$ = new BehaviourSubject&amp;lt;boolean&amp;gt;(true);observable$.pipe(withLatestFrom(otherStream$))    .subscribe(([value, otherStreamValue]) =&amp;gt; console.log(&#39;value received from first stream: {}, from other stream: {}&#39;, value, otherStreamValue))// value received from first stream: 15000, from other stream: truetakeUntiltakeUntil is an operator that comes in handy when you want to unsubscribe from a given stream’s observers list in a clear and simple way. Let’s have a look at the code:@Component({  selector: &#39;app-some-component&#39;,  template: `        some template  `,  styleUrls: [&#39;some-styles.scss&#39;]})export class SomeComponent implements OnDestroy, OnInit {    private destroySubject$ = new Subject&amp;lt;void&amp;gt;();    private someValues$ = new BehaviourSubject&amp;lt;string&amp;gt;(&#39;initial value&#39;);        ngOnInit(): void {       this.someValues$.asObservable().pipe(takeUnitl(this.destroySubject$))            .subscribe(value =&amp;gt; ...)    }    ngOnDestroy(): void {       this.destroySubject.next();    }   }The above snippet contains SomeComponent, which listens to the values from the someValues$ stream until the other stream emits value destroySubject$. This way you don’t need to remember about manually unsubscribing from the first stream because it will automatically close when the component is killed by Angular.switchMapAnother operator that you might find useful is switchMap(). It can be used to maintain clarity and prevent the so-called callback hell, which is a chain of nested subscribe() calls that are hard to read and maintain.observable$.subscribe(    value =&amp;gt; someService.processValue(value)        .subscribe(someServiceResponse =&amp;gt; andYetAnotherService.processAnotherValue(someServiceResponse)                .subscribe(yetAnotherResponse =&amp;gt; veryImportantService.processVeryImportantValue(yetAnotherResponse)                    .subscribe(veryImportantResponse =&amp;gt; ...))))In this case each service returns an observable to the stream from which you need a value to call another service. Instead, you can write:observable$.pipe(    switchMap(value =&amp;gt; someService.processValue(value)),    switchMap(someServiceResponse =&amp;gt; andYetAnotherService.processAnotherValue(someServiceResponse)),    switchMap(yetAnotherResponse =&amp;gt; veryImportantService.processVeryImportantValue(yetAnotherResponse)))        .subscribe(veryImportantResponse =&amp;gt; ...)SwitchMap automatically subscribes you to the next stream, which makes the code clearer. Naturally, nesting streams introduces load that may prevent you from quickly figuring out what the code does, but it is still a more elegant approach.More operatorsTo learn how to use other operators available in the RxJS library, I recommend visiting learnrxjs.SummaryRxJS is a powerful tool that gives you multiple options of writing code that will be reactive, asynchronous and intuitive for your fellow developers. I hope you will continue your journey with reactive programming and will find this article helpful in creating asynchronous code that will make you proud.",
"url": "/2020/01/09/rxjs-with-angular-introduction.html",
"author": "Konrad Gabara",
"authorUrl": "/authors/kgabara.html",
"image": "kgabara.jpg",
"highlight": "/assets/img/posts/2020-01-09-rxjs-wstep/RxJS.png",
"date": "09-01-2020",
"path": "en/2020-01-09-rxjs-with-angular-introduction"
}
,


"2019-12-19-testowanie-frontendu-asynchronicznego-html": {
"title": "Testowanie frontendu - Cz. 4 Testy jednostkowe kodu działającego asynchronicznie",
"lang": "pl",
"tags": "Jasmine Angular async unit testing unit test komponent",
"content": "Kończąc serię dotyczącą testowania komponentów Angularowych przy pomocy Jasmine, chciałbym poruszyć temat testów kodu wykonywanego asynchronicznie.Testy jednostkowe asynchronicznych aplikacji frontendowych często wydają się być zagadką dla developerów. Na szczęście twórcy narzędzi pomyśleli również o tym i dostarczyli nam narzędzia, które zdecydowanie ułatwiają pracę z testowaniem takiego kodu.W tym wpisie jednak nie poruszę kwestii testowania opartego na mockowaniu/stubowaniu kodu. Jeśli jesteś zainteresowany tym tematem, zachęcam do zajrzenia do artykułu Krzysztofa Czechowskiego o testowaniu serwisów.Kod poddany testomW celu sprawdzenia możliwości testowania asynchronicznych wywołań weźmy na warsztat przykładowy komponent:@Component({  selector: &#39;app-company&#39;,  template: &#39;&amp;lt;div *ngIf=&quot;messageVisible&quot; id=&quot;welcomeMessage&quot;&amp;gt;Hello!&amp;lt;/div&amp;gt;&#39;,})export class CompanyComponent {  messageVisible: boolean = false;  getCompany(): Promise&amp;lt;string&amp;gt; {    return Promise.resolve(&quot;company&quot;);  }  showMessage() {    setTimeout(() =&amp;gt; {      this.messageVisible = true;    }, 2000)  }}Posiada on dwie metody: jedną, która zwraca Promise z nazwą firmy oraz drugą, która wykonuje zmianę widoczności flagi po dwóch sekundach. Na podstawie tej flagi wyświetlana jest wiadomość w templacie HTML.Klasa testowa, do której będą dodawane test-case’y. W tym przypadku jest ona w 100% standardowa:describe(&#39;AppComponent&#39;, () =&amp;gt; {  let fixture: ComponentFixture&amp;lt;AppComponent&amp;gt;;  let debugElement: DebugElement;  beforeEach(async(() =&amp;gt; {    TestBed.configureTestingModule({      declarations: [        AppComponent      ],    }).compileComponents();      fixture = TestBed.createComponent(AppComponent);      debugElement = fixture.debugElement;  })); });Testowanie metod zwracających PromisyJeśli chcemy przetestować metodę, która zwraca wartość opakowaną w Promise, oraz której wynik nie jest zależny od dostępności zewnętrznych usług, możemy w łatwy sposób sprawdzić zwracane przez nie wartości przy pomocy mechanizmu async/await:it(&#39;resolves company using async/await&#39;, async function () {    const company = await fixture.componentInstance.getCompany();    expect(company).toEqual(&quot;company&quot;);});Jeśli z jakiegoś powodu nie możesz wykorzystać async/await, to wówczas zastosowanie znajdzie tradycyjne rozwiązywanie Promisów:it(&#39;resolves company promise manually&#39;, function () {    fixture.componentInstance.getCompany().then(company =&amp;gt; {        expect(company).toEqual(&quot;company&quot;);    });});Oczekiwanie na wykonanie metody przy użyciu fakeAsyncMetoda showMessage() z naszego komponentu ma narzucony czas dwóch sekund oczekiwania przed jej wykonaniem.W teście możemy powtórzyć ten zabieg i po wywołaniu metody uruchomić asercje wewnątrz setTimeout(). Jednak wprowadzanie realnego czasu oczekiwania nie jest efektywnym rozwiązaniem i bardzo spowolni nasze testy. Dzięki Angularowemu fakeAsync możemy testować kod asynchroniczny, w synchroniczny sposób.Zobaczmy:it(&quot;tests the message visibility&quot;, fakeAsync(() =&amp;gt; {  fixture.componentInstance.showMessage();  tick(2000);  fixture.detectChanges();  fixture.whenStable().then(() =&amp;gt; {    const helloMessage = fixture.debugElement.query(By.css(&quot;#welcomeMessage&quot;));    expect(helloMessage).toBeTruthy();    expect(helloMessage.nativeElement.innerHTML).toBe(&#39;Hello!&#39;);  })}));Spoglądając od góry:  najpierw opakowujemy nasz test jednostkowy w blok fakeAsync(), który pozwala nam oszukać asynchroniczny przepływ,  wywołujemy asynchroniczną metodę,  symulujemy upływ czasu - w rzeczywistości nie trwa to dwóch sekund, jednak aplikacja “myśli”, że tyle upłynęło,  wykrywamy zmiany, a kiedy detekcja zmian się zakończy, robimy tradycyjne asercje.A co jeśli nie znamy czasu, który powinien upłynąć zanim wykonamy asercje? W miejscu tick(2000), możemy wykorzystać flush() i efekt będzie dokładnie taki sam. Czym się charakteryzuje flush()? Podobnie jak tick, symuluje on upływ czasu, jednak robi do momentu opustoszenia kolejki macrotasków (czyli m.in. setTimeout, setInterval).Testowanie kodu asynchronicznego - podsumowanieDzięki uzbrojeniu JavaScriptu w wygodne mechanizmy oraz ułatwieniom ze strony Angulara, testowanie jednostkowe kodu działającego asynchronicznie staje się nawet nie tyle proste, co całkiem przyjemne.",
"url": "/2019/12/19/testowanie-frontendu-asynchronicznego.html",
"author": "Adrian Marszałek",
"authorUrl": "/authors/amarszalek.html",
"image": "amarszalek.jpg",
"highlight": "/assets/img/posts/2019-12-20-testowanie-frontendu-asynchronicznego/testowanie-frontendu-asynch.jpg",
"date": "19-12-2019",
"path": "pl/2019-12-20-testowanie-frontendu-asynchronicznego"
}
,


"2019-12-04-testowanie-komponentow-z-inputami-i-outputami-html": {
"title": "Testowanie frontendu - Cz. 3. Testowanie komponentów angularowych z inputami i outputami",
"lang": "pl",
"tags": "Jasmine Angular unit test komponent",
"content": "Czas na kolejną dawkę informacji dotyczących testowania przy użyciu Jasmine. Po przeczytaniu wcześniejszych wpisów (Cz. 1 i Cz. 2) pora skupić się na testowaniu komponentów angularowych, a w szczególności ich wejść i wyjść. Przykłady oprzemy o aplikację, która będzie składała się z kilku drobnych wzajemnie się ze sobą komunikujących elementów.Schemat przykładowej aplikacji wygląda następująco:Jest to prosty program mający na celu dodawanie i zapisywanie wyników. Został rozpisany na komponenty tak, aby można było spokojnie przetestować wejścia i wyjścia w różnych wariantach.Aplikacja zawiera dwa komponenty przekazujące wpisaną liczbę (CalculatorInputFieldComponent), następnie wysyłane są one do komponentu, który zliczy nam wynik (CalculatorResultComponent) i zaprezentuje go w czytelnej formie (CalucatorResultPresentationComponent). Z poziomu komponentu prezentującego wyniki możemy je również zapisać (CalculatorSavedListComponent). Strzałki na diagramie oznaczają wejścia i wyjścia komponentów.Pisanie testów dotyczących @InputNa początek weźmy na warsztat komponent pozwalający wyświetlić listę wyników, które chcielibyśmy zapisać.@Component({  selector: &#39;calculator-saved-list&#39;,  template: `      &amp;lt;div class=&quot;result&quot;&amp;gt;{{getText()}}{{savedValues}}&amp;lt;/div&amp;gt;  `,})export class CalculatorSavedListComponent {  @Input() savedValues: Array&amp;lt;number&amp;gt;;  getText(): string {    if (this.savedValues.length === 0) {      return &#39;Brak zapisanych wartości&#39;;    } else {      return &#39;Zapisane wartości to: &#39;;    }  }}Trudno sobie wyobrazić prostszy komponent. Mamy tutaj jeden input, przekazujący listę wyników, którą chcemy wyświetlić.Pora na napisanie testu sprawdzającego, czy komponent poprawnie interpretuje przekazane mu wynikidescribe(&#39;CalculatorSavedListComponent&#39;, () =&amp;gt; {  let component: CalculatorSavedListComponent;  let fixture: ComponentFixture&amp;lt;CalculatorSavedListComponent&amp;gt;;  let resultText: DebugElement;  beforeEach(async(() =&amp;gt; {    TestBed.configureTestingModule({      declarations: [CalculatorSavedListComponent]    }).compileComponents();    fixture = TestBed.createComponent(CalculatorSavedListComponent);    component = fixture.componentInstance;    resultText = fixture.debugElement.query(By.css(&#39;.result&#39;));  }));  it(&#39;should show correct text when list is empty&#39;, () =&amp;gt; {    component.savedValues = [];    fixture.detectChanges();    expect(resultText.childNodes[0].nativeNode.wholeText).toEqual(&#39;Brak zapisanych wartości&#39;);  });  it(&#39;should show elements of list with correct text&#39;, () =&amp;gt; {    component.savedValues = [12, 3];    fixture.detectChanges();    expect(resultText.childNodes[0].nativeNode.wholeText).toEqual(&#39;Zapisane wartości to: 12,3&#39;);  });});Zaczęliśmy tutaj, podobnie jak w poprzednim artykule, od stworzenia TestBed i dodatkowo wyciągnęliśmy za pomocą zapytania element html prezentujący tekst umieszczony z poziomu komponentu. Możemy się do niego odwołać na wiele sposobów, na przykład za pomocą klasy lub id.Dysponujemy tutaj dwoma testami o identycznej strukturze. Przekazujemy wartości w inpucie i sprawdzamy, czy element umieszczony w html został poprawnie zmieniony. Jeżeli inputów w komponencie mamy więcej, możemy je przetestować dokładnie w ten sam sposób.Pisanie testów dotyczących @OutputNastępnie napiszmy test do komponentu pozwalającego wpisać liczbę, którą przekazuje do komponentu nadrzędnego.@Component({  selector: &#39;calculator-input-field&#39;,  template: `      &amp;lt;input (keyup)=&quot;onKey($event)&quot;&amp;gt;  `,})export class CalculatorInputFieldComponent {  @Output() fieldValue = new EventEmitter&amp;lt;number&amp;gt;();  onKey(event) {    this.fieldValue.emit(event.target.value);  }}Komponent równie prosty jak poprzedni z takim wyjątkiem, że mamy tutaj do czynienia z outputem.describe(&#39;CalculatorInputFieldComponent&#39;, () =&amp;gt; {  let component: CalculatorInputFieldComponent;  let fixture: ComponentFixture&amp;lt;CalculatorInputFieldComponent&amp;gt;;  let input: DebugElement;  beforeEach(async(() =&amp;gt; {    TestBed.configureTestingModule({      declarations: [CalculatorInputFieldComponent]    }).compileComponents();    fixture = TestBed.createComponent(CalculatorInputFieldComponent);    component = fixture.componentInstance;    input = fixture.debugElement.query(By.css(&#39;input&#39;));  }));  it(&#39;should emit correct value on input change&#39;, () =&amp;gt; {    component.fieldValue.subscribe((value) =&amp;gt; expect(value).toBe(&#39;7&#39;));    input.nativeElement.value = 7;    document.querySelector(&#39;input&#39;).dispatchEvent(new KeyboardEvent(&#39;keyup&#39;));  });});Czyli standardowo, zaczynamy od przygotowania modułu testowego i uchwycenia elementu, który chcemy przetestować. Warto tutaj zaznaczyć, że istnieją inne sposoby na zainicjalizowanie komponentu niż TestBed, wszystko zależy od tego co, i w jaki sposób, będziemy testować. Na przykład kiedy nie potrzebujemy testować elementów z DOM możemy zostać przy zwykłym inicjalizowaniu komponentów. Więcej szczegółów można znaleźć w dokumentacji Angulara.Test zaczynamy od nasłuchiwania na zmienną fieldValue po której spodziewamy się, że przekaże nam wartość wpisaną w pole. Następnie uzupełniamy wartość i aktywujemy event, pozwalający wywołać metodę nasłuchującą na zmiany w polu. Test zakończy się, kiedy zostanie wywołana metoda expect sprawdzająca, czy wpisana przez nas wartość zgadza się z oczekiwaniami. Test może się również zakończyć niepowodzeniem, kiedy po upływie danego czasu (domyślna konfiguracja wskazuje na 5s) żaden expect nie zostanie wywołany.Na koniec możemy zebrać wiedzę i przetestować komponent zawierający wejścia, jak i wyjścia. Poniższy kod przedstawia komponent, który dodaje wskazane liczby oraz opcjonalnie potrafi wysłać dane o wyniku do komponentu ‘wyżej’.@Component({  selector: &#39;calculator-result&#39;,  template: `      &amp;lt;calculator-result-presentation [calculationResult]=&quot;getCalculationResult()&quot;&amp;gt;&amp;lt;/calculator-result-presentation&amp;gt;      &amp;lt;button (click)=&quot;saveValue()&quot;&amp;gt;Zapisz wartość&amp;lt;/button&amp;gt;  `,})export class CalculatorResultComponent {  @Input() values: Array&amp;lt;number&amp;gt;;  @Input() text: string;  @Output() savedValue = new EventEmitter&amp;lt;number&amp;gt;();  getResult(): number {    return Number(this.values[0]) + Number(this.values[1]);  }  getCalculationResult(): string {    return `${this.text} ${this.getResult()}`;  }  saveValue() {    this.savedValue.emit(this.getResult());  }}Możemy zauważyć, że komponent potrzebuje innego komponentu (calculator-result-presentation), któremu przekazuje wynik. W testach nie będzie miało to dużego znaczenia, należy jednak pamiętać o jego deklaracji przy tworzeniu TestBed.describe(&#39;CalculatorResultComponent&#39;, () =&amp;gt; {  let component: CalculatorResultComponent;  let fixture: ComponentFixture&amp;lt;CalculatorResultComponent&amp;gt;;  let button: DebugElement;  beforeEach(async(() =&amp;gt; {    TestBed.configureTestingModule({      declarations: [        CalculatorResultComponent,        CalculatorResultPresentationComponent      ]    }).compileComponents();    fixture = TestBed.createComponent(CalculatorResultComponent);    component = fixture.componentInstance;    button = fixture.debugElement.query(By.css(&#39;button&#39;));  }));  it(&#39;should emit correct value on button&#39;, () =&amp;gt; {    component.values = [1, 2];    component.savedValue.subscribe((value) =&amp;gt; expect(value).toBe(3));    button.nativeElement.click();  });});Powyższy test przekazuje wartości do inputów oraz naciska przycisk. Jeżeli wszystko działa poprawnie, po jego naciśnięciu powinniśmy zostać powiadomieni o wyniku działaniu, który został wyemitowany na output.Testowanie komponentów angularowych - podsumowanie.Powyższe przykłady przedstawiają najprostsze scenariusze testowe. Jest to jednak dobry punkt wyjścia do napisania własnych testów komponentów angularowych, przy których zaprezentowane rozwiązania mogą okazać się użyteczne.",
"url": "/2019/12/04/testowanie-komponentow-z-inputami-i-outputami.html",
"author": "Piotr Grobelny",
"authorUrl": "/authors/pgrobelny.html",
"image": "pgrobelny.webp",
"highlight": "/assets/img/posts/2019-12-05-testowanie-komponentow-z-inputami-i-outputami/frontend-test.jpg",
"date": "04-12-2019",
"path": "pl/2019-12-05-testowanie-komponentow-z-inputami-i-outputami"
}
,


"2019-11-28-templatowanie-jobow-jenkinsa-html": {
"title": "Automat dodający joby do Jenkinsa",
"lang": "pl",
"tags": "jenkins job jenkins template devops jenkins-job-builder jenkins-jobs",
"content": "W ogarniającym nas świecie mikroserwisów skala projektów do utrzymania staje się ogromna. Każdy z tych projektów musimy przecież: zbudować, przetesować, zdeployować itd. Przy dużej liczbie projektów przestaje to być trywialne. W tym artykule zajmiemy się pierwszym zagadnieniem - automatyzacją buildów, jednak opisany tutaj sposób bez problemu można zastosować do innych aspektów.Do budowania projektów starajmy się używać jednego narzędzia, ogromnie wpłynie to na proces unifikacji. W tym artykule będziemy posługiwać się mavenem.Do zautomatyzowania procesu posłuży nam Jenkins.Przejdźmy do sedna, czyli jak budować dużą liczbę projektów z jak najmniejszym nakładem pracy i ilością kodu do utrzymania.StartujemyPrzy założeniu, że projekty budujemymvn clean package-/+ jakieś super ważne przełączniki typu -DskipTes... ;) jesteśmy w stanie w bardzo prosty i schludny sposób zbudować kod/konfigurację, która zautomatyzuje cały proces.Automatyzację rozpoczniemy od użycia narzędzia: jenkins-job-builderInstalacja:pip install --user jenkins-job-buildermacOS:brew install jenkins-job-builderDefiniujemy plik konfiguracyjny dla jenkins-jobs w lokalizacji /etc/jenkins_jobs/jenkins_jobs.ini:[jenkins]query_plugins_info=Falseuser=jenkins #Użytkownik Jenkinsowypassword=93a1160c11dc014b7214d4e8769fe8c9url=http://localhost:8080 #Url do jenkinsa  user - Użytkownik Jenkinsowy  password - API Token dla swojego użytkownika link  url - Adres URL do JenkinsaTak skonfigurowane narzędzie pozwoli nam utworzyć dowolny job jenkinsowy.Utwórzmy plik o nazwie project1-build.yaml w katalogu jobs z zawartością- job:    name: project-1-build    project-type: freestyle    disabled: false    builders:        - shell: &#39;mvn clean package&#39;Zasilenie jenkinsa nowo utworzonym jobem:jenkins-jobs update jobsPo wykonaniu polecenia, utworzony zostanie pierwszy z projektów jenkinsowych. Good Job!SzablonyUwielbiamy opakowywać wszystko w pewne wzorce, wspólne procesy, reużywać raz dobrze napisany kod. :) Dlatego ten wątek będzię esencją artykułu.Wiemy już, że projekty budujemy w bardzo podobny sposób. Zbudujmy więc pierwszy szablon.Utwórzmy szablon o nazwie project-build-template.yaml w katalogu jobs- job-template:    name: &#39;{name}-{subname}-build&#39;    project-type: freestyle    disabled: false    builders:        - shell: &#39;mvn clean package&#39;Szablon posiada dwie zmiennename : nazwa projektusubname: numer oznaczajacy jeden z koljenych projektówZwróć uwagę na wartość w polu name {name}-{subname}-build jest to pattern, po którym będzie szukany szablon.Aby użyć szablonu tworzymy plik w katalogu jobs o nazwie projects.yaml- project:    name: project    subname:        - 1        - 2        - 3    jobs:        - &#39;{name}-{subname}-build&#39;Całość kończymy aktualizacją jobów: jenkins-jobs update jobsW ten sposób jednym ruchem wygenerowaliśmy 3 joby:project-1-buildproject-2-buildproject-3-buildKażdy z nich zawiera definicję joba opisanego w szablonie o nazwie {name}-{subname}-build&#39; czyli wywołanie mvn clean packagePodsumowanieCel został osiągnięty! Raz napisana definicja builda została użyta wiele razy (w naszym przykładzie tylko 3 ;) ). Zmniejszyliśmy liczbę zdublowanych konfiguracji, dzięki czemu jesteśmy w stanie lepiej nimi zarządzać.Był to prosty przykład ukazujący istnienie takiego narzędzia. Jeśli chcesz dowiedzieć się czegoś więcej - zostawiam kilka linków.DokumentacjaRepozytorium projektu",
"url": "/2019/11/28/templatowanie-jobow-jenkinsa.html",
"author": "Dawid Kubiak",
"authorUrl": "/authors/dkubiak.html",
"image": "dkubiak.webp",
"highlight": "/assets/img/posts/2019-11-28-templatowanie-jobow-jenkinsa/jenkins-job.jpg",
"date": "28-11-2019",
"path": "pl/2019-11-28-templatowanie-jobow-jenkinsa"
}
,


"2019-11-20-testowanie-komponentow-i-serwisow-html": {
"title": "Testowanie frontendu - Cz. 2. Testowanie komponentów i serwisów",
"lang": "pl",
"tags": "Jasmine Angular unit test",
"content": "2 tygodnie temu Marcin Mendlik pisał o konfiguracji Karmy i Jasmine w projekcie.Dziś będzie o tym, jak rozpocząć testy komponentów i serwisu.Wprowadzenie do testowaniaProjekt dostępny jest tutaj.Zaczniemy od AnimalsComponent - komponent prezentuje listę zwierząt:@Component({  selector: &#39;app-animals&#39;,  templateUrl: &#39;./animals.component.html&#39;,  styleUrls: [&#39;./animals.component.scss&#39;]})export class AnimalsComponent implements OnInit {  animals$: Observable&amp;lt;Animal[]&amp;gt;;  constructor(private animalService: AnimalService) { }  ngOnInit() {    this.animals$ = this.animalService.getAnimals();  }}Na początek sprawdzmy czy komponent zostanie utworzony:describe(&#39;AnimalsComponent&#39;, () =&amp;gt; {  let component: AnimalsComponent;  beforeEach(() =&amp;gt; {    component = new AnimalsComponent(null);  });  it(&#39;should have a component&#39;, () =&amp;gt; {    expect(component).toBeTruthy();  });});Te testy powinny przejść pozytywnie. AnimalComponent potrzebuje AnimalService, ale ponieważ z niego nie korzystamy, możemy do konstruktora przekazać null. Jednak jeżeli będziemy chcieli sprawdzić, czy na liście są jakieś zwierzęta, np.:it(&#39;should have a animals list with 1 animal&#39;, () =&amp;gt; {    component.animals$.subscribe(animals =&amp;gt; {      expect(animals.length).toEqual(1);      expect(animals).toEqual([fakeAnimal]);    });  });Otrzymamy błąd: TypeError: Cannot read property &#39;subscribe&#39; of undefinedSubscribe, wywoływany jest na zmiennej animals$, która inicjowana jest dopiero w metodzie ngOnInit(), wywołajmy więc ją na początku naszego nowego testu:it(&#39;should have a animals list with 1 animal&#39;, () =&amp;gt; {    component.ngOnInit();    component.animals$.subscribe(animals =&amp;gt; {      expect(animals.length).toEqual(1);      expect(animals).toEqual([fakeAnimal]);    });  });Tym razem mamy błąd: TypeError: Cannot read property &#39;getAnimals&#39; of nullW ngOnInit(), które wywołaliśmy, jest metoda: animalService.getAnimals(), a do naszego komponentu przekazaliśmy null’a.Możemy temu zaradzić przekazując spreparowany serwis na początku naszego pliku z testami:const fakeAnimal = {id: 1, name: &#39;pies&#39;};let fakeAnimalService;beforeEach(() =&amp;gt; {    fakeAnimalService = {      getAnimals: () =&amp;gt; of([fakeAnimal]),      httpClient: {}    } as any;    component = new AnimalsComponent(fakeAnimalService);  });Taki fakeAnimalService możemy przekazać do konstruktora. Pusty obiekt httpClient nam nie przeszkadza - i tak nie chcemy z niego korzystać. Wykorzystująca go funkcja getAnimals() od razu zwróci nam wynik nie korzystając z httpClient’a. Po tych zmianach cała klasa testu wygląda jak poniżej:import {AnimalsComponent} from &#39;./animals.component&#39;;import {of} from &#39;rxjs&#39;;describe(&#39;AnimalsComponent&#39;, () =&amp;gt; {  let component: AnimalsComponent;  const fakeAnimal = {id: 1, name: &#39;pies&#39;};  let fakeAnimalService;  beforeEach(() =&amp;gt; {    fakeAnimalService = {      getAnimals: () =&amp;gt; of([fakeAnimal]),      httpClient: {}    } as any;    component = new AnimalsComponent(fakeAnimalService);  });  it(&#39;should have a component&#39;, () =&amp;gt; {    expect(component).toBeTruthy();  });  it(&#39;should have a animals list&#39;, () =&amp;gt; {    component.ngOnInit();    component.animals$.subscribe(animals =&amp;gt; {      expect(animals.length).toEqual(1);      expect(animals).toEqual([fakeAnimal]);    });  });});Takie testy nie dają nam jednak odpowiedzi na pytania czy nasz serwis został wywołany i ile razy.Jest to też cenna informacja gdy nasz serwis robi np. jakieś kosztowne obliczenia.W takiej sytuacji pomoże nam funkcja createSpyObj, którą dostarcza nam Jasmine. Do funkcji tej przekażemy dwa parametry: nazwę serwisu i tablicę nazw metod.fakeAnimalService = jasmine.createSpyObj(&#39;animalService&#39;, [&#39;getAnimals&#39;]);Teraz jeszcze w naszym przypadku testowym musimy ustalić co funkcja getAnimals ma zwracać:const spy = fakeAnimalService.getAnimals.and.returnValue(of([fakeAnimal]));Odpowiedzi których szukaliśmy udzieli nam obiekt spy:it(&#39;should call getAnimals 1 time without parameters &#39;, () =&amp;gt; {    const spy = fakeAnimalService.getAnimals.and.returnValue(of([fakeAnimal]));    component.ngOnInit();    component.animals$.subscribe( () =&amp;gt; {      expect(spy).toHaveBeenCalled();      expect(spy).toHaveBeenCalledWith();      expect(spy).toHaveBeenCalledTimes(1);    });  });Istnieje też możliwość skorzystania z prawdziwego serwisu i ustalenia co ma zwrócić dana metoda.W tym celu dokonamy kilku zmian:  fakeAnimalService = jasmine.createSpyObj(&#39;animalService&#39;, [&#39;getAnimals&#39;]);zmienimy na animalService = new AnimalService(null);  nowy serwis przekarzemy do konstruktora componentu: animalService = new AnimalService(null);  wykorzystamy też metodę spyOn();const spy = spyOn(animalService, &#39;getAnimals&#39;).and.returnValue(of([fakeAnimal]));Cały plik z testami wygląda następująco:import {AnimalsComponent} from &#39;./animals.component&#39;;import {of} from &#39;rxjs&#39;;import {AnimalService} from &#39;../animal.service&#39;;describe(&#39;AnimalsComponent&#39;, () =&amp;gt; {  let component: AnimalsComponent;  const fakeAnimal = {id: 1, name: &#39;pies&#39;};  let animalService;  beforeEach(() =&amp;gt; {    animalService = new AnimalService(null);    component = new AnimalsComponent(animalService);  });  it(&#39;should have a component&#39;, () =&amp;gt; {    expect(component).toBeTruthy();  });  it(&#39;should have a animals list&#39;, () =&amp;gt; {    spyOn(animalService, &#39;getAnimals&#39;).and.returnValue(of([fakeAnimal]));    component.ngOnInit();    component.animals$.subscribe(animals =&amp;gt; {      expect(animals.length).toEqual(1);      expect(animals).toEqual([fakeAnimal]);    });  });  it(&#39;should call getAnimals 1 time without parameters &#39;, () =&amp;gt; {    const spy = spyOn(animalService, &#39;getAnimals&#39;).and.returnValue(of([fakeAnimal]));    component.ngOnInit();    component.animals$.subscribe( () =&amp;gt; {      expect(spy).toHaveBeenCalled();      expect(spy).toHaveBeenCalledWith();      expect(spy).toHaveBeenCalledTimes(1);    });  });})Testy z wykorzystaniem TestBedAby pomóc nam w testach Angular dostarcza interfejs TestBed.Na początku, gdy wygenerowaliśmy komponent przy pomocy Angular CLI, zawierał on również testy. Dla komponentu AnimalsComponent wyglądały one tak:import { async, ComponentFixture, TestBed } from &#39;@angular/core/testing&#39;;import { AnimalsComponent } from &#39;./animals.component&#39;;describe(&#39;AnimalsComponent&#39;, () =&amp;gt; {  let component: AnimalsComponent;  let fixture: ComponentFixture&amp;lt;AnimalsComponent&amp;gt;;  beforeEach(async(() =&amp;gt; {    TestBed.configureTestingModule({      declarations: [ AnimalsComponent ]    })    .compileComponents();  }));  beforeEach(() =&amp;gt; {    fixture = TestBed.createComponent(AnimalsComponent);    component = fixture.componentInstance;    fixture.detectChanges();  });  it(&#39;should have a component&#39;, () =&amp;gt; {    expect(component).toBeTruthy();  });});Niestety od początku testy wskazywały błędy.W powyższym teście TestBed chce nam dostarczyć cały komponent AnimalsComponent wraz z html’em, jednak nie ma wszystkich składowych jak chociażby AnimalsListComponent.Musimy poprawić naszą konfigurację tak aby zawierała wszystkie wymagane elementy:describe(&#39;AnimalsComponent&#39;, () =&amp;gt; {  let component: AnimalsComponent;  let fixture: ComponentFixture&amp;lt;AnimalsComponent&amp;gt;;  let animalService: AnimalService;  const fakeAnimal = { id: 1, name: &#39;fake&#39; };  beforeEach(async(() =&amp;gt; {      TestBed.configureTestingModule({        imports: [RouterTestingModule],        declarations: [AnimalsComponent, AnimalsListComponent],        providers: [          AnimalService,          { provide: HttpClient, useValue: {} }]      })        .compileComponents();  }));  beforeEach(() =&amp;gt; {    fixture = TestBed.createComponent(AnimalsComponent);    component = fixture.componentInstance;    animalService = TestBed.get(AnimalService);  });});Ta konfiguracja pozwoli nam już otrzymać przygotowany przez TestBed komponent:it(&#39;should have a component&#39;, () =&amp;gt; {    expect(component).toBeTruthy();  });Jednak test sprawdzający prezentowane zwierzęta ponownie wykaże błędy:  it(`should have a list of animals`, () =&amp;gt; {    component.ngOnInit();    component.animals$.subscribe(animals =&amp;gt; {      expect(animals).toBeTruthy();      expect(animals).toEqual([fakeAnimal]);    });  });AnimalService wywoła httpClient.get.W sekcji providers dostarczamy pusty obiekt jako httpClient: { provide: HttpClient, useValue: {} }Jest dobrze, bo nie chcemy żeby nasz test komunikował się z zewnętrznym serwisem.Ponownie wykorzystamy spyOn który zapewni, że animalService zwróci nam dane do testów:  it(`should have a list of animals`, () =&amp;gt; {    spyOn(animalService, &#39;getAnimals&#39;).and.returnValue(of([fakeAnimal]));    component.ngOnInit();    component.animals$.subscribe(animals =&amp;gt; {      expect(animals).toBeTruthy();      expect(animals).toEqual([fakeAnimal]);    });  });Dzięki testom z wykorzystaniem TestBed możemy przetestować nasz szablon html:it(`should have a button with text &quot;fake&quot;`, (() =&amp;gt; {    spyOn(animalService, &#39;getAnimals&#39;).and.returnValue(of([fakeAnimal]));    component.ngOnInit();    fixture.detectChanges();    const buttons = fixture.debugElement.queryAll(By.css(&#39;.animal-button&#39;));    expect(buttons[0].nativeElement.textContent).toEqual(&#39;fake&#39;);  }));Poniższe dwie linie pozwalają nam pobrać buttony i sprawdzić, czy są odpowiednio podpisane.const buttons = fixture.debugElement.queryAll(By.css(&#39;.animal-button&#39;));expect(buttons[0].nativeElement.textContent).toEqual(&#39;fake&#39;);Testy z wykorzystaniem HttpClientTestingModuleTymczasem możemy jeszcze wrócić do serwisu i sprawdzić jak przetestować go z wykorzystaniem TestBed i HttpClientTestingModule:Ponownie konfigurujemy moduł do testów:describe(&#39;AnimalService&#39;, () =&amp;gt; {  beforeEach(() =&amp;gt; {    TestBed.configureTestingModule({      imports: [HttpClientTestingModule],      providers: [        AnimalService      ]    });  });  it(&#39;should have a service&#39;, inject([AnimalService], (service: AnimalService) =&amp;gt; {    expect(service).toBeTruthy();  }));  it(&#39;should have a service&#39;, () =&amp;gt; {    const service = TestBed.get(AnimalService);    expect(service).toBeTruthy();  });});Powyżej widzimy dwa testy “should have a service”. Sprawdzają one to samo, jednak zaprezentowane są dwie różne możliwości dostarczenia serwisu do testu:  poprzez const service = TestBed.get(AnimalService);  inject([AnimalService], (service: AnimalService) - funkcja inject przyjmuje dwa parametry:          tablicę serwisów do wstrzyknięcia - tu jest to AnimalService. Gdybyśmy chcieli wstrzyknąć więcej serwisów byłyby one kolejnymi elementami tablicy, np.: [AnimalService, NextService]      drugi parametr to funkcja, gdzie określamy referencję do serwisu i jego typ. Dla dwóch serwisów wyglądałoby to tak: (service: AnimalService, nextService: NextService). Ważna jest ich kolejność tak aby była zgodna z kolejnością w tablicy, gdyż do pierwszej referencji będzie wstrzyknięty pierwszy element z tablicy.      Gdy moduł jest gotowy możemy przygotować test, który sprawdzi czy trafimy pod odpowiedni adres - i tylko tam. describe(&#39;getAnimals&#39;, () =&amp;gt; {    it(&#39;should call get with correct url&#39;,      inject([AnimalService, HttpTestingController], (service: AnimalService, controller: HttpTestingController) =&amp;gt; {        service.getAnimals().subscribe();        const request = controller.expectOne(&#39;http://localhost:3000/animals&#39;);        request.flush({id: 1, name: &#39;pies&#39;});        controller.verify();      }));  });Przy konfiguracji testów z wykorzystaniem TestBed pojawiło się słowo async. Ma ono związek z asynchronicznością, o której więcej napisze Adrian. Stay tuned!Więcej na temat testów Angulara i Jasmine znajdziesz:  https://angular.io/guide/testing#service-tests  https://jasmine.github.io/2.0/introduction.html",
"url": "/2019/11/20/testowanie-komponentow-i-serwisow.html",
"author": "Krzysztof Czechowski",
"authorUrl": "/authors/kczechowski.html",
"image": "kczechowski.jpg",
"highlight": "/assets/img/posts/2019-11-20-testowanie-komponentow-i-serwisow/testowanie_frontendu.jpg",
"date": "20-11-2019",
"path": "pl/2019-11-20-testowanie-komponentow-i-serwisow"
}
,


"2019-11-06-testowanie-frontendu-wprowadzenie-do-jasmine-html": {
"title": "Testowanie frontendu - Cz. 1. Wprowadzenie do Jasmine - konfiguracja i przykładowe testy",
"lang": "pl",
"tags": "javascript test jasmine",
"content": "Mój poprzedni wpis był o tym co testować w projektach frontendowych, teraz przyszedł czas aby wybrać odpowiednie narzędzia, zakasać rękawy i przejść do praktyki. Pokażę jak zainstalować i używać frameworka Jasmine w projekcie Node’owym.NodeInstalacja za pomocą nvmAby zainstalować lokalnie Node’a można posłużyć się nvm, więcej w artykule autorstwa Krzysztofa Czechowskiego na łamach naszego bloga.Do używania frameworka będzie potrzebny projekt Node’owy, w przypadku jego braku można w dowolnym katalogu taki stworzyć za pomocą polecenianpm initJasmineFrameworkJasmine jest frameworkiem służącym do testowania napisanym w duchu behaviour-driven development, nie ma dodatkowych zależności oraz, jak twierdzą twórcy, dostajemy go z bateriami, czyli powinien zawierać wszystko, co jest potrzebne do pisania testów jednostkowych w naszym projekcie.InstalacjaInstrukcja instalacji jest zwięźle opisana w dokumentacji. Aby zainstalować framework w projekcie, należy:dodać zależność w devDependencies,npm:npm install --save-dev jasmineyarn:yarn add jasmine --devinicjować lokalnie zainstalowany framework, polecenie utworzy domyślną konfigurację w katalogu spec, domyślnie Jasmine będzie wykonywał testy w plikach w katalogu spec o nazwach kończących się na spec.js lub Spec.js.node node_modules/jasmine/bin/jasmine initframework zainicjował się z domyślną konfiguracją:{  &quot;spec_dir&quot;: &quot;spec&quot;,  &quot;spec_files&quot;: [    &quot;**/*[sS]pec.js&quot;  ],  &quot;helpers&quot;: [    &quot;helpers/**/*.js&quot;  ],  &quot;stopSpecOnExpectationFailure&quot;: false,  &quot;random&quot;: true}następnie należy dodać wpis do package.json,&quot;scripts&quot;: { &quot;test&quot;: &quot;jasmine&quot; }zweryfikować instalację.npm testalboyarn testAngular CLIW przypadku użycia frameworka Angular CLI mamy już dostępny framework Jasmine i wymaga on jedynie innej konwencji nazewniczej plików z testami.Struktura testuTesty są znajdowane przez Jasmine na podstawie ścieżki i nazwy pliku, są to po prostu pliki o strukturze:describe(&quot;A suite is just a function&quot;, () =&amp;gt; {  let a;  it(&quot;and so is a spec&quot;, () =&amp;gt; {    a = true;    expect(a).toBe(true);  });});Randomized with seed 12095Started.1 spec, 0 failuresFinished in 0.008 secondsRandomized with seed 12095 (jasmine --random=true --seed=12095)Nawet doświadczonemu programiście przyzwyczajonemu do testów backendu powyższy przykład może wydawać się mało intuicyjny, na szczęście jest to tylko złudzenie.describe - to funkcja opisująca zestaw testów, jako pierwszy parametr przyjmuje opis zestawu, jako drugi funkcję zawierającą poszczególne przypadki testowe.it - funkcja opisująca pojedynczy przypadek testowy, tak samo jako describe pierwszym parametrem jest opis, drugim funkcja zawierający faktyczny test.expect - przyjmuje bieżącą wartość i porównuje ją za pomocą wbudowanych metod porównujących z oczekiwaną wartością, w powyższym przypadku jest to toBe, ale są dostępne bardziej specyficzne porównania albo ich zaprzeczenia (np. not.toBeNull, toBeUndefined etc.)Dostępne są również funkcje wykonujące się przed lub po wszystkich testach, lub każdym (beforeEach, afterEach, beforeAll, afterAll), przykładowo:describe(&quot;Test for resetting value before each test&quot;, () =&amp;gt; {  let value;  beforeEach(() =&amp;gt; {    value = 0;  });  it(&quot;value should be 1&quot;, () =&amp;gt; {    value = value + 1;    expect(value).toBe(1);  });    it(&quot;value should be 0&quot;, () =&amp;gt; {    expect(value).toBe(0);  });});Randomized with seed 22493Started..2 specs, 0 failuresFinished in 0.009 secondsRandomized with seed 22493 (jasmine --random=true --seed=22493)Jeśli chcemy wyłączyć dany zestaw testów, lub pojedynczy przypadek testowy, możemy posłużyć się funkcjami xdescribe lub xit,fdescribe oraz fit służą po to, by wyłączyć resztę testów, a zostawić te oznaczone właśnie tą literką f (od focus). Jest to szczególnie przydatne gdy pracujemy nad nowymi funkcjonalnościami i nie chcemy marnować czasu na wykonywanie testów, które w tym momencie są nieistotne.Jak widać na powyższych przykładach podstawy frameworka Jasmine są proste, sama instalacja i konfiguracja nie jest specjalnie skomplikowana, a w przypadku Angular CLI wszystko mamy już dostępne po instalacji. Ten wpis jest wprowadzeniem do testów jednostkowych we frontendzie, w następnej odsłonie pokażemy jak mockować lub stubować zależności w komponentach angularowych i serwisach.Materiały źródłowe  https://jasmine.github.io  https://github.com/nvm-sh/nvm  https://angular.io/guide/testing",
"url": "/2019/11/06/testowanie-frontendu-wprowadzenie-do-jasmine.html",
"author": "Marcin Mendlik",
"authorUrl": "/authors/mmendlik.html",
"image": "mmendlik.jpg",
"highlight": "/assets/img/posts/2019-11-06-testowanie-frontendu-wprowadzenie-do-jasmine/frontend-testing.jpeg",
"date": "06-11-2019",
"path": "pl/2019-11-06-testowanie-frontendu-wprowadzenie-do-jasmine"
}
,


"2019-10-21-batchowe-inserty-w-hibernate-droga-ku-szybkosci-html": {
"title": "Batchowe inserty w Hibernate - droga ku szybkości",
"lang": "pl",
"tags": "programming hibernate persistence jpa",
"content": "W tym poście powiemy o przykładowej ścieżce optymalizacji wstawiania grup rekordów do bazy danych za pomocą Hibernate’a i SpringBoota z założeniem użycia spring-boot-starter-data-jpa.Skupimy się na aspektach konfiguracyjnym i diagnostycznym systemu.Zapytany o to czy lepiej używać EntityManagera czy Hibernate’owego Session, Emmanuel Bernard  bez wahania opowiedział się za tym pierwszym [1]. Jest to wypowiedź zgodna z zasadą, którą jako programiści wszyscy dobrze znamy – bazowanie na specyfikacji, a nie implementacji danej technologii. Stosowanie się do tej reguły sprawia, że zmiany technologiczne są o wiele prostsze - jesteśmy związani tylko z interfejsem, a podmiana dostawcy jego implementacji jest przecież w założeniu tylko formalnością.Zdarza się jednak, że musimy zrobić coś co wychodzi poza ramy abstrakcyjnej specyfikacji i chcąc nie chcąc użyć mechanizmów konkretnej implementacji. Tak jest też w przypadku batchowych insertów czyli zapisywania większych grup rekordów w jednej transakcji. W tym poście przejdziemy przez typową drogę optymalizacji tejże operacji, przykłady wizualizując statystykami generowanymi przez Hibernate’a oraz przepływami zilustrowanymi za pomocą Zipkina [2].Załóżmy dobrze znany scenariusz - podczas spokojnego dnia w pracy nagle otrzymujemy maila:  Złe wieści!Wystawiona usługa co prawda działa, ale nie możemy za jej pomocą w jednym żądaniu złożyć 2000 zamówień.Okazuje sie, że oczekiwanie na odpowiedź trwa zbyt długo i dostajemy timeout!Nie pozostaje nam nic innego jak przystąpić do optymalizacji. Pierwsze kroki, który warto podjąć to ustawienie w celach diagnostycznych wpisu konfiguracyjnegospring.jpa.properties.hibernate.generate_statistics=truew pliku .properties lub .yml oraz odpowiednie skonfigurowanie Zipkina (tu tę konfigurację pominiemy bo jest ona obszernym materiałem, który mógłby wypełnić osobny artykuł). Te kroki pomogą nam w prześledzeniu powodu wyjątkowo długiego czasu odpowiedzi usługi. Przykłady będziemy badać na realnej usłudze w dwóch wariantach – żądanie z małą liczbą encji zwizualizowane za pomocą Zipkina oraz żądanie z dużą liczbą encji opisane za pomocą kluczowych statystyk i czasu wykonania.Przejdźmy do analizy. Na początek przyjrzyjmy się usłudze bez żadnych optymalizacji.Co powoduje, że widzimy tak dużo wykonanych operacji? Już na pierwszy rzut oka widać, że każde wstawianie rekordu jest realizowane osobno. Spójrzmy na wygenerowane statystyki dla normalnego wywołania usługi (przy dużej liczbie encji)Łączny czas odpowiedzi usługi przy wywołaniu przez HTTP 9503ms (POSTMAN)Kluczowe statystyki wygenerowane przez Hibernate:195762869 nanoseconds spent preparing 4005 JDBC statements;6984223566 nanoseconds spent executing 4005 JDBC statements;0 nanoseconds spent executing 0 JDBC batches;4163640487 nanoseconds spent executing 1 flushes (flushing a total of 2003 entities and 0 collections);5927 nanoseconds spent executing 1 partial-flushes (flushing a total of 0 entities and 0 collections)Ze statystyk wynika, że nie wykonały się żadne paczki operacji, widać natomiast informację o zrealizowaniu ponad 4000 komend JDBC. Wąskim gardłem jest sposób wstawiania rekordów do bazy danych. Jak temu zaradzić?Z pomocą przychodzi nam konfiguracja operacji batchowych. Skupimy się przede wszystkim na wstawianiu rekordów. Skonfigurujmy batchowe inserty poprzez dodanie do wcześniej wspomnianych plików konfiguracyjnych odpowiednich wpisówspring.jpa.properties.hibernate.jdbc.batch_size=1000Dodając ten wpis ustawiliśmy wielkość paczek w operacjach paczkowanych.Warto również, choć nie jest to wymagane, ustawić inny wpis konfiguracyjny.spring.jpa.properties.hibernate.order_inserts=trueJest to przydatne szczególnie w przypadku gdy występuje relacja encji rodzic-dziecko z kaskadową persystencją, pozwoli to w takim przypadku na zgrupowanie zapisów typami encji.Po wykonaniu pierwszego kroku optymalizacyjnego sprawdźmy jak zmieniły się wygenerowane statystyki.Dla dużej liczby encji statystyki prezentują się następująco:Łączny czas odpowiedzi usługi przy wywołaniu przez HTTP 5138ms (POSTMAN)Kluczowe statystyki wygenerowane przez Hibernate:84610265 nanoseconds spent preparing 2006 JDBC statements;3139685203 nanoseconds spent executing 2003 JDBC statements;349347467 nanoseconds spent executing 4 JDBC batches;539223064 nanoseconds spent executing 1 flushes (flushing a total of 2003 entities and 0 collections);5446 nanoseconds spent executing 1 partial-flushes (flushing a total of 0 entities and 0 collections)Zarówno z zwizualizowanego przepływu dla małej liczby encji, jak i wygenerowanych statystyk dla dużej ich liczby widzimy, że operacji jest mniej więcej o połowę mniej, a wygenerowane statystyki wprost mówią, że zostały wykonane „paczki” operacji. Nadal jednak wywołań komend JDBC jest dużo ponieważ wciąż wykonywane są indywidualne zapytania po przydział numerów bazodanowej sekwencji, które służą jako identyfikatory wstawianych encji. Jak możemy temu zaradzić?Początkowa konfiguracja identyfikatora naszej encji wygląda tak:@Id@GeneratedValueprivate Long transactionId;W praktyce oznacza to zostawienie dostawcy implementacji dowolności w doborze strategii generowania id. Weźmy sprawy w swoje ręce i zmieńmy strategię generowania id dla naszej encji na odpowiadającą naszym potrzebom.@Id@GeneratedValue(strategy = GenerationType.SEQUENCE, generator = &quot;hilo_sequence_generator&quot;)@GenericGenerator(      name = &quot;hilo_sequence_generator&quot;,      strategy = &quot;org.hibernate.id.enhanced.SequenceStyleGenerator&quot;,      parameters = {            @Parameter(name = &quot;sequence_name&quot;, value=&quot;EXAMPLE_SEQ&quot;),            @Parameter(name = &quot;initial_value&quot;, value=&quot;1&quot;),            @Parameter(name = &quot;increment_size&quot;, value = &quot;100&quot;),            @Parameter(name = &quot;optimizer&quot;, value = &quot;hilo&quot;)      })private Long transactionId;Co właściwie skonfigurowaliśmy w tym momencie?W adnotacji GeneratedValue informujemy Hibernate by użył strategii sekwencji generowania id dla encji i wskazujemy nazwę generatora. Niżej podajemy wcześniej wspomnianą nazwę generatora, klasę wskazującą na strategię generatora, podstawowe parametry i optymalizator. Jak działa ten ostatni, u nas przyjmujący wartość „hilo”?Sam algorytm hi/lo jest opisany w wielu miejscach na internecie [3], dlatego skupimy się tylko na tym jak działa na poziomie koncepcyjnym:Skoro nie chcemy za każdym razem pytać bazę danych o nowy numer sekwencji, to możemy jako klient zapytać o niego raz na N encji (wartość N została przez nas ustawiona w parametrze increment_size), a pomiędzy tymi zapytaniami sami inkrementować licznik.W praktyce oznacza to, że mogą powstać „dziury” w numeracji, gdy w danej transakcji wstawimy jedną encję to mimo wszystko potrzebujemy numeru sekwencji z bazy danych, a w konsekwencji podbijemy ją o N. Zyskiem z korzystania z tego mechanizmu jest rzadka potrzeba pytania bazy o kolejną wartość sekwencji.Sprawdźmy kolejny raz jak nasze optymalizacje wpłynęły na szybkość działania usługi.Dla dużej liczby encji statystyki prezentują się następująco:Łączny czas odpowiedzi usługi przy wywołaniu przez HTTP: 1746 ms (POSTMAN)Kluczowe statystyki wygenerowane przez Hibernate:4294453 nanoseconds spent preparing 26 JDBC statements;118572140 nanoseconds spent executing 23 JDBC statements;531069793 nanoseconds spent executing 4 JDBC batches;732899796 nanoseconds spent executing 1 flushes (flushing a total of 2003 entities and 0 collections);5497 nanoseconds spent executing 1 partial-flushes (flushing a total of 0 entities and 0 collections)Widzimy kolejny drastyczny spadek liczby wykonywanych operacji wynikający z tego, że nie musimy tak często odpytywać bazy o kolejne numery sekwencji.Porównajmy teraz łączne wywołania usługiStan początkowy – bez optymalizacji: 9503 msZ batchowymi insertami: 5138 msZ batchowymi insertami i optymalizacją sekwencji: 1746 msDzięki użyciu odpowiednich mechanizmów zmniejszyliśmy czas odpowiedzi naszej usługi o ponad 80%!Warto wspomnieć, że przy tego typu optymalizacjach zmiany w czasach wykonywania żądań mogą być wysoce zależne od użytych technologii, dla konkretnych baz danych istnieją też specyficzne możliwe optymalizacje. Dobrym przykładem tego jest ustawienie obecne w MySQLrewriteBatchedStatements=truepozwalające na zoptymalizowanie liczby zapytań przesyłanych w pakiecie sieciowym.W przypadku optymalizacji takich jak pokazane w tym poście warto zajrzeć pod maskę - do dokumentacji, a nawet kodu źródłowego frameworku, a także zaopatrzyć się w dobre narzędzia wspomagające diagnostykę jak chociażby wyżej przytoczony Zipkin.Warto też strzec się błędów i niepoprawnych użyć mechanizmów frameworków, należy chociażby pamiętać o tym, że aby wykonać batch insert za pomocą klasy JpaRepository z frameworku Spring musimy skorzystać z metody saveAll, a nie save. Osobnej optymalizacji poprzez ustawienie order_updates w plikach konfiguracyjnych może wymagać również uaktualnianie rekordów. Kolejna pułapka o której należy pamiętać związana jest z cache poziomu L1. Jak wiemy, działa ono na poziomie pojedynczej transakcji. Oznacza to, że zapisując dużą liczbę encji w jednej transakcji narażamy się na problemy pamięciowe.Podsumowując, wiemy że ceną za wysoki poziom abstrakcji frameworków jest to, że dużo rzeczy dzieje się poza naszym wzrokiem. Czasem musimy wyjść poza specyfikację interfejsu technologicznego i skorzystać z mechanizów konkretnej implementacji, takie podejście często bazowane jest na eksperymentowaniu by dowiedzieć jaki mechanizm rozwiąże nasz problem. Wymaga to nieraz diagnostycznego spojrzenia na problem oraz zagłębienia się w dokumentację używanej przez nas technologii.",
"url": "/2019/10/21/batchowe-inserty-w-hibernate-droga-ku-szybkosci.html",
"author": "Robert Rudko",
"authorUrl": "/authors/rrudko.html",
"image": "rrudko.jpg",
"highlight": "/assets/img/posts/2019-10-21-batchowe-inserty-w-hibernate-droga-ku-szybkosci/inserty-w-hibernate.jpg",
"date": "21-10-2019",
"path": "pl/2019-10-21-batchowe-inserty-w-hibernate-droga-ku-szybkosci"
}
,


"2019-10-13-wprowadzenie-do-zed-attack-proxy-html": {
"title": "Wprowadzenie do Zed Attack Proxy",
"lang": "pl",
"tags": "programming pentest pentesting tool OWASP security",
"content": "ZAP (Zed Attack Proxy) jest opensourcowym narzędziem tworzonym przez organizację OWASP wspomagającym testy penetracyjne, które służy do znajdowania podatności bezpieczeństwa w aplikacjachwebowych. Działa na zasadzie proxy, dzięki czemu pozwala nie tylko na podglądanie żądań wysyłanych do serwera aplikacji i odpowiedzi z serwera otrzymywanych, ale daje równieżmożliwość debugowania, modyfikowania oraz wysyłania własnych żądań. Jest narzędziem dostosowanym do obsługi HTTP, oferuje prosty sposób na rozszyfrowanie HTTPS poprzez dodanie własnegocertyfikatu do przeglądarki. Dostarcza automatyczne skanery, jak również narzędzia pomagające manualnie testować aplikację.Instalacja oraz konfiguracjaLink do pobrania ZAPaBy móc korzystać ze wszystkich funkcjonalności jakie dostarcza nam ZAP powinniśmy zacząć pracę od konfiguracji przeglądarki, tak by łączyła się ona przez proxy (lokalnie odpalonego ZAPa).Z uwagi na prostą konfigurację polecaną przeglądarką jest Firefox. W menu przeglądarki odnajdujemy Preferencje → Sieć i wchodzimy w Ustawienia. Zaznaczamy “Ręczna konfiguracjaserwerów proxy”. Domyślne wartości dla serwera to 127.0.01, a dla portu 8080.Od tego momentu ZAP działa jako proxy, wszystkie żądania, które wysyłamy do aplikacji oraz wszystkie odpowiedzi, które otrzymujemy, przechodzą od teraz przez ZAPa.HTTPSW poprzednim kroku skonfigurowaliśmy przeglądarkę, by móc podglądać ruch HTTP w sieci. Aby używać ZAPa na stronach wymagających HTTPS, musimy dodać certyfikat ZAPa do naszejprzeglądarki. Certyfikat znajdziemy w Options → Dynamic SSL Certificates.W przeglądarce importujemy certyfikat w Preferencje → Prywatność i bezpieczeństwo → Certyfikaty → Wyświetl certyfikaty… → Importuj. Certyfikat ten jest certyfikatem CA - zaimportowaniego spowoduje dodanie OWASP Root CA do listy organów certyfikacji w naszej przeglądarce:Teraz już możemy podglądać zarówno ruch nieszyfrowany jak i szyfrowany pomiędzy przeglądarką i serwerami, z którymi się łączy. Musimy pamiętać, że od tej chwili nasza przeglądarka jest podatnana atak Man in the middle i nie powinniśmy z niej korzystać w innych celach, niż do testowania. Przejdźmy zatem do zabawy ZAPem.ZAP - wprowadzenieDo prezentacji działania ZAPa wykorzystamy inne narzędzie: Webgoat, które jest celowo podatną aplikacją opensourcową napisaną w Javie umożliwiającą testowanie często spotykanych błędówbezpieczeństwa w aplikacjach korzystających z popularnych opensourcowych komponentów.Korzystając z tych dwóch aplikacji musimy pamiętać, by jedną z nich uruchomić z innym portemniż domyślny 8080. Po stronie ZAPa port możemy zmienić w ustawieniach Options → Local Proxies.Po uruchomieniu aplikacji Webgoat możemy podglądać całą komunikację z serwerem. W zakładce History widzimy historię wszystkich zapytań. Możemy wykluczyć wewnętrzne zapytania Webgoataz historii komunikacji, dzięki czemu stanie się ona bardziej przejrzysta. W tym celu klikamy prawym przyciskiem myszy na jeden z wpisów w historii, wybieramy Exclude from → Proxyi dodajemy URLe, które będą ignorowane (możemy w tym celu użyć wyrażeń regularnych).Z lewej strony programu umiejscowione jest menu. W zakładce Sites, podobnie jak w History, znajdują się wszystkie zapytania do serwera, jednak tutaj mamy zachowaną strukturę zasobówna serwerze aplikacji. W tym miejscu możemy zauważyć również zakładkę Contexts. Konteksty umożliwiają testowanie aplikacji na różnych poziomach dostępu w tym samym momencie - by to osiągnąćdodajemy kontekst dla poszczególnych użytkowników. Możemy też w tym miejscu w zakładce Structure skonfigurować parametry URL.Debugowanie oraz modyfikacja zapytańW zakładce Request oraz Response możemy podejrzeć żądanie i odpowiedź z serwera. ZAP umożliwia przechwytywanie zapytań oraz ich modyfikację. By przetestować działanie tej funkcjonalnościustawiamy breakpointy za pomocą okrągłej zielonej ikonki umieszczonej w górnym pasku menu.Klikając na ikonkę ustawimy breakpointy na każdym zapytaniu oraz odpowiedzi - ikonka po kliknięciu zmienia kolor na czerwony. Następnie w aplikacji Webgoat wprowadzamy dane wejściowe w dowolnym poludo tego przeznaczonym, np. w sekcji General -&amp;gt; HTTP Basics. Na potrzeby naszego testu wprowadziłam tekst “Consdata”.Szczegóły zapytania w trybie debugowania otwierają się w zakładce Break, a poniżej okna z żądaniem znajduje się pole, w którym możemy je modyfikować.Modyfikujemy żądanie wprowadzając tekst “123” w miejsce “Consdata”:W rezultacie otrzymamy odpowiedź z serwera zawierającą zmodyfikowane przez nas dane “The server has reversed your name: 321”, co możemy zaobserwować w ZAPie oraz Webgoacie.Funkcjonalności wspierające testy penetracyjneZAP posiada szereg funkcji umożliwiających przeskanowanie aplikacji w poszukiwaniu różnych zasobów i podatności oraz wykonanie ataków. Jest to jednak narzędzie wspomagające pracę pentestera,bez manualnego przeklikania się przez aplikację nie jest możliwe znalezienie wszystkich podatności. Narzędzia umożliwiające wykonanie skanów oraz ataków znajdziemy klikając prawym przyciskiemmyszy w zakładce Sites na folderze, który chcemy przetestować lub w zakładce History na interesującym nas endpointcie (bądź kilku) w sekcji Attack albo w górnym pasku menu w zakładce Tool.SpiderJedynym z początkowych etapów testów penetracyjnych jest manualne przeszukanie aplikacji w celu znalezienia zasobów znajdujących się na serwerze. Narzędziem, które może zautomatyzować częśćpracy jest Spider. Skanuje on aplikację w poszukiwaniu ukrytych zasobów. W przypadku, gdy chcemy przeszukać zasoby pobierane asynchronicznie powinniśmy użyć skanera AjaxSpider. Należy jednakzwrócić uwage na fakt, że narzędzie to nie zastąpi ręcznego przeszukania aplikacji, ponieważ przechodzi jedynie przez HTMLowe linki na stronie. Nie sprawdzi się również w przypadku aplikacji,gdzie DOM jest generowany dynamicznie, nie obsłuży też eventów innych niż standardowe. Po uruchomieniu narzędzia w aplikacji pojawia się sekcja Spider, w której możemy znależć wyniki skanu.ActiveScanActiveScan jest narzędziem aktywnie skanującym aplikację, wykonującym serię ataków, którego zadaniem jest znalezienie podatności. Wysyła żądania do kolejnych endpointów automatycznie modyfikującich treść, analizuje odpowiedzi i określa na ich podstawie podatności. Jednak tak jak przy Spiderze, tak i tutaj nie obędzie się bez ręcznego przejścia aplikacji. ActiveScan nie potrafi bowiemsamodzielnie wyszukać wszystkich endpointów, trzeba mu je wskazać. Dopiero gdy znajdą się one w historii komunikacji mamy pewność, że skaner je przetestuje. Gdy skaner znajdzie podatnośćzobaczymy czerwoną flagę przy endpointcie, a w zakładce Alerts znajdą się informacje na temat znalezionej podatności.Działanie ActiveScan przetestujemy w miejscu, o którym wiemy, że zawiera błąd bezpieczeństwa. W Webgoacie otwieramy sekcję Injection Flaws → SQL Injection (introduction) → krok 11 i wprowadzamydowolne dane w widocznych polach.Po wprowadzeniu danych zobaczymy w ZAPie w sekcji History URL: http://localhost:9000/WebGoat/SqlInjection/attack8 i będziemy mogli podejrzeć żądanie.ActiveScan uruchomimy na tym endpoincie, by skrócić czas skanowania. Po uruchomieniu narzędzia w ZAPie pojawi się zakładka Active Scan, w której będziemy mogli zobaczyć wszystkie żądania wysyłanedo serwera przez ActiveScan, np.:Jeśli zostaną znalezione błędy, zobaczymy je w zakładce Alerts. Widzimy tu informacje dotyczące URLa, w którym ActiveScan znalazł błąd, jakie dane wejściowe sprowokowały błąd, opis błędu,dodatkowe informacje, proponowane rozwiązanie oraz referencje.W panelu Sites pojawi się również czerwona flaga w miejscu, w którym znaleziony zostal błąd.ForcedBrowseForcedBrowse jest narzędziem, który szuka plików o znanych lub łatwych do przewidzenia nazwach na serwerze aplikacji korzystając z odpowiednich słowników. W ten sposób możemy znaleźć zasoby,które na serwerze nie powinny się znaleźć i stanowią zagrożenie. Może to na przykład być repozytorium, pliki konfiguracyjne, pliki z backupem, panel administracyjny, itp. W ZAPie wbudowanychjest kilka podstawowych słowników, jednak bardziej rozbudowane można znaleźć na githubie, na przykład:https://github.com/danielmiessler/SecLists/blob/master/Discovery/Web-Content/SVNDigger/all.txtSłownik możemy dodać wybierając z górnego menu Tools -&amp;gt; Options -&amp;gt; ForcedBrowse -&amp;gt; Add custom Forced Browse file. Po uruchomieniu narzędzia w ZAPie pojawi się zakładaka Forced Browse, w którejwidzimy pasek postępu zadania, plik z jakiego pobierane są dane oraz wyniki skanu.W rzeczywistości ForcedBrowse bazuje na narzędziu DirBuster, które zostało wbudowane w ZAPa. Czas trwania skanu jest uzależniony od wielu czynników - wydajnosci serwera, wielkosci słownikai zastosowanych na sererze mechanizmów zabezpieczających. Warto zaznaczyć, że skan może zająć nawet do kilkudziesięciu godzin.FuzzNarzędzie Fuzz wykonuje atak za pomocą techniki fuzzingu, czyli wysyłaniu do aplikacji predefiniowanych lub dynamicznie generowanych danych wejściowych w celu sprowokowania błędów.By przeprowadzić ten atak należy wywołać żądanie, zaznaczyć na nim element, który będziemy poddawać modyfikacji, wybrać Fuzz z bocznego menu oraz dodać dane wejściowe (Payloads).Mamy możliwość zdefiniowana różnego rodzaju danych wejściowych w Fuzz -&amp;gt; Payloads -&amp;gt; Add. Mogą to być na przykład stringi, wyrażenia regularne, skrypty, pliki ze zdefiniowanymi wejściami. Następnienależy ustawić odpowiednie kodowanie w zależności od formy w jakiej dane mają być dostarczone do aplikacji.Po ukończeniu fuzzowania należy przeanalizować odpowiedzi na wysyłane żądania - nietypowe odpowiedzi, na przykład zawierające komunikaty o błędach, mogą zasugerować obecność podatności danego typu(na przykład błędy SQL sugerują, że możemy testować SQL injection). Odpowiedzi znajdują się w zakładce Fuzzer, która otwiera się podczas przeprowadzania ataku.W analizie odpowiedzi pomaga posegregowanie ich po wielkości. Większy rozmiar odpowiedzi sugeruje, że możemy znaleźć tam interesujący błąd. W przypadku naszego testu otrzymaliśmy w odpowiedzi danez bazy danych oraz informację potwierdzającą nasz suckes: “You have succeeded! You successfully compromised the confidentiality of data by viewing internal information that you should not haveaccess to. Well done!”.PodsumowanieZAP jest narzędziem dostosowanym do obsługi protokołu HTTP i oferującym łatwy sposób na rozszyfrowanie HTTPS. Dzięki funkcji proxy daje możliwość nie tylko podglądania, ale też modyfikowania i wysyłaniawłasnych żądań. Posiada również wiele dodatkowych funkcjonalności ułatwiających przeprowadzenie testów penetracyjnych. Musimy jednak pamiętać o ograniczeniach podczas korzystania z tego typu narzędzi.Żadne narzędzie nie zautomatyzuje w pełni procesu testowania bezpieczeństwa aplikacji. Wyszukiwanie ewentualnych błędów bezpieczeństwa nie obędzie się bez manualnego przejścia aplikacji i umiejętnejanalizy oraz interpretacji odpowiedzi serwera.Materiały źródłowe  https://github.com/zaproxy/zaproxy/wiki/Downloads  https://github.com/zaproxy/zaproxy/wiki  https://owasp-academy.teachable.com/p/owasp-zap-tutorial  https://github.com/zaproxy/zap-core-help/wiki/HelpIntro  https://github.com/WebGoat/WebGoat  https://github.com/danielmiessler/SecLists/blob/master/Discovery/Web-Content/SVNDigger/all.txt",
"url": "/2019/10/13/wprowadzenie-do-zed-attack-proxy.html",
"author": "Aleksandra Mak",
"authorUrl": "/authors/amak.html",
"image": "amak.jpg",
"highlight": "/assets/img/posts/2019-10-14-wprowadzenie-do-zed-attack-proxy/zed-attack-proxy.jpeg",
"date": "13-10-2019",
"path": "pl/2019-10-14-wprowadzenie-do-zed-attack-proxy"
}
,


"2019-10-03-kafka-companion-html": {
"title": "Kafka Companion",
"lang": "pl",
"tags": "programming kafka event sourcing tool",
"content": "Stalo się standardem, że współczesne narzędzia i biblioteki dystrybuowane są z mniej lub bardziej zaawansowanym interfejsem graficznym. RabbitMQ, będący najpopularniejszym obecnie systemem kolejkowym, wita nas po zainstalowaniu przejrzystym i funkcjonalnym panelem administracyjnym. Może więc budzić zdziwienie, że Apache Kafka nie została wyposażona w żadne GUI. Do dyspozycji dostajemy jedynie zestaw skryptów bashowych, które co prawda pozwalają wykonać wiele czynności administracyjnych, to jednak korzystanie z nich nie należy do przyjemnych. Sprawiają wrażenie pisanych przez różne zespoły developerskie, ponieważ te same parametry potrafią mieć inne nazwy. W dodatku skrypty są bezstanowe - każde wywołanie musi mieć pełen zestaw parametrów identyfikujących serwer i zasoby do których chcemy się odwołać. Naturalnym jest zatem, że do wygodnej i wydajnej pracy warto wdrożyć rozwiązanie, które pozwala w łatwy sposób podejrzeć stan serwera oraz topików na Kafce. Z kolei ze względu na prostotę testowania i diagnostyki, warto mieć możliwość dodawania nowych komunikatów.Dlaczego piszemy własne narzędziePoszukując administracyjnego frontendu do naszej Kafki, zainstalowaliśmy i przetestowaliśmy szereg dostępnych w sieci darmowych aplikacji. Niestety żadna z nich nie spełniła wszystkich wyspecyfikowanych wymagań. Nie pozostało nam zatem nic innego, jak przygotować własne narzędzie. Tak oto powstał Kafka Companion. Uważni czytelnicy naszego bloga zauważą pewnie, że nie jest to pierwszy Companion, który powstał w naszej firmie. Podobne pobudki doprowadziły chociażby do implementacji SQCompaniona, o którym pisał Grzegorz Lipecki w artykule Monitorowanie zespołowych trendów jakości koduPodstawowe funkcjonalnościAplikacja podzielona jest na trzy moduły dostarczające istotnych funkcjonalności podczas codziennej pracy developerskiej z Kafką.Podgląd kondycji klastraEkranem, od którego należałoby rozpocząć, jest lista wszystkich węzłów w klastrze. Znając topologię możemy upewnić się, czy węzły klastra są żywe oraz jakie mają identyfikatory.Podgląd i dodawanie wiadomości do topikuMożliwość podglądania stanu kolejek oraz dodawania nowych wiadomości jest niezwykle istotna podczas rozwijania oraz testowania systemu. W naszym rozwiązaniu wszystkie wiadomości są JSONami, więc dodatkowo widok wiadomości próbuje rozłożyć je do postaci tabeli tak, aby można było przejrzyście zaprezentować stan topiku.  Prezentowane są także metadane związane z numerem partycji oraz offsetem wiadomości.W każdej chwili możemy zarzucić topik dowolną liczbą wiadomości. Wprowadziliśmy także placeholdery pozwalające na różnicowanie wiadomości dodawanych w jednej serii.Podgląd stanu grup konsumentówOstatnim ekranem jest podgląd grup konsumentów.Dowiadujemy się z niego kto konsumuje wiadomości z topików oraz, co bardzo ważne, czy wiadomości są konsumowane na bieżąco.PodsumowanieKafka Companion jest darmowy i dostępny na naszym Githubie. Zachęcam do pobrania, testowania i zgłaszania uwag.Zaproszenie na 4developersJeżeli zainteresowała was tematyka poruszana w tym artykule to serdecznie zapraszam na moje wystąpienie na 4Developers, gdzie wykorzystam Kafka Companiona podczas mojej prezentacji. Wielkopolska edycja 4Developers odbędzie się 18.11. Ścieżki tematyczne, jakie pojawią się w Poznaniu, to: .NET, Architektury Aplikacji, Java, JavaScriptTutaj zdobędziecie bilety",
"url": "/2019/10/03/kafka-companion.html",
"author": "Jacek Grobelny",
"authorUrl": "/authors/jgrobelny.html",
"image": "jgrobelny.jpg",
"highlight": "/assets/img/posts/2019-10-03-kafka-companion/kafka.png",
"date": "03-10-2019",
"path": "pl/2019-10-03-kafka-companion"
}
,


"2019-09-16-haproxy-small-but-handy-html": {
"title": "Haproxy - mała rzecz, a cieszy",
"lang": "pl",
"tags": "programming load-balancing ha server network",
"content": "HAProxy to pakiet wolnego oprogramowania, który najczęściej pełni rolę reverse-proxy, zapewniając load-balancing i high-availability serwerów aplikacji.Klienci (np. przeglądarki) nie łączą się bezpośrednio z serwerami aplikacji, lecz właśnie z reverse-proxy, które stosując dodatkowe reguły przekazuje żądania do serwerów aplikacji i odpowiedzi z powrotem do klienta.Projekt jest aktywnie rozwijany od 2002 roku i coraz częściej wykorzystywany z powodu popularyzacji systemów rozproszonych, w szczególności architektur opartych o mikroserwisy. Używają go takie tuzy jak Digital Ocean, Github, Dropbox, Instagram, czy StackOverflow. Jest też komponentem platformy kontenerowej OpenShift.Pakiet jest dostępny dla wszystkich popularnych dystrybucji Linuxa, a także w postaci obrazu Dockerowego.Standardowe zastosowania HAProxy.Zapewnienie Load BalancinguJedno z podstawowych zastosowań HAProxy to software’owy load-balancer. Mając kilka węzłów naszej aplikacji chcielibyśmy rozdzielać ruch pomiędzy nimi. W konfiguracji HAProxy deklarujemy obiekt zwany backendem, który reprezentuje klaster naszych serwerów aplikacji. Następnie deklarujemy obiekt zwany frontendem, na który będą kierowani klienci oraz reguły kierowania ruchu z frontendu do backendu:frontend my-load-balancer    bind 172.19.0.1:81    default_backend my-application-servers    backend my-application-servers    balance roundrobin    server my-app-server-1 172.19.0.2:8080    server my-app-server-2 172.19.0.3:8080    server my-app-server-3 172.19.0.4:8080    Powyższa konfiguracja powoduje że połączenia na endpoint 172.19.0.1:81 będą przekierowywane na jeden z trzech podanych serwerów w sposób równomierny.Zapewnienie High AvailabilityHAProxy potrafi monitorować stan serwerów zadeklarowanych w sekcji backend i zaprzestać kierowania ruchu na serwery, które przestały poprawnie funkcjonować.W konfiguracji serwera możemy określić m.in. jak często HAProxy ma sprawdzać status serwera, ile razy weryfikacja musi się nie udać, żeby serwer został uznany za dysfunkcyjny oraz ilerazy po wykluczeniu serwera weryfikacja musi się powieść, żeby serwer znów został uznany za “zdrowy”.Przykładowo:    server my-app-server-1 172.19.0.2:8080 check inter 5s fall 3 downinter 1m raise 5oznacza że:  status serwera ma być sprawdzany co 5 sekund (inter 5s)  serwer ma zostać wykluczony jeśli nie powiodą się 3 kolejne sprawdzenia (fall 3)  wykluczony serwer ma być sprawdzany co minutę (downinter 1m)  serwer ma zostać ponownie uznany za sprawny jeśli powiedzie się kolejne 5 sprawdzeń (raise 5)TLS/SSLJeżeli chodzi o obsługę szyfrowanych połączeń TLS/SSL, HAProxy może działać w jednym z trybów:  zwykłe proxy  terminacja szyfrowania  ponowne szyfrowanie ruchuZwykłe proxyW tym trybie HAProxy zwyczajnie przekazuje strumień bajtów z frontendu do backendu, nie wnikając w to, że ruch jest szyfrowany.Terminacja szyfrowaniaW tym trybie HAProxy otrzymuje szyfrowany ruch z frontendu, odszyfrowuje go (z użyciem klucza prywatnego) i przekazuje do backendu w postaci niezaszyfrowanej. Przykładowa konfiguracja:frontend my-terminating-load-balancer    bind 172.19.0.1:443 ssl crt /etc/ssl/certs/my-certs.pem    default_backend my-application-servers    backend my-application-servers    balance roundrobin    server my-app-server-1 172.19.0.2:8080    server my-app-server-2 172.19.0.3:8080Ponowne szyfrowanie ruchuW tym trybie HAProxy otrzymuje szyfrowany ruch z frontendu, odszyfrowuje go (z użyciem klucza prywatnego), a następnie szyfruje ponownie i przekazuje do backendu. Przykładowa konfiguracja:frontend my-terminating-load-balancer    bind 172.19.0.1:443 ssl crt /etc/ssl/certs/my-certs.pem    default_backend my-secure-application-servers    backend my-secure-application-servers    balance roundrobin    server my-secure-app-server-1 172.19.0.2:8443 ssl    server my-secure-app-server-2 172.19.0.3:8443 sslW odróżnieniu od trybu zwykłego proxy, dzięki odszyfrowaniu HAProxy może manipulować żądaniami HTTP, które przekierowuje.Session affinityHAProxy pozwala na taką konfigurację, że requesty od danego użytkownika zawsze będą kierowane na ten sam serwer aplikacji. Taka potrzeba zachodzi m.in. kiedy używamy lokalnych sesji użytkownika, które istnieją tylko na tym serwerze aplikacji, na którym zostały utworzone.Przykładowa konfiguracja, która opiera się na prefiksie ciasteczka sesyjnego:frontend my-sticky-load-balancer    bind 172.19.0.1:81     default_backend my-application-servers    backend my-application-servers    cookie JSESSIONID prefix nocache    server my-app-server-1 172.19.0.2:8080 cookie node1    server my-app-server-2 172.19.0.3:8080 cookie node2Przy takiej konfiguracji wszystkie requesty, które niosą ciasteczko sesyjne z prefiksem “node1” będą kierowane na pierwszy serwer aplikacji.ACLHAProxy pozwala konfigurować reguły (ACL - Access Control List), które określają w jaki sposób poszczególne żądania mają być obsługiwane. Przykładowa konfiguracja:frontend my-acl-load-balancer    bind 172.19.0.1:81     acl is-passive method GET    use_backend my-passive-application-server if is-passive    default_backend my-active-application-server    backend my-passive-application-server    server my-passive-app-server-1 172.19.0.2:8080     backend my-active-application-server    server my-active-app-server-1 172.19.0.3:8080 Przy takiej konfiguracji żądania z metodą GET będą kierowane na pasywny serwer aplikacji, a wszystkie pozostałe na aktywny serwer aplikacji.Zastosowania w warsztacie programistycznymPoza klasycznym użyciem jako reverse-proxy zapewniające load-balancing i high-availability, HAProxy może być z powodzeniem wykorzystane w warsztacie programistycznym. Przedstawię poniżej kilka zastosowań z własnego doświadczenia.Routing, który łatwo zmienićRozwijając aplikację, która wywołuje serwisy z innych aplikacji, chcemy niekiedy móc przełączać się między różnymi adresami tych zewnętrznych usług, np. możemy chcieć sprawnie przełączać się między prawdziwymi aplikacjami oraz ich zmockowanymi wersjami. Często restart naszej aplikacji jest długotrwały, a zmiana namiarów na zewnętrzne usługi wymaga edycji więcej niż jednego pliku.W takim przypadku HAProxy może nam posłużyć jako wygodna “centralka”, w której będziemy się sprawnie przełączać między różnymi wersjami zewnętrznych usług.Konfigurując HAProxy w ten sposób:frontend my-forward-proxy    bind 172.19.0.1:81    default_backend my-external-services    backend my-external-services    server my-external-server-1 172.19.0.2:8080backend my-mocked-services    server my-mocked-server-1 172.19.0.3:8080i ustawiając w naszej aplikacji adres zewnętrznych usług na 172.19.0.1:81, możemy bez restartu aplikacji zmieniać jej zewnętrzne zależności przez zmianę wartości “default_backend” i szybki “reload” HAProxy.Podglądanie TLS/SSLJeśli serwisy w naszym systemie przesyłają sobie dane przez szyfrowane połączenia TLS/SSL, może to stanowić przeszkodę w debugowaniu komunikacji przy pomocy analizatorów ruchu sieciowego, takich jak tcpdump, czy Wireshark. Możemy jednak pomiędzy serwisami ustawić pośrednika w postaci HAProxy, które będzie terminowało szyfrowane połączenie i umożliwiało nam podglądanie ruchu sieciowego.Opóźnienia requestówPodczas rozwijania aplikacji zachodzi czasem potrzeba przetestowania jak zachowa się ona w przypadku dłuższego niż oczekiwane przetwarzania w wywoływanych usługach zewnętrznych.Jeśli pośrednikiem w dostępie do zewnętrznych usług jest HAProxy, to mamy możliwość zasymulowania takiego opóźnienia.HAProxy samo w sobie nie posiada takiej funkcjonalności, ale pozwala na rozszerzanie możliwości za pomocą skryptów pisanych w języku Lua.Przykładowy skrypt dodający opóźnienie:function delay_request(txn)    core.msleep(15000)endcore.register_action(&quot;delay_request&quot;, { &quot;http-req&quot; }, delay_request);Załadowanie skryptu do HAProxy:    lua-load /etc/haproxy/delay.luai użycie w konfiguracji:frontend my-delay-frontend    bind 172.19.0.1:81    mode http    http-request lua.delay_request    default_backend my-delay-backendbackend my-delay-backend    server my-external-server-1 172.19.0.2:8080Prosty serwer statycznej zawartościW dość przewrotny sposób możemy wykorzystać HAProxy jako prosty serwer statycznych plików. Dla każdego backendu możemy skonfigurować pliki, które mają zostać zaserwowane w przypadku określonych statusów HTTP.Z drugiej strony, w przypadku gdy w konfiguracji backendu nie zdefiniowano żadnego serwera, HAProxy generuje status HTTP 503.Łącząc te dwa fakty możemy skonfigurować frontend, który dla określonych requestów będzie zwracał statyczny plik:frontend my-frontend    bind 172.19.0.1:81    mode http    use_backend my-backend-static if { path_end /index.html }    default_backend my-backend-servicesbackend my-backend-static    mode http    errorfile 503 /etc/haproxy/index.html    backend my-backend-services    server my-external-server-1 172.19.0.2:8080Podłączanie się do sesjiW środowisku deweloperskim zdarza się, że chcielibyśmy podłączyć się przeglądarką do istniejącej na serwerze sesji użytkownika (znając oczywiście identyfikator/token sesji).Nowoczesne desktopowe przeglądarki zazwyczaj pozwalają nam ręcznie dodać ciasteczko sesyjne do zbioru ciasteczek danej witryny. Co jednak, jeśli musimy obsłużyć również te mniej lubiane przeglądarki albo wersje mobilne? Tu z pomocą również może przyjść nam HAProxy:frontend my-setcookie-frontend    bind 172.19.0.1:81    mode http    default_backend my-setcookie-backendbackend my-setcookie-backend    mode http    http-request redirect location http://172.19.0.1:80/\\r\\nSet-Cookie:\\ JSESSIONID=%[urlp(session_id)] code 302Przy takiej konfiguracji, wejście z dowolnej przeglądarki na adres http://172.19.0.1:81/?session_id=123456 spowoduje przekierowanie na http://172.19.0.1:80 z ustawionym już ciasteczkiem sesyjnym (wykorzystujemy tu fakt że ciasteczka ustawione dla domeny nie uwzględniają portów).PodsumowanieJak widać, HAProxy ma wiele klasycznych i mniej klasycznych zastosowań.Jego niewątpliwym atutem jest stosunkowo prosta konfiguracja i runtime’owa wydajność oraz “lekkość” (zaczytanie zmienionej konfiguracji odbywa się zwykle w ułamku sekundy).Jest to narzędzie, które polecam do toolboxa każdego architekta systemów rozproszonych, a także do warsztatu deweloperskiego zwinnego programisty.",
"url": "/2019/09/16/haproxy-small-but-handy.html",
"author": "Tomasz Lewandowski",
"authorUrl": "/authors/tlewandowski.html",
"image": "tlewandowski.jpg",
"highlight": "/assets/img/posts/2019-09-19-haproxy-mala-rzecz-a-cieszy/haproxy.jpg",
"date": "16-09-2019",
"path": "pl/2019-09-19-haproxy-small-but-handy"
}
,


"2019-09-05-kafka-retry-dlq-html": {
"title": "Niezawodne dostarczanie zdarzeń w Apache Kafka oparte o ponawianie i DLQ",
"lang": "pl",
"tags": "programming kafka event sourcing retry dlq dlt",
"content": "W każdym dostatecznie złożonym systemie informatycznym dochodzimy w pewnym momencie do miejsca, w którym musimy sobie odpowiedzieć na pytanie: a co jeśli coś pójdzie nie tak. Jeśli mamy szczęście, to może się okazać, że rozwiązania, które wybraliśmy, dostarczają nam gotowe narzędzia do radzenia sobie w sytuacjach wyjątkowych. Może też się okazać, że nie mieliśmy tyle szczęścia i wybraliśmy Kafkę…W niniejszym wpisie znajdziesz odpowiedź na to, dlaczego w Kafce nie ma DLQ (ang. Dead Letter Queue) oraz jak sobie poradzić w sytuacji, gdy potrzebujesz takiego mechanizmu w swoim systemie.Dlaczego w Kafce nie ma DLQ?Zacznijmy zatem od odpowiedzi na pytanie. Większość popularnych systemów kolejkowych takich jak RabbitMQ czy ActiveMQ ma wbudowane systemy odpowiedzialne za niezawodne dostarczanie komunikatów. Dlaczego zatem Kafka nie oferuje takowego. Odpowiedź na to pytanie ściśle związana jest z jednym z rozwiązań architektonicznych leżących u podstaw działania Kafki: głupi broker i sprytny konsument (ang. dumb broker / smart consumer). Wzorzec ten sprowadza się do tego, że ciężar logiki związanej z obsługą odczytów przenoszony jest na konsumenta. Konsekwencją takiego podejścia jest brak gotowego rozwiązania mogącego wspomóc konsumenta w przypadku wystąpienia problemu podczas przetwarzania komunikatu. Broker jest zainteresowany tylko jedną informacją: pozycją, na której konsument zakończył przetwarzanie (ang. committed offset). Oczywiście zawsze można rzec, że w tej sytuacji należy dobrać odpowiednie narzędzie do problemu i zastosować system kolejkowy mający takie wsparcie. Nie zawsze jednak mamy nieograniczoną swobodę wprowadzania wielu rozwiązań w jednym systemie. Jeśli tak jak ja wybraliście Kafkę jako silnik rejestrujący zdarzenia, to w przypadku wystąpienia opisywanego problemu musicie poradzić sobie sami i odpowiednio go oprogramować.Jak sobie radzić z błędamiWyobraźmy sobie sytuację, w której elementem procesu obsługi zdarzenia jest komunikacja z zewnętrznym systemem. Musimy podjąć decyzję jak ma zachować się konsument w momencie, gdy zewnętrzny system odpowiada w inny sposób, niż się spodziewaliśmy albo, co gorsza - w ogóle nie odpowiada. Jest wiele strategii obsługi takiej sytuacji. Ja na potrzeby tego artykułu wybrałem cztery, które doprowadzą nas do rozwiązania, którego implementacja zostanie zaprezentowana w kolejnych akapitach.Brak obsługiBardzo popularna i często stosowana strategia obsługi sytuacji wyjątkowych, to brak reakcji. Może to potwierdzić każdy programista. Na powyższym rysunku prostokąty oznaczają kolejne wiadomości w topiku. Gdy konsument napotka problem z przetwarzaniem komunikatu o offsecie 1486, ignoruje go i przechodzi do następnego. I mimo że takie podejście wydaje się niezbyt rozsądnym rozwiązaniem, to istnieją sytuacje, gdy utrata części komunikatów nie niesie za sobą ryzyka. Za przykład można podać wszelkie rozwiązania przechowujące i analizujące zachowanie użytkowników w aplikacji. Ponieważ zadaniem takiego systemu jest zbieranie danych statystycznych, utrata pojedynczych zdarzeń nie wpłynie znacząco na wyniki. Ważne jest jednak, żeby dysponować skutecznym monitoringiem, który wychwyci sytuację, w której utrata komunikatów przekracza pewien arbitralnie ustalony poziom.Nieskończone ponawianie w miejscuGdy nie możemy pozwolić sobie na utratę komunikatów, najprostszym podejściem jest ponawianie do skutku. Oczywistą konsekwencją jest tzw. zatrzymanie świata. Dopóki błąd nie zostanie poprawiony albo zewnętrzny system udrożniony - żaden kolejny komunikat nie zostanie przetworzony. Takie rozwiązanie jest konieczne w przypadku, gdy chcemy zachować kolejność przetwarzania zdarzeń systemie. W ten scenariusz tym bardziej wpisuje się potrzeba stałego monitoringu.Skończone ponawianie w miejscu z topikiem błędów Omawiane do tej pory strategie zachowują kolejność przetwarzania zdarzeń. Jest to niezwykle istotne w sytuacji, gdy zdarzenia są od siebie zależne i spójność naszego systemu opiera się na kolejności przetwarzania. Nie zawsze jednak zdarzenia mają taką właściwość i brak konieczności zachowania kolejności otwiera przed nami nowe możliwości. Wyobraźmy sobie co się stanie, jak nieco poluzujemy wymaganie bezwzględnego zachowania kolejności. Załóżmy, że próbujemy przez jakiś czas ponawiać, ponieważ statystyka i doświadczenie podpowiada nam, że 99% problemów z przetwarzaniem komunikatów jest chwilowych i samoczynnie ustępuje po pewnym czasie. Dodatkowo komunikaty, których nie udało się przetworzyć, kopiujemy na oddzielny topik traktowany jako DLQ. Dzięki temu mamy od razu wyłuskane problematyczne wiadomości i możemy uruchomić na nich osobną grupę konsumentów.Krótkie wyjaśnienie, dlaczego komunikaty są kopiowane a nie przenoszone. Odpowiedź jest bardzo prosta - nie mogą być przenoszone. Wynika to z kolejnego fundamentu architektonicznego Kafki czyli niezmienności topików (ang. topics immutability). Niezależnie jaka była przyczyna błędu, komunikat na zawsze pozostanie utrwalony. Istnieją sposoby na radzenie sobie z tym tematem i wrócimy do tego później.Skończone ponawianie na wydzielonym topikuDochodzimy niniejszym do naszego ostatecznego rozwiązania. Skoro mamy osobny topik dla zepsutych wiadomości to może warto wprowadzić kolejny, na którym odbywa się ponawianie. W tym modelu jeszcze bardziej luzujemy konieczność zachowania kolejności, ale dostajemy w zamian możliwość bezprzerwowego przetwarzania głównego topiku. W konsekwencji nie zatrzymujemy świata a wiadomości kaskadowo kopiowane są najpierw na topik wiadomości ponawianych a w przypadku niepowodzenia - topik DLQ (technicznie powinniśmy nazwać go DLT, ale zostańmy przy akronimie DLQ, jako że jest on dobrze kojarzony z tego rodzaju technikami).W systemie, na podstawie którego powstał ten wpis, występuje wszystkie cztery opisywane warianty postępowania w sytuacji awaryjnej. Wyzwanie polega na dopasowania odpowiedniej metody do natury danych przetwarzanych w topiku. Warto też zaznaczyć, że należy uczyć się od największych i dwa ostatnie modele są mocno inspirowane sposobem, w jaki Kafkę w swoich systemach używa Uber.ImplementacjeMając za sobą część teoretyczną, możemy w końcu przejść do kodu. Poglądową aplikację, z której pochodzą poniższe snippety, znajdziecie na Githubie. Od razu zaznaczam, że nie jest to coś, z czym można by pójść na produkcję. Chodzi bardziej o zarysowanie sposobu, w jaki można wdrożyć ostatnią strategię.Konsument głównego topiku    @KafkaListener(topics = &quot;${topic.main}&quot;)    public void consumeFromMainTopic(String message,                                     @Header(KafkaHeaders.RECEIVED_MESSAGE_KEY) String key,                                     @Header(KafkaHeaders.OFFSET) String offset) {        log.info(&quot;Consume from main topic [key={}, offset={}, message={}]&quot;, key, offset, message);        Message serializedMessage;        try {            serializedMessage = objectMapper.readValue(message, Message.class);            restTemplate.getForEntity(POSTMAN_RESOURCE_URL + serializedMessage.getAction(), String.class);            log.info(&quot;Done processing [key={}, offset={}]&quot;, key, offset);        } catch (Exception e) {            log.error(&quot;Cannot handle message: {}&quot;, e.getMessage());            copyMessageToRetry(message);        }    }Większość przykładów i kursów używa niskopoziomowego klienta dostarczonego przez Apache, który może przerazić mniej doświadczonych programistów konstrukcją wykorzystującą nieskończoną pętlę. Nie ma się czego bać, jest to po prostu konstrukcja wynikająca ze sposobu w jaki klient Kafki działa. Nie zmienia to jednak faktu, że nawet prosty odczyt wymaga nieco kodu. Można to znacznie uprościć, wykorzystując implementację, która opakowuje boilerplate kod, dostarczoną przez Springa. Tak też zostało to zaproponowane w niniejszym przykładzie, mamy więc Kafka Listenera zapiętego na zdarzenia z głównego topiku. Dodatkowo wstrzykujemy elementy nagłówka wiadomości, aby można było na podstawie logów prześledzić procesowanie. Sama implementacja jest w tym przypadku bardzo prosta. Wywołujemy zewnętrzny serwis i w przypadku błędów, kopiujemy wiadomość do topiku realizującego ponawianie.W celu symulowania problemów z czasem odpowiedzi zewnętrznego systemu, wykorzystamy tutaj publicznie dostępne API Postmana a w szczególności endpoint /delay. Dzięki mnogości dostępnych zachowań Postman Echo świetnie sprawdza się w testach integracyjnych z zewnętrznym systemem.Konsument topiku do ponawiania    @KafkaListener(topics = &quot;${topic.retry}&quot;)    public void consumeFromRetryTopic(String message,                                      @Header(KafkaHeaders.RECEIVED_MESSAGE_KEY) String key,                                      @Header(KafkaHeaders.OFFSET) String offset) {        log.info(&quot;Consume from retry topic [key={}, offset={}, message={}]&quot;, key, offset, message);        Message serializedMessage;        long loop = 1;        boolean success = false;        while (loop &amp;lt;= 3 &amp;amp;&amp;amp; !success) {            try {                serializedMessage = objectMapper.readValue(message, Message.class);                Thread.sleep(loop * 500);                log.info(&quot;Retrying message in loop {}&quot;, loop);                restTemplateForRetrying.getForEntity(POSTMAN_RESOURCE_URL + serializedMessage.getAction(), String.class);                success = true;                log.info(&quot;Done processing [key={}, offset={}]&quot;, key, offset);            } catch (Exception e) {                log.error(&quot;Cannot handle message {}&quot;, e.getMessage());            }            loop++;        }        if (!success) {            copyMessageToDlq(message);        }    }Konsument topiku ponawiającego podejmuje trzy próby przetworzenia wiadomości. Realizuje on też koncept polegający na tym, że każda następna próba jest bardziej oddalona w czasie. W tej konkretnej implementacji jest to po prosty wynik mnożenia numeru iteracji przez 500. W docelowym rozwiązaniu warto rozważyć wykładniczy wzrost odstępów pomiędzy kolejnymi próbami. Jeżeli uda się przetworzyć wiadomość to konsument przechodzi do kolejnej, jeśli nie - kopiuje do DLQ.Konsument DLQ    @KafkaListener(topics = &quot;${topic.dlq}&quot;)    public void consumeFromDlqTopic(String message,                                    @Header(KafkaHeaders.RECEIVED_MESSAGE_KEY) String key,                                    @Header(KafkaHeaders.OFFSET) String offset) {        log.error(&quot;Consume from DLQ topic [key={}, offset={}, message={}]&quot;, key, offset, message);    }W przypadku podglądowej aplikacji konsument DLQ wypisuje tylko wiadomość na ekranie. Natomiast w docelowych rozwiązaniach jest to dobre miejsce na zbieranie statystyk o częstotliwości oraz przyczynie błędów i uruchamianie w systemie alertów.Uruchomienie przykładuPo uruchomieniu aplikacji możemy zacząć bombardować ją wiadomościami. Użyję w tym celu narzędzia Kafka Companion, które sami przygotowaliśmy w trakcie prac z Kafką. Aplikacja jest darmowa i dostępna na naszym GitHubie. O tym jakie ma możliwości i dlaczego powstało, będziecie mogli przeczytać w moim kolejnym wpisie na blogu.Poprawnie przetworzona wiadomośćConsume from main topic [key=1, offset=0, message={    &quot;id&quot; : &quot;5dad21b1-9f5c-4b67-a583-dfad8b00b0b5&quot;,    &quot;action&quot; : &quot;0&quot;}]Done processing [key=1, offset=0]Wysłanie wiadomości z action=0 spowoduje, że Postman Echo odpowiada bez zwłoki i przetwarzanie kończy się na głównym topiku.Wiadomość ponawiana - poprawnie przetworzonaConsume from main topic [key=1, offset=2, message={    &quot;id&quot; : &quot;646ad855-fe50-4e96-ba1e-78dfa939acef&quot;,    &quot;action&quot; : &quot;1&quot;}]Cannot handle message: I/O error on GET request for &quot;https://postman-echo.com/delay/1&quot;: Read timed outCopying message [target=external-retry, key=2019-09-01]Consume from retry topic [key=2019-09-01, offset=1, message={    &quot;id&quot; : &quot;646ad855-fe50-4e96-ba1e-78dfa939acef&quot;,    &quot;action&quot; : &quot;1&quot;}]Retrying message in loop 1Done processing [key=2019-09-01, offset=1]Aplikacja korzysta z dwóch instancji RestTemplate o innej konfiguracji timeoutów. Ten główny czeka maksymalnie 900ms, podczas gdy ten ponawiający 1900ms. Jest to wygodne na nasze potrzeby prezentacyjne, ale warto także zaznaczyć, że w docelowym rozwiązaniu również należy rozważyć rozdzielenie konfiguracji. Ponieważ w tym modelu główny topik nie jest blokowany przez niepowodzenia, można bardziej liberalnie skonfigurować połączenie do zewnętrznego systemu w przypadku konsumenta ponawiającego.Wiadomość w DLQConsume from main topic [key=1, offset=3, message={    &quot;id&quot; : &quot;77954f26-583b-4e55-9fe8-2a1f7c204541&quot;,    &quot;action&quot; : &quot;5&quot;}]Cannot handle message: I/O error on GET request for &quot;https://postman-echo.com/delay/5&quot;: Read timed outCopying message [target=external-retry, key=2019-09-01]Consume from retry topic [key=2019-09-01, offset=2, message={    &quot;id&quot; : &quot;77954f26-583b-4e55-9fe8-2a1f7c204541&quot;,    &quot;action&quot; : &quot;5&quot;}]Retrying message in loop 1Cannot handle message I/O error on GET request for &quot;https://postman-echo.com/delay/5&quot;: Read timed outRetrying message in loop 2Cannot handle message I/O error on GET request for &quot;https://postman-echo.com/delay/5&quot;: Read timed outRetrying message in loop 3Cannot handle message I/O error on GET request for &quot;https://postman-echo.com/delay/5&quot;: Read timed outCopying message [target=external-dlq, key=2019-09-01]Consume from DLQ topic [key=2019-09-01, offset=0, message={    &quot;id&quot; : &quot;77954f26-583b-4e55-9fe8-2a1f7c204541&quot;,    &quot;action&quot; : &quot;5&quot;}]Trzeci i ostatni przykład zadaje 5 sekundowe opóźnienie i wiemy z naszej konfiguracji, że taki komunikat na pewno nie zostanie przetworzony i trafi ostatecznie do DLQ. Wprawne oko zauważy, że w podczas kopiowania nadawany jest inny klucz, w tym przypadku oznaczający dzień wystąpienia błędu. Będzie to istotne w kolejnym punkcie.DLQ a grupy konsumentówMarcin Mergo, którego możecie znać jako autora artykułu Czy Apache Kafka nadaje się do Event Sourcingu?, zapytał mnie ostatnio jak ma się ponawianie na oddzielnych topikach do sytuacji, gdy mamy wiele różnych grup konsumentów. Na pierwszy rzut oka mogłoby się wydawać, że podobnie jak w RabbitMQ, topik ponawiający i DLQ są ściśle powiązane z głównym topikiem. Nic bardziej mylnego. Koncepcja grup konsumentów działających w Kafce na tym samym topiku, ale mających inną implementację powoduje, że mechanizm ponawiający musi być powiązany z konkretną grupą. W szczególności różne grupy mogą mieć inną logikę ponawianie i obsługi błędów. Sytuacja jeszcze bardziej się komplikuje, gdy grupy konsumentów są w jakiś sposób od siebie zależne. Jedna grupa może bazować na fakcie, że inna grupa poprawnie przetworzyła dany komunikat. Trzeba wtedy bardzo rozważnie dostosować mechanizmy ponawianie i obsługi błędów tak, aby zachować spójność przetwarzania komunikatów.Sprzątanie.Na zakończenie pozostaje jeszcze rozwiązanie problemu związanego z redundancją danych wynikającą z faktu kopiowania wiadomości pomiędzy topikami. W przypadku głównego topika mamy sytuację, że każda wiadomość, która ostatecznie trafiła do DLQ uznawana jest za uszkodzoną. Jeśli w naszej aplikacji jest możliwość ponownego przetworzenia strumienia wiadomości, to musimy jakoś obsłużyć tę sytuację. Istnieją co najmniej dwa rozwiązania:  rejestr uszkodzonych wiadomości - może być budowany automatycznie na podstawie wiadomości trafiających do DLQ. Składają się na niego offsety wiadomości z głównego topiku. Podczas ponownego przetworzenia konsument, wiedząc o rejestrze, pomija wszystkie oznaczone w nim wiadomości,  kompaktowanie - napisałem wcześniej, że nie można zmieniać i usuwać wiadomości w topiku. Jest od tej reguły wyjątek - mechanizm kompaktowania topiku. W największym skrócie działa to w ten sposób, że broker uruchamia cyklicznie zadanie, które przegląda topik, zbiera wiadomości o tym samym kluczu i pozostawia tylko tą najnowszą. Trik polega na tym, żeby wstawić do strumienia wiadomość o tym samym kluczu co uszkodzona, ale o pustej treści. Konsument musi wcześniej być przygotowany na obsługę takich wiadomości.Można obie techniki stosować jednocześnie, należy jednak pamiętać, że offsety skompaktowanych wiadomości znikną bezpowrotnie z topiku.Topik retry zawiera wiadomości, które po przetworzeniu nie mają żadnej wartości, więc w tym przypadku wystarczy skonfigurować retencję, czyli czas życia wiadomości. Trzeba tylko pamiętać, żeby retencja nie była krótsza, niż najdłuższy możliwy czas przetwarzania pojedynczej wiadomości.Topik DLQ powinien zawierać wiadomości dopóki, dopóty nie zostaną zdiagnozowane a system - poprawiony. Jako, że ten czas nie jest łatwy do ustalenia to nie wchodzi w rachubę retencja. Stąd też trik z kluczami opartymi na datach. Jeśli uznajemy, że incydenty z określonego dnia zostały rozwiązane, to wprowadzamy do DLQ pusty komunikat z kluczem takim jak dzień i przy najbliższej sesji kompaktowania - wszystkie wiadomości zostaną usunięte z DLQ.PodsumowanieW ten oto sposób dobrnęliśmy do końca. Liczę, że udało mi się zaprezentować na tym prostym przykładzie, że iteracyjne podejście do problemu potrafi doprowadzić nas do ciekawych i skutecznych rozwiązań.Zaproszenie na 4developersJeżeli zainteresowała was tematyka poruszana w tym artykule to serdecznie zapraszam was na moje wystąpienie na 4Developers, gdzie postaram się ten temat jeszcze bardziej zgłębić. Wielkopolska edycja 4Developers odbędzie się 18.11. Ścieżki tematyczne, jakie pojawią się w Poznaniu, to: .NET, Architektury Aplikacji, Java, JavaScriptTutaj zdobędziecie bilety",
"url": "/2019/09/05/kafka-retry-dlq.html",
"author": "Jacek Grobelny",
"authorUrl": "/authors/jgrobelny.html",
"image": "jgrobelny.jpg",
"highlight": "/assets/img/posts/2019-09-05-kafka-retry-dlq/apache-kafka.png",
"date": "05-09-2019",
"path": "pl/2019-09-05-kafka-retry-dlq"
}
,


"2019-08-22-view-encapsulation-w-angularze-html": {
"title": "View Encapsulation w Angularze - czyli o kapsułkowaniu słów kilka",
"lang": "pl",
"tags": "angular view encapsulation kapsułkowanie",
"content": "Tworząc komponenty w Angularze mamy możliwość zarządzania kapsułkowaniem (enkapsulacją) stylów - czyli tym jak style z jednego komponentu wpływają na inne komponenty.Zanim omówimy kapsułkowanie, wyjaśnijmy w kilku słowach czym jest Shadow DOM.Shadow DOMShadow DOM wprowadza kapsułkowanie do DOM-u. Pozwala to odseparować styl i kod potrzebny do wyświetlenia elementu od dokumentu, w którym się znajduje. Przykładem może być np. element HTML &amp;lt;video&amp;gt;&amp;lt;video width=&quot;320&quot; height=&quot;240&quot;&amp;gt;    &amp;lt;source src=&quot;movie.mp4&quot; type=&quot;video/mp4&quot;&amp;gt;    &amp;lt;source src=&quot;movie.ogg&quot; type=&quot;video/ogg&quot;&amp;gt;&amp;lt;/video&amp;gt;Po włączeniu opcji wyświetlania Shadow Root w przeglądarce (na przykładzie Google Chrome):DevTools &amp;gt; Settings &amp;gt; Preferences &amp;gt; Elementsmożemy zobaczyć z czego tak naprawdę składa się element &amp;lt;video&amp;gt;:&amp;lt;video width=&quot;320&quot; height=&quot;240&quot;&amp;gt;    #shadow-root        &amp;lt;div pseudo=&quot;-webkit-media-controls&quot; class=&quot;sizing-small phase-ready state-stopped&quot;&amp;gt;            &amp;lt;div pseudo=&quot;-internal-media-controls-loading-panel&quot; aria-label=&quot;buforowanie&quot; aria-live=&quot;polite&quot;                style=&quot;display: none;&quot;&amp;gt;&amp;lt;/div&amp;gt;            &amp;lt;div pseudo=&quot;-webkit-media-controls-overlay-enclosure&quot;&amp;gt;&amp;lt;input                    pseudo=&quot;-internal-media-controls-overlay-cast-button&quot; type=&quot;button&quot;                    aria-label=&quot;odtwarzanie na urządzeniu zdalnym&quot; style=&quot;display: none;&quot;&amp;gt;&amp;lt;/div&amp;gt;            &amp;lt;div pseudo=&quot;-webkit-media-controls-enclosure&quot;&amp;gt;                &amp;lt;div pseudo=&quot;-webkit-media-controls-panel&quot;&amp;gt;                    &amp;lt;div pseudo=&quot;-internal-media-controls-scrubbing-message&quot; style=&quot;display: none;&quot;&amp;gt;&amp;lt;/div&amp;gt;                    &amp;lt;div pseudo=&quot;-internal-media-controls-button-panel&quot;&amp;gt;&amp;lt;input type=&quot;button&quot;                            pseudo=&quot;-webkit-media-controls-play-button&quot; aria-label=&quot;odtwórz&quot; class=&quot;pause&quot; style=&quot;&quot;&amp;gt;                        &amp;lt;div aria-label=&quot;upłynęło: 0:00&quot; pseudo=&quot;-webkit-media-controls-current-time-display&quot; style=&quot;&quot;&amp;gt;0:00                        &amp;lt;/div&amp;gt;                        &amp;lt;div aria-label=&quot;pozostało: / 0:12&quot; pseudo=&quot;-webkit-media-controls-time-remaining-display&quot; style=&quot;&quot;&amp;gt;                            /                            0:12&amp;lt;/div&amp;gt;                        &amp;lt;div pseudo=&quot;-internal-media-controls-button-spacer&quot;&amp;gt;&amp;lt;/div&amp;gt;                        &amp;lt;div pseudo=&quot;-webkit-media-controls-volume-control-container&quot; class=&quot;closed&quot; style=&quot;&quot;&amp;gt;                            &amp;lt;div pseudo=&quot;-webkit-media-controls-volume-control-hover-background&quot;&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;input type=&quot;range&quot;                                step=&quot;any&quot; max=&quot;1&quot; aria-valuemax=&quot;100&quot; aria-valuemin=&quot;0&quot; aria-label=&quot;volume&quot;                                pseudo=&quot;-webkit-media-controls-volume-slider&quot; aria-valuenow=&quot;100&quot; class=&quot;closed&quot;                                style=&quot;&quot;&amp;gt;&amp;lt;input type=&quot;button&quot; pseudo=&quot;-webkit-media-controls-mute-button&quot;                                aria-label=&quot;wyciszenie&quot; style=&quot;&quot;&amp;gt;                        &amp;lt;/div&amp;gt;&amp;lt;input type=&quot;button&quot; role=&quot;button&quot; aria-label=&quot;włącz tryb obrazu w&amp;amp;nbsp;obrazie&quot;                            pseudo=&quot;-internal-media-controls-picture-in-picture-button&quot; style=&quot;display: none;&quot;&amp;gt;&amp;lt;input                            type=&quot;button&quot; pseudo=&quot;-webkit-media-controls-fullscreen-button&quot;                            aria-label=&quot;przejdź do pełnego ekranu&quot; style=&quot;&quot;&amp;gt;&amp;lt;input type=&quot;button&quot;                            aria-label=&quot;pokaż więcej opcji sterowania multimediami&quot; title=&quot;więcej opcji&quot;                            pseudo=&quot;-internal-media-controls-overflow-button&quot; style=&quot;&quot;&amp;gt;                    &amp;lt;/div&amp;gt;&amp;lt;input type=&quot;range&quot; step=&quot;any&quot; pseudo=&quot;-webkit-media-controls-timeline&quot; max=&quot;12.612&quot;                        aria-label=&quot;pasek czasu odtwarzania filmu 0:00 / 0:12&quot; aria-valuetext=&quot;upłynęło: 0:00&quot;&amp;gt;                &amp;lt;/div&amp;gt;            &amp;lt;/div&amp;gt;            &amp;lt;div role=&quot;menu&quot; aria-label=&quot;Opcje&quot; pseudo=&quot;-internal-media-controls-text-track-list&quot; style=&quot;display: none;&quot;&amp;gt;            &amp;lt;/div&amp;gt;            &amp;lt;div pseudo=&quot;-internal-media-controls-overflow-menu-list&quot; role=&quot;menu&quot; class=&quot;closed&quot; style=&quot;display: none;&quot;&amp;gt;                &amp;lt;label pseudo=&quot;-internal-media-controls-overflow-menu-list-item&quot; role=&quot;menuitem&quot; tabindex=&quot;0&quot;                    aria-label=&quot; Odtwórz &quot; style=&quot;display: none;&quot;&amp;gt;&amp;lt;input type=&quot;button&quot;                        pseudo=&quot;-webkit-media-controls-play-button&quot; tabindex=&quot;-1&quot; aria-label=&quot;odtwórz&quot; class=&quot;pause&quot;                        style=&quot;display: none;&quot;&amp;gt;                    &amp;lt;div aria-hidden=&quot;true&quot;&amp;gt;&amp;lt;span&amp;gt;Odtwórz&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;                &amp;lt;/label&amp;gt;&amp;lt;label pseudo=&quot;-internal-media-controls-overflow-menu-list-item&quot; role=&quot;menuitem&quot; tabindex=&quot;0&quot;                    aria-label=&quot;przejdź do pełnego ekranu Pełny ekran &quot; style=&quot;display: none;&quot;&amp;gt;&amp;lt;input type=&quot;button&quot;                        pseudo=&quot;-webkit-media-controls-fullscreen-button&quot; aria-label=&quot;przejdź do pełnego ekranu&quot;                        tabindex=&quot;-1&quot; style=&quot;display: none;&quot;&amp;gt;                    &amp;lt;div aria-hidden=&quot;true&quot;&amp;gt;&amp;lt;span&amp;gt;Pełny ekran&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;                &amp;lt;/label&amp;gt;&amp;lt;label pseudo=&quot;-internal-media-controls-overflow-menu-list-item&quot; role=&quot;menuitem&quot; tabindex=&quot;0&quot;                    aria-label=&quot;pobierz multimedia Pobierz &quot; class=&quot;animated-1&quot; style=&quot;&quot;&amp;gt;&amp;lt;input type=&quot;button&quot;                        aria-label=&quot;pobierz multimedia&quot; pseudo=&quot;-internal-media-controls-download-button&quot; tabindex=&quot;-1&quot;                        style=&quot;&quot;&amp;gt;                    &amp;lt;div aria-hidden=&quot;true&quot;&amp;gt;&amp;lt;span&amp;gt;Pobierz&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;                &amp;lt;/label&amp;gt;&amp;lt;label pseudo=&quot;-internal-media-controls-overflow-menu-list-item&quot; role=&quot;menuitem&quot; tabindex=&quot;0&quot;                    aria-label=&quot; Wycisz &quot; class=&quot;animated-2&quot; style=&quot;display: none;&quot;&amp;gt;&amp;lt;input type=&quot;button&quot;                        pseudo=&quot;-webkit-media-controls-mute-button&quot; tabindex=&quot;-1&quot; aria-label=&quot;wyciszenie&quot;                        style=&quot;display: none;&quot;&amp;gt;                    &amp;lt;div aria-hidden=&quot;true&quot;&amp;gt;&amp;lt;span&amp;gt;Wycisz&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;                &amp;lt;/label&amp;gt;&amp;lt;label pseudo=&quot;-internal-media-controls-overflow-menu-list-item&quot; role=&quot;menuitem&quot; tabindex=&quot;0&quot;                    aria-label=&quot;odtwarzanie na urządzeniu zdalnym Przesyłaj &quot; class=&quot;animated-1&quot;                    style=&quot;display: none;&quot;&amp;gt;&amp;lt;input pseudo=&quot;-internal-media-controls-cast-button&quot; type=&quot;button&quot;                        aria-label=&quot;odtwarzanie na urządzeniu zdalnym&quot; tabindex=&quot;-1&quot; style=&quot;display: none;&quot;&amp;gt;                    &amp;lt;div aria-hidden=&quot;true&quot;&amp;gt;&amp;lt;span&amp;gt;Przesyłaj&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;                &amp;lt;/label&amp;gt;&amp;lt;label pseudo=&quot;-internal-media-controls-overflow-menu-list-item&quot; role=&quot;menuitem&quot; tabindex=&quot;0&quot;                    aria-label=&quot;wyświetlanie menu napisów Napisy &quot; class=&quot;animated-0&quot; style=&quot;display: none;&quot;&amp;gt;&amp;lt;input                        aria-label=&quot;wyświetlanie menu napisów&quot; type=&quot;button&quot;                        pseudo=&quot;-webkit-media-controls-toggle-closed-captions-button&quot; tabindex=&quot;-1&quot; style=&quot;display: none;&quot;&amp;gt;                    &amp;lt;div aria-hidden=&quot;true&quot;&amp;gt;&amp;lt;span&amp;gt;Napisy&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;                &amp;lt;/label&amp;gt;&amp;lt;label pseudo=&quot;-internal-media-controls-overflow-menu-list-item&quot; role=&quot;menuitem&quot; tabindex=&quot;0&quot;                    aria-label=&quot;włącz tryb obrazu w&amp;amp;nbsp;obrazie Obraz w&amp;amp;nbsp;obrazie &quot; class=&quot;animated-0&quot; style=&quot;&quot;&amp;gt;&amp;lt;input                        type=&quot;button&quot; role=&quot;button&quot; aria-label=&quot;włącz tryb obrazu w&amp;amp;nbsp;obrazie&quot;                        pseudo=&quot;-internal-media-controls-picture-in-picture-button&quot; tabindex=&quot;-1&quot; style=&quot;&quot;&amp;gt;                    &amp;lt;div aria-hidden=&quot;true&quot;&amp;gt;&amp;lt;span&amp;gt;Obraz w&amp;amp;nbsp;obrazie&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;                &amp;lt;/label&amp;gt;&amp;lt;/div&amp;gt;        &amp;lt;/div&amp;gt;    &amp;lt;source src=&quot;movie.mp4&quot; type=&quot;video/mp4&quot;&amp;gt;    &amp;lt;source src=&quot;movie.ogg&quot; type=&quot;video/ogg&quot;&amp;gt;&amp;lt;/video&amp;gt;Shadow DOM ukrywa całą implementację pod prostym tagiem.Dzięki temu style zaaplikowane do naszego elementu nie wpływają na inne elementy DOM-u.Wsparcie Shadow DOM przez główne przeglądarkiźródło: www.webcomponents.org - dostęp: 2019-08-20View Encapsulation w AngularzeDomyślnie Angular korzysta z własnego kapsułkowania stylów (ViewEncapsulation.Emulated), ale udostępnia jeszcze 3 inne tryby kapsułkowania (w tym jeden deprecated).Aby zmienić domyślny tryb kapsułkowania, wystarczy dodać odpowiednią opcję w dekoratorze @Component, np.:encapsulation: ViewEncapsulation.ShadowDomOmówimy je na przykładzie kodu z projektu demo. Link do repozytoriumProjekt demo składa się z 4 komponentów:  app-root - główny komponent zawierający w sobie pozostałe komponenty  app-red  app-green  app-blueKażdy z komponentów app-red, app-green oraz app-blue składa się z jednego paragrafu z odpowiednim kolorem tekstu dla tego elementu. Oprócz tego istnieją 3 branche, po jednym dla każdego z omawianych trybów, co pozwoli na zobrazowanie nakładania się oraz kapsułkowania stylów.ViewEncapsulation.NoneBrak kapsułkowania, czyli style utworzone w komponencie są globalne (w sekcji &amp;lt;head&amp;gt;).W tym trybie elementy HTML i odpowiadające im selektory CSS wyglądają tak samo jak te, które napisaliśmy w kodzie.Może to spowodować niechciane nadpisywanie stylów lub dodawanie ich do elementów, które nie posiadają żadnego stylu.W przykładzie usunęliśmy styl paragrafu w komponencie app-green.Link do repozytoriumimport {Component, ViewEncapsulation} from &#39;@angular/core&#39;;@Component({    selector: &#39;app-red&#39;,    template: `        &amp;lt;p&amp;gt;Red paragraph!&amp;lt;/p&amp;gt;    `,    styles: [`        p {            color: red;        }    `],    encapsulation: ViewEncapsulation.None})export class RedComponent {}import {Component, ViewEncapsulation} from &#39;@angular/core&#39;;@Component({    selector: &#39;app-green&#39;,    template: `        &amp;lt;p&amp;gt;Green paragraph!&amp;lt;/p&amp;gt;    `,    encapsulation: ViewEncapsulation.None})export class GreenComponent {}import {Component, ViewEncapsulation} from &#39;@angular/core&#39;;@Component({    selector: &#39;app-blue&#39;,    template: `        &amp;lt;p&amp;gt;Blue paragraph!&amp;lt;/p&amp;gt;    `,    styles: [`        p {            color: blue;        }    `],    encapsulation: ViewEncapsulation.None})export class BlueComponent {}Wynikowy kod HTML:&amp;lt;head&amp;gt;    &amp;lt;style&amp;gt;        p {            color: red;        }    &amp;lt;/style&amp;gt;    &amp;lt;style&amp;gt;        p {            color: blue;        }    &amp;lt;/style&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;    &amp;lt;app-root ng-version=&quot;8.2.2&quot;&amp;gt;        &amp;lt;app-red&amp;gt;            &amp;lt;p&amp;gt;Red paragraph!&amp;lt;/p&amp;gt;        &amp;lt;/app-red&amp;gt;        &amp;lt;app-green&amp;gt;            &amp;lt;p&amp;gt;Green paragraph!&amp;lt;/p&amp;gt;        &amp;lt;/app-green&amp;gt;        &amp;lt;app-blue&amp;gt;            &amp;lt;p&amp;gt;Blue paragraph!&amp;lt;/p&amp;gt;        &amp;lt;/app-blue&amp;gt;    &amp;lt;/app-root&amp;gt;&amp;lt;/body&amp;gt;Wynik widoczny w przeglądarceJak widzimy, style zostały dodane w sekcji &amp;lt;head&amp;gt;, co spowodowało nadpisanie pierwszego stylu paragrafu drugim - color: blue. W efekcie wszystkie paragrafy mają ten sam kolor, również paragraf z komponentu app-green, który nie posiada żadnego stylu i powinien mieć kolor domyślny.ViewEncapsulation.Emulated (default)Domyślny tryb kapsułkowania w Angularze, w którym style są domknięte w komponencie.W tym trybie style również znajdują się w sekcji &amp;lt;head&amp;gt;, ale posiadają dodatkowe atrybuty które wiążą je z elementami HTML pochodzącymi z tego samego komponentu.Dzięki temu na stronie może istnieć kilka komponentów zawierających element tego samego typu, ale z różnymi stylami.Uwaga! W tym trybie style nie mają wpływu na inne elementy na stronie (jednak mogą mieć wpływ na elementy komponentu dziecka - jeśli komponent dziecka posiada tryb kapsułkowania inny niż Shadow DOM), ponieważ są domknięte unikalnymi atrybutami. Globalne style strony (oraz style innych komponentów, które mają wyłączony tryb kapsułkowania) mogą jednak mieć wpływ na ten komponent.W przykładzie przenieśliśmy komponent app-green z komponentu app-root do komponentu app-blue i usunęliśmy jego style.Link do repozytoriumimport {Component} from &#39;@angular/core&#39;;@Component({    selector: &#39;app-red&#39;,    template: `        &amp;lt;p&amp;gt;Red paragraph!&amp;lt;/p&amp;gt;    `,    styles: [`        p {            color: red;        }    `]})export class RedComponent {}import {Component} from &#39;@angular/core&#39;;@Component({    selector: &#39;app-green&#39;,    template: `        &amp;lt;p&amp;gt;Green paragraph!&amp;lt;/p&amp;gt;    `})export class GreenComponent {}import {Component} from &#39;@angular/core&#39;;@Component({    selector: &#39;app-blue&#39;,    template: `        &amp;lt;app-green&amp;gt;&amp;lt;/app-green&amp;gt;        &amp;lt;p&amp;gt;Blue paragraph!&amp;lt;/p&amp;gt;    `,    styles: [`        p {            color: blue;        }    `]})export class BlueComponent {}Wynikowy kod HTML:&amp;lt;head&amp;gt;    &amp;lt;style&amp;gt;        p[_ngcontent-pes-c0] {            color: red;        }    &amp;lt;/style&amp;gt;    &amp;lt;style&amp;gt;        p[_ngcontent-pes-c1] {            color: blue;        }    &amp;lt;/style&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;    &amp;lt;app-root ng-version=&quot;8.2.2&quot;&amp;gt;        &amp;lt;app-red _nghost-pes-c0&amp;gt;            &amp;lt;p _ngcontent-pes-c0&amp;gt;Red paragraph!&amp;lt;/p&amp;gt;        &amp;lt;/app-red&amp;gt;        &amp;lt;app-blue _nghost-pes-c1&amp;gt;            &amp;lt;app-green _ngcontent-pes-c1&amp;gt;                &amp;lt;p&amp;gt;Green paragraph!&amp;lt;/p&amp;gt;            &amp;lt;/app-green&amp;gt;            &amp;lt;p _ngcontent-pes-c1&amp;gt;Blue paragraph!&amp;lt;/p&amp;gt;        &amp;lt;/app-blue&amp;gt;    &amp;lt;/app-root&amp;gt;&amp;lt;/body&amp;gt;Wynik widoczny w przeglądarceDomyślny tryb pozwolił nam odseparować style między poszczególnymi komponentami. W kodzie wynikowym widzimy, że style z komponentu rodzica app-blue nie zostały zaaplikowane do komponentu dziecka app-green, w efekcie czego paragraf ma kolor domyślny. Stało się tak, ponieważ Angular dodał atrybut do stylu. Gdybyśmy dodali styl w runtime, to zostałby zaaplikowany również do komponentu dziecka.Na przykładzie komponentu app-red - Angular dodał atrybut _ngcontent-pes-c0 do selektora CSS oraz elementu HTML. W ten sposób style dodane w sekcji &amp;lt;head&amp;gt; aplikują się tylko do odpowiednich elementów z tego samego komponentu. Oprócz tego, na komponencie dodany został atrybut _nghost-pes-c0. Z czego składają się te atrybuty?  _ngcontent - określa typ elementu, w tym przypadku zawartość komponentu  _nghost - określa element root komponentu  -pes - oznacza ID aplikacji (APP_ID), jeśli nie został ustawiony to zostanie przyjęty wygenerowany ciąg znaków - dzięki temu nie nakładają się style między różnymi aplikacjami wyświetlanymi w jednym oknie  -c0 - numeruje kolejno elementy w komponencieViewEncapsulation.ShadowDomKapsułkowanie oparte na Shadow DOM (wymaga wsparcia przeglądarki dla Shadow DOM).W tym trybie style nie są dodawane w sekcji &amp;lt;head&amp;gt;, a istnieją w Shadow Root.Uwaga! W tym trybie style nie mają wpływu na inne elementy na stronie (jednak mogą mieć wpływ na elementy komponentu dziecka - jeśli komponent dziecka posiada tryb kapsułkowania inny niż Shadow DOM). Globalne style strony (oraz style innych komponentów) również nie mają wpływu na ten komponent.W przykładzie przenieśliśmy komponent app-green z komponentu app-root do komponentu app-blue, usunęliśmy jego style i ustawiliśmy domyślny tryb kapsułkowania.Link do repozytoriumimport {Component, ViewEncapsulation} from &#39;@angular/core&#39;;@Component({    selector: &#39;app-red&#39;,    template: `        &amp;lt;p&amp;gt;Red paragraph!&amp;lt;/p&amp;gt;    `,    styles: [`        p {            color: red;        }    `],    encapsulation: ViewEncapsulation.ShadowDom})export class RedComponent {}import {Component} from &#39;@angular/core&#39;;@Component({    selector: &#39;app-green&#39;,    template: `        &amp;lt;p&amp;gt;Green paragraph!&amp;lt;/p&amp;gt;    `})export class GreenComponent {}import {Component, ViewEncapsulation} from &#39;@angular/core&#39;;@Component({    selector: &#39;app-blue&#39;,    template: `        &amp;lt;app-green&amp;gt;&amp;lt;/app-green&amp;gt;        &amp;lt;p&amp;gt;Blue paragraph!&amp;lt;/p&amp;gt;    `,    styles: [`        p {            color: blue;        }    `],    encapsulation: ViewEncapsulation.ShadowDom})export class BlueComponent {}Wynikowy kod HTML:&amp;lt;head&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;    &amp;lt;app-root ng-version=&quot;8.2.2&quot;&amp;gt;        &amp;lt;app-red&amp;gt;            #shadow-root            &amp;lt;style&amp;gt;                p {                    color: red;                }            &amp;lt;/style&amp;gt;            &amp;lt;p&amp;gt;Red paragraph!&amp;lt;/p&amp;gt;        &amp;lt;/app-red&amp;gt;        &amp;lt;app-blue&amp;gt;            #shadow-root            &amp;lt;style&amp;gt;                p {                    color: blue;                }            &amp;lt;/style&amp;gt;            &amp;lt;app-green&amp;gt;                &amp;lt;p&amp;gt;Green paragraph!&amp;lt;/p&amp;gt;            &amp;lt;/app-green&amp;gt;            &amp;lt;p&amp;gt;Blue paragraph!&amp;lt;/p&amp;gt;        &amp;lt;/app-blue&amp;gt;    &amp;lt;/app-root&amp;gt;&amp;lt;/body&amp;gt;Wynik widoczny w przeglądarceTryb Shadow DOM również pozwolił nam odseparować style między poszczególnymi komponentami. W sekcji &amp;lt;head&amp;gt; nie ma już żadnych stylów, natomiast są ukryte w Shadow Root elementów DOM-u. Na przykładzie widzimy, że style z komponentu rodzica app-blue zostały zaaplikowane do komponentu dziecka app-green, w efekcie czego paragraf ma kolor niebieski. Gdyby komponent app-green również posiadał tryb ViewEncapsulation.ShadowDom, to style rodzica nie zostałyby zaaplikowane, ponieważ korzystałby ze stylów z własnego Shadow Root. Tryb Shadow DOM zabezpiecza nasz komponent również przed stylami z komponentu rodzica, dodanymi w runtime.ViewEncapsulation.NativeDo niedawna zamiast ViewEncapsulation.ShadowDom dostępny był tryb ViewEncapsulation.Native.Działał on w podobny sposób, ale został wycofany z powodu wykorzystywania przestarzałego standardu Shadow DOM.PodsumowanieOgólnie rzecz biorąc, powinniśmy unikać braku kapsułkowania stylów, ponieważ powoduje to często niechciane efekty.Tryb Shadow DOM zapewnia całkowite domknięcie stylów w komponencie, dzięki czemu style globalne oraz inne komponenty nie mają na niego wpływu, tak samo jak komponent w tym trybie nie ma wpływu na inne komponenty na stronie (za wyjątkiem komponentów dzieci które mają włączony tryb kapsułkowania inny niż Shadow DOM).Niestety nie wszystkie przeglądarki mogą wspierać ten tryb, dlatego Angular domyślnie udostępnił własny, emulowany tryb kapsułkowania. W trybie domyślnym na nasz komponent mają jednak wpływ style globalne, a także mogą mieć wpływ inne komponenty, ponieważ komponent w tym trybie nadal wykorzystuje style z sekcji &amp;lt;head&amp;gt;. W większości przypadków tryb domyślny jest wystarczający, więc jeśli zależy nam na jak najlepszym wsparciu przeglądarek i nie mamy problemów z nadpisywaniem stylów przez inne komponenty lub aplikacje, to możemy z powodzeniem z niego korzystać.Musimy jednak pamiętać, że mieszanie różnych trybów kapsułkowania między komponentami również może spowodować niezamierzone efekty.",
"url": "/2019/08/22/view-encapsulation-w-angularze.html",
"author": "Michał Hoja",
"authorUrl": "/authors/mhoja.html",
"image": "mhoja.jpg",
"highlight": "/assets/img/posts/2019-08-22-view-encapsulation-w-angularze/View-Encapsulation.jpg",
"date": "22-08-2019",
"path": "pl/2019-08-22-view-encapsulation-w-angularze"
}
,


"2019-08-06-angular-onpush-strategy-html": {
"title": "Angular - detekcja zmian strategią onPush",
"lang": "pl",
"tags": "angular frontend",
"content": "Każda aplikacja rozwijana odpowiednio długi czas może rozrosnąć się do ogromnych rozmiarów, a konkretniej do sporej liczby komponentów, jeżeli mówimy o aplikacji frontendowej pisanej z wykorzystaniem Angulara. Z czasem przyrost kolejnych funkcjonalności może spowodować, że nasz produkt przestanie spełniać oczekiwania odnośnie wydajności. W takim momencie powinniśmy pomyśleć nad możliwościami naprawy tego problemu. W tym artykule pokażemy jedno z możliwych rozwiązań tego problemu - zmiana strategii detekcji zmian.Detekcja zmianZałóżmy, że mamy przed sobą kod aplikacji odpowiedzialnej za zarządzanie hodowlą zwierząt. Przykładowym komponentem odpowiedzialnym za wyświetlanie informacji o krówkach byłby cow-run.component, czyli wybieg krówek, który przekazuje obiekt pojedynczej krówki do cow.component. Z drugiej strony mamy pig-run.component, który spełnia te same założenia co komponent krówek. Przykładowe drzewo komponentów mogłoby wyglądać tak:Angular dla każdego komponentu tworzy odpowiadający jemu (komponentowi) ChangeDetector. Przejdźmy dalej, czyli jak to działa?Jak to działa?Domyślnie ChangeDetector nasłuchuje na każdą zmianę stanu aplikacji - zmianę inputów, zmianę modelu prezentowanego na templatce, wywołania asynchroniczne, zdarzenia DOM, interwały. Każda taka zmiana powoduje porównanie obecnie prezentowanych w drzewie DOM wartości do tych, które przechowuje komponent - w momencie wykrycia różnic komponent oznaczany jest jako “brudny” - proces ten nazywa się “dirty checking”. Następnie dokonywana jest projekcja modelu na drzewo DOM, czyli faktyczne zaktualizowanie widoku.Detekcja zmian w każdym świeżo utworzonym komponencie ustawiona jest na wartość ChangeDetectionStrategy.Default, co przekłada się na detekcję zmian strategią CheckAlways. Strategia ta sprawia, że podczas każdej zmiany stanu aplikacji - asynchronicznego zapytania wysyłanego do serwera, zdarzenia DOM, interakcji użytkownika z naszą aplikacją sprawdzane jest całe drzewo komponentów. Wyobraźmy sobie sytuację, kiedy zdarzenie DOM zostało wyemitowane przez CowComponent. Angular zanim sprawdziłby komponent, który faktycznie wyemitował zdarzenie, musiałby sprawdzić wszystkie komponenty, zgodnie z utworzonym przez siebie drzewem. Spójrzmy na obrazek:Strzałki obrazują kierunek przechodzenia przez drzewo mechanizmu detekcji. Jest to prosty przykład, gdyż drzewo jest bardzo małe. Wyobraźmy sobie jednak drzewo zbudowane z setek komponentów. Z każdą zmianą Angular musiałby na nowo przeszukać całe drzewo komponentów celem znalezienia tego komponentu, który wyemitował zmianę. Dość sporo obliczeń, czyż nie?Strategia onPushNa szczęście Angular pozwala nam na zmianę domyślnej strategii detekcji zmian. Jeżeli nie chcemy korzystać z domyślnego mechanizmu, to na ratunek przychodzi nam strategia onPush! Strategia ta mówi nam, że komponent zależny jest tylko i wyłącznie od swoich inputów. Taki komponent nazywamy “czystym”. Zmiana propagowana jest w momencie zmiany referencji inputów komponentu jak i w przypadku wyemitowania zdarzenia DOM w szablonie komponentu (np. kliknięcie w przycisk - event onclick). Co więcej, komponent emitujący zmianę z wykorzystaniem strategii onPush powiadamia mechanizm detekcji Angulara, że to właśnie on wyemitował zmianę! To drastycznie zmniejsza koszt przeszukania drzewa komponentów, gdyż Angular wie, którego komponentu szukać, albo który komponent pominąć. Zdjęcie poniżej pozwoli zobrazować tę sytuację.Jak używać?Używanie takiej strategii wymusza na nas zmiany podejścia odnośnie projektowania naszych komponentów. Inputy czystego komponentu powinny być niezmienialne, co oznacza, że wartości naszych inputów powinny być aktualizowane przez zmianę referencji, a nie wartości. Prosty przykład:@Input() cowDonation: { donation: number };onClickUpdateDonation() {    this.cowDonation.donation = 500;}Powyższa zmiana nie zadziała, ponieważ zmieniamy wartość, a nie referencję. Aby strategia onPush zadziałała, wartość dotacji musimy zmienić poprzez zmianę referencji, czyli przykładowo:@Input() cowDonation: { donation: number };updateDonation() {    this.cowDonation = {        donation: 500    };}Zdarzenia DOM są zdarzeniami asynchronicznymi, więc moglibyśmy wyciągnąć wniosek: detekcja zmian zadziała, kiedy użyjemy takich funkcji asynchronicznych jak setTimeout, setInterval albo subskrypcja do Observable’a zwracanego przez serwis HTTP, prawda? Otóż nie. Na szczęście w przypadku bytów typu Observable Angular przychodzi nam z pomocą i udostępnia AsyncPipe. Dlaczego to działa z użyciem AsyncPipe a nie z manualną subskrypcją? Zajrzyjmy więc w kod:_updateLatestValue(async, value) {    if (async === this._obj) {        this._latestValue = value;        this._ref.markForCheck();    }}Jak widać powyżej, w momencie aktualizacji wartości wywoływana jest funkcja markForCheck(), która powiadamia mechanizm detekcji zmian o konieczności sprawdzenia danego komponentu.Przejęcie kontroli nad mechanizmem detekcji zmianCo w przypadku, gdy bardzo potrzebujemy użyć funkcji setInterval lub setTimeout, ale jednocześnie chcielibyśmy również używać strategii onPush? Angular daje nam możliwość wstrzyknięcia dedykowanego ChangeDetectora danemu komponentowi, a potem wywołanie na nim funkcji markForCheck() - analogicznie jak w opisywanym przykładzie z AsyncPipe!Przykładowy kod wyglądałby tak:constructor(private cowService: CowService, private changeDetectorRef: ChangeDetectorRef) {    this.updateCowDonationWithTimeout();}private updateCowDonationWithTimeout() {    setTimeout(() =&amp;gt; {        this.cowDonation = {            donation: 500        };        this.changeDetectorRef.markForCheck();    }, 500);}Przy stworzeniu komponentu zostanie wywołana funkcja zmieniająca wartość dotacji dla krowy na 500 po upływie około 500ms, a wszystko dzięki wywołaniu markForCheck() na referencji do detektora zmian komponentu.Na co należy uważać?Przypomnijmy, że przy korzystaniu ze strategii onPush musimy pamiętać o tym, że:  zmiany inputów komponentu muszą zachodzić poprzez zmianę referencji, a nie wartości!  funkcje asynchroniczne (setTimeout, setInterval, manualna subskrypcja do Observable’a) nie wywołują mechanizmu detekcji zmian.Kilka słów na zakończenieOnPush wymusza na nas projektowanie komponentów w określony sposób - tak, żeby komponent odpowiedzialny był jedynie za prezentację danych na podstawie otrzymanych inputów. Cała skomplikowana logika mogłaby wtedy być przeniesiona do serwisów. Przeniesienie logiki do serwisu umożliwiłoby też łatwiejsze otestowanie kodu - fajnie jest mieć jakieś potwierdzenie, że nasz kod robi to, co powinien :). Pisanie komponentów niezmienialnych (ang. immutable) i ogółem kodu opartego na niezmienialności to tworzenie dobrych przyzwyczajeń, które mogą być wykorzystane przy adaptacji nowych rozwiązań w projekcie - przykładowo kontrolowanie stanu z wykorzystaniem biblioteki ngRx, która również wymusza na programistach pisanie kodu opartego na niezmienialności. Stosowanie strategii onPush z pewnością może zwiększyć wydajność aplikacji, choć zalecałbym korzystanie z tej strategii w nowo tworzonych komponentach, pisanych od początku z myślą o niezmienialności. Wprowadzanie onPush’a na siłę do już istniejących, czasami mocno rozbudowanych komponentów może doprowadzić do niepożądanych zachowań (a w tym przypadku braku reakcji na zmiany :P), więc trzeba wziąć to pod uwagę adaptując tę strategię do już istniejącego kodu.",
"url": "/2019/08/06/angular-onpush-strategy.html",
"author": "Konrad Gabara",
"authorUrl": "/authors/kgabara.html",
"image": "kgabara.jpg",
"highlight": "/assets/img/posts/2019-05-29-angular-onpush-strategy/angular-onpush.jpeg",
"date": "06-08-2019",
"path": "pl/2019-05-29-angular-onpush-strategy"
}
,


"2019-07-26-testy-frontendu-okiem-full-stacka-html": {
"title": "Testy jednostkowe frontendu okiem programisty full stack",
"lang": "pl",
"tags": "unit test jasmine angular",
"content": "Niełatwo znaleźć wymówkę, żeby nie pisać testów jednostkowych. Obecność frameworków ułatwiających tę czynność w projektach, z którymi stykamy się na co dzień, nie powinna na żadnym chociaż trochę doświadczonym programiście robić wrażenia i nie trzeba go przekonywać, że jedne z wielu zalet pisania testów jednostkowych, to:  zmuszenie twórcy do zastanowienia się nad zadaniem sprawdzanego kodu (co może potencjalnie poprawić design aplikacji),  ułatwienie wczesnego wyłapywania błędów,  ekspozycja przypadków brzegowych,  ułatwienie zrozumienia działania kodu osobom, które go nie tworzyły.W wielu nowoczesnych aplikacjach internetowych, w tym np. we wnioskach Eximee, duża część logiki znajduje się po stronie klienckiej, hipokryzją byłoby pominięcie testów w tak istotnym elemencie aplikacji, ponieważ najbardziej rzucające się w oczy błędy są właśnie tam. Niemniej nawet przy ogromnej liczbie narzędzi wspomagających proces pisania testów jednostkowych, programiści mogą mieć problem z wyznaczeniem właśnie tych jednostek.Co testowaćNiezależnie od tego jaki framework został użyty w danym projekcie, zawsze możemy z niego wydzielić komponenty. Przeważnie jest to JavaScriptowa klasa z jakimś odniesieniem do szablonu HTML. Akurat w tym artykule jako przykład użyty został Angular. W praktyce możemy podzielić te komponenty na dwa typy:  komponent prezentacyjny, który nie posiada logiki biznesowej i jego jedynymi zadaniami są wyświetlenie szablonu na podstawie wejścia i ew. przekazanie jakiegoś zdarzenia (np. kliknięcia, wciśnięcia klawisza itp.) do komponentu nadrzędnego,  komponent, który używa i zarządza innymi komponentami nie zajmując się jednocześnie prezentacją.Brak tego podziału może znacząco utrudnić pisanie testów jednostkowych, co zresztą okaże się bardzo szybko przy próbie napisania ich do słabo zaprojektowanego komponentu.Uważam, że testy jednostkowe komponentów prezentacyjnych są zasadne tylko w przypadku, gdy wejście w jakiś sposób zmienia jego zachowanie lub obsługa uaktualnienia widoku jest skomplikowana (np. animacja przeliczając atrybuty elementu w locie). Najważniejsze jest zwiększenie pokrycia logiki biznesowej.Testowanie logiki biznesowejW pierwszej kolejności powinniśmy się zastanowić nad tym, czy z komponentu możemy wydzielić logikę, np. do osobnego serwisu, czyli w praktyce klasy odpowiedzialnej za jakąś funkcjonalność z możliwością używania jej w wielu miejsciach (np. serwis zarządzający widocznością popupów w aplikacji). Serwisy testuje się o wiele prościej niż komponenty, ze względu na brak szablonu i związanego z frameworkiem narzutu (serwis może być zwykłą JavaScriptową klasą).Praktyka na przykładzie Jasmine i AngularaNiech przykładem będzie komponent wyboru daty z formatterem - zakładając, że cała logika znajduje się w komponencie, trzeba będzie zadbać o stworzenie jego instancji ze wszystkimi zależnościami pisząc testy dla formattera, następnie zasymulować zdarzenie wpisania danych w pole tekstowe. Gdyby wydzielono wcześniej osobny serwis do formatowania, to wystarczyłoby przetestować tylko jego logikę. Testy całego komponentu możemy przeprowadzić zaślepiając odpowiednie zależności, co znacznie ułatwi pracę.Tak wyglądałby komponent, jeśli zaniedbalibyśmy wyżej zaproponowany podział:Komponent@Component({    selector: &#39;date-picker&#39;,    template: `        &amp;lt;input [value]=&quot;formattedValue&quot; (change)=&quot;onValueChange($event.target.value)&quot;&amp;gt;    `})export class DatePicker implements OnChanges  {    @Input() value: string;    formattedValue: string;    constructor(private datePickerRestService: DatePickerRestService) {    }        ngOnChanges(changes: SimpleChanges): void {        this.formattedValue = this.format(changes.value.currentValue);    }     onValueChange(event: string): void {        ...    }    private format(string: value): string {        ...    }        private sendValue(value: string): void {       this.datePickerRestService.send(value);    }}Testlet fixture: ComponentFixture&amp;lt;DatePicker&amp;gt;;describe(&#39;DatePicker&#39;, () =&amp;gt; {    class DatePickerRestServiceMock implements Partial&amp;lt;DatePickerRestService&amp;gt; {        send(): void {        }    }    beforeEach(() =&amp;gt; {        TestBed.configureTestingModule({            declarations: [DatePicker],            providers: [              {                  provide: DatePickerRestService,                  useClass: DatePickerRestServiceMock              }            ]        });        fixture = TestBed.createComponent(DatePicker);    });    it(&#39;should return formatted date from timestamp&#39;, () =&amp;gt; {        // given        const hostElement = fixture.nativeElement;        const input: HTMLInputElement = hostElement.querySelector(&#39;input&#39;);        // when        fixture.componentInstance.value = &#39;2018-07-01&#39;;        fixture.detectChanges();        // then        expect(input.value).toBe(&#39;1 lipca 2018&#39;);    });});Jak widać testy są słabo czytelne, ponieważ widoczne są detale implementacyjne związane z działaniem frameworku (TestBed, ComponentFixture). Wraz z dodawaniem funkcjonalności i zależności coraz trudniej będzie utrzymać klarowność.Zaprojektowany w ten sposób komponent pozwoli na przetestowanie głównej funkcjonalności nie przejmując się zależnościami komponentu i jego szablonem.Komponent@Component({    selector: &#39;date-picker&#39;,    template: `        &amp;lt;date-picker-input [value]=&quot;formattedValue&quot; (change)=&quot;onValueChange($event)&quot;&amp;gt;&amp;lt;/date-picker-input&amp;gt;    `})export class DatePicker implements OnChanges  {    @Input() value: string;    formattedValue: string;    constructor(private service: FormatterService ) {    }        ngOnChanges(changes: SimpleChanges): void {        this.formattedValue = this.service.format(changes.value.currentValue);    }    onValueChange(event: string): void {        // ...    }}Test serwisu formattera mógłby wyglądać następująco:Testdescribe(&#39;FormatterService&#39;, () =&amp;gt; {   beforeEach(() =&amp;gt; {      service = new FormatterService();   });   it(&#39;should return formatted date from timestamp&#39;, () =&amp;gt; {      // given      const timestamp: string = Date.now(&#39;2018-07-01&#39;).toString();       // when      const result = service.format(timestamp);      // then      expect(result).toBe(&#39;1 lipca 2018&#39;);   });});Nie trzeba się martwić o dodatkowe zależności, można się skupić tylko na testowaniu funkcjonalności.Jak widać przy odpowiednim podejściu pisanie testów jednostkowych funkcjonalności na frontendzie nie musi się tak bardzo różnić od tworzenia ich dla części backendowej gdy wie się co i w jaki sposób testować, wtedy samo pisanie testów staje się o wiele prostsze.",
"url": "/2019/07/26/testy-frontendu-okiem-full-stacka.html",
"author": "Marcin Mendlik",
"authorUrl": "/authors/mmendlik.html",
"image": "mmendlik.jpg",
"highlight": "/assets/img/posts/2019-07-25-testy-frontendu-okiem-full-stacka/testy-jednostkowe.jpg",
"date": "26-07-2019",
"path": "pl/2019-07-25-testy-frontendu-okiem-full-stacka"
}
,


"2019-07-12-10-praw-uzytecznego-designu-html": {
"title": "10 praw użytecznego designu",
"lang": "pl",
"tags": "ux ui design",
"content": "W latach 80. Dieter Rams zagubiony w chaosie form i kolorów postanowił coś z tym zrobić. Doszedł do wniosku, że dobry design nie może trzymać się sztywnych ram, ale opierając się na pewnych regułach, można stworzyć dobry produkt. Idąc tym tokiem rozumowania opracował 10 reguł dobrego designu, które w branży zdążyły urosnąć do rangi dziesięciu przykazań, a on sam uchodzi dziś za jednego z najlepszych i najbardziej wpływowych projektantów XX wieku.Choć świat UX designu nie doczekał się swojego Ramsa nie oznacza to, że projektanci interfejsów i doświadczeń użytkowników zostali pozostawieni na pastwę losu. Jak się sami za chwilę przekonacie, niezliczone ilości badań na przestrzeni ostatnich kilkudziesięciu lat zaowocowały całkiem pokaźną listą maksym i reguł.1. Efekt „estetycznej funkcjonalności”W latach 90 zeszłego stulecia Masaaki Kurosu i Kaori Kashimura postanowili zbadać wpływ atrakcyjności estetycznej na sposób, w jaki postrzega się funkcjonalność. W tym celu poprosili 252 uczestników badania o to, aby ocenili 26 różnych bankomatów, pod kątem tego, który z nich jest najłatwiejszy w obsłudze, a który najładniejszy. Dzięki temu Masaki i Kaori zaobserwowali, że istnieje pewna korelacja - im coś jest piękniejsze, tym jest bardziej użyteczne. Dodatkowo piękny wygląd sprawia, że jesteśmy skłonni wybaczyć drobne błędy użyteczności, czar jednak pryska jak bańka mydlana, gdy natrafimy na dużą wadę jak np. brak możliwości złożenia zamówienia.Spróbujcie przypomnieć sobie sytuację, kiedy musieliście podjąć decyzję i wybrać produkt spośród kilku dostępnych? Jeśli wybraliście coś tylko dlatego, że miało ładną etykietę, to właśnie ulegliście efektowi estetycznej funkcjonalności :)Rys. 1. Strona główna firmy Appleźródło: apple.com - dostęp: 2019-07-072. Prawo JakobaJakob Nielsen, jeden z czołowych specjalistów w dziedzinie użyteczności, a także założyciel Nielsen Norman Group zauważył, że użytkownicy spędzają większość czasu na innych stronach. Oznacza to, że użytkownicy wolą, aby Twoja strona działała tak samo, jak wszystkie inne witryny, które już znają. Dobrym przykładem wykorzystania tej zasady są sklepy sprzedające odzież i obuwie on-line.Jeśli strona wygląda podobnie do innych, to wtedy użytkownicy będą z łatwością umieli się po niej poruszać, skupiając się na oferowanych produktach.Rys. 2. Wybrany produkt w sklepie on-line marki Kazarźródło: kazar.com - dostęp: 2019-07-07Rys. 3. Wybrany produkt w sklepie on-line marki Gino Rossiźródło: gino-rossi.com - dostęp: 2019-07-073. Efekt izolacjiW 1933 roku Hedwig von Restorff odkryła, że przy uczeniu się listy bezsensownych słów łatwiej zapamiętuje się te, które w jakiś sposób wyróżniają się spośród innych, na przykład są napisane większymi literami lub mają inny kolor. Jak się już pewnie domyślacie, ta zasada oprócz tego, że świetnie nadaję się jako pomoc w nauce na jutrzejszą kartkówkę z biologii, to również znajduje zastosowanie w dziedzinie użyteczności. Przyjrzyjmy się chociażby Material Design, czyli jednemu z najpopularniejszych obecnie trendów w tworzeniu interfejsów. Jednym z charakterystycznych elementów tego stylu jest okrągły przycisk, który pływa przypięty w jednym miejscu (zazwyczaj jest to prawy dolny róg ekranu) i pojawia się ponad innymi elementami. Taki przycisk odpowiada za najważniejszą akcję na ekranie. Jeśli chcemy zwrócić uwagę użytkownika na dany element strony wyróżnijmy go na tle innych obiektów.Rys. 4. Floating Action Button wykonuje główną akcję w aplikacji z systemem Androidźródło: material.io - dostęp: 2019-07-074. Efekt ZeigarnikMówi o tym, że ludzie pamiętają nieukończone lub przerwane zadania lepiej niż zadania zakończone. Bluma Zeigarnik przeprowadziła serię eksperymentów sprawdzających zdolność do zapamiętywania różnych zadań. Osoby biorące udział w badaniach proszono o rozwiązywanie łamigłówek. Części z nich przeszkadzano w trakcie wykonywania poleceń, a pozostałym pozwalano w spokoju skończyć. Zeigarnik zauważyła, że czynności, których dokończenie przerwano, są o 90% lepiej zapamiętywane, niż te, które można było dokończyć. Ten efekt określany jest jej nazwiskiem i zyskał szerokie zastosowanie, także w dziedzinie użyteczności. Efekt ten wykorzystuje na przykład Udemy, czyli platforma edukacyjna, na której mamy dostęp do kursów internetowych przygotowanych przez instruktorów z całego świata. Jesteśmy na stronie zasypywani z każdej strony informacjami o tym, że kurs gotowania, który wykupiliśmy kilka dni wcześniej jest ukończony w 42%. Taki zabieg sprawia, że kurs nie daje nam spokoju i chcemy go doprowadzić do końca.Rys. 5. Strona główna serwisu Udemyźródło: udemy.com - dostęp: 2019-07-075. Efekt pierwszeństwa i świeżościTo tak naprawdę dwie zasady, mówiące o tym, że najlepiej zapamiętujemy to, co znajduje się na początku (efekt pierwszeństwa) oraz na końcu (efekt świeżości). Taki zabieg stosuje Allegro, które przedstawia oferty sponsorowane na szczycie i u dołu listy.Rys. 6. Portal Allegro - Oferty sponsorowane na szczycie listy produktówźródło: Fotele wiszące na Allegro - dostęp: 2019-07-07Rys. 7. Portal Allegro - Oferty sponsorowane na dole listy produktówźródło: Fotele wiszące na Allegro - dostęp: 2019-07-076. Prawo MilleraWedług badań opublikowanych w 1956 roku w artykule “Magiczna liczba siedem plus minus dwa…” przez Georga A. Millera człowiek jest w stanie zapamiętać 7 informacji naraz. Dobrym przykładem wykorzystania tego prawa jest serwis Netflix. Po wejściu na stronę główną platformy zostaniemy zasypani propozycjami filmówi i seriali, które z pewnością umilą nam niedzielny wieczór. Projektanci serwisu, zadbali o to, abyśmy nie zagineli w gąszczu propozycji, zostały one podzielone na kategorie, a każda z nich z kolei kusi 6 pozycjami.Rys. 8. Strona główna serwisu Netflixźródło: netflix.com - dostęp: 2019-07-077. Prawo TesleraStwierdza, że dla każdego systemu istnieje pewna złożoność, której nie można zmniejszyć. Zbyt duża liczba koniecznych działań może nas skutecznie zniechęcić do skorzystania z niektórych usług, jak chociażby wypełnianie mnóstwa pól formularza w sklepie internetowym podczas zakupu wymarzonej pary butów. Niestety w przypadku, gdy chcemy zarezerwować idealne miejsce na weekendowy pobyt, bez podania tych wszystkich informacji jak np. dokąd chcemy lecieć, na jak długo czy kiedy planujemy wrócić, system nie domyśli się, że chcielibyśmy spędzić tydzień w igloo na mroźnej północy Norwegii.Rys. 9. Strona główna serwisu Booking.comźródło: booking.com - dostęp: 2019-07-078. Prawo HickaWilliam Edmund Hick i Ray Hyman przeprowadzili serię eksperymentów, dzięki którym zaobserwowali, że im większa liczba opcji, tym dłużej będziemy podejmować decyzję W związku z tym rozsądną strategią jest zminimalizowanie alternatyw. Jako przykład może nam posłużyć serwis Airbnb, który podszedł do problemu rezerwacji w zupełnie inny sposób niż np. Booking.com. Nie musimy od razu wypełniać wszystkich informacji - wystarczy, że wpiszemy miejsce do którego chcemy się udać, a resztę wypełniamy w trakcie przeglądania dostępnych ofert.Rys. 10. Strona główna serwisu Airbnb.comźródło: airbnb.com - dostęp: 2019-07-079. Prawo FittsaMówi o tym, że czas jaki potrzebny jest do wykonania akcji, będzie zależny od wielkości obiektu, z którym mamy wejść w interakcję i odległości w jakiej znajduje się ten obiekt od użytkownika. Mówiąc inaczej, duży przycisk, który znajduje się bliżej użytkownika będzie prostszy do wciśnięcia, niż ten, który będzie mały i oddalony.Rys. 11. Zbiórka na projekt wodoodpornych butów w serwisie Kickstarterźródło: kickstarter.com - dostęp: 2019-07-0710. Prawo bliskościMówi o tym, że elementy znajdujące się blisko siebie traktujemy jako powiązane ze sobą lub jako jedną całość. Często wykorzystuje się je w nawigacji. Zgrupowanie linków w wierszu lub kolumnie pozwala zinterpretować użytkownikowi tę część serwisu jako jedność. W zasadzie bliskości chodzi o zastosowanie takich efektów, by użytkownik zrozumiał, że należą do jednej grupy. Taki efekt uzyskamy przez zastosowanie odpowiednio dużych odstępów czy też umieszczenie ich w ramce.Ryz. 12. Strona główna Amazonźródło: amazon.com - dostęp: 2019-07-07PodsumowaniePrzedstawiłem jedynie wybrane prawa spośród 19, które istnieją. Zachęcam do zapoznania się z pozostałymi prawami dostępnymi pod adresem: https://lawsofux.com",
"url": "/2019/07/12/10-praw-uzytecznego-designu.html",
"author": "Piotr Świerzko",
"authorUrl": "/authors/pswierzko.html",
"image": "pswierzko.jpg",
"highlight": "/assets/img/posts/2019-07-12-10-praw-uzytecznego-designu/design-w-it.jpg",
"date": "12-07-2019",
"path": "pl/2019-07-12-10-praw-uzytecznego-designu"
}
,


"2019-05-09-node-version-menager-html": {
"title": "Korzyści wynikające z użycia Node Version Manager",
"lang": "pl",
"tags": "nvm node",
"content": "JavaScript i cały ekosystem z nim związany jest bardzo rozbudowany i wydaje się, że wcale nie zamierza przestać się rozrastać. Można czasem usłyszeć, że tydzień, w którym nie powstał nowy framework do JSa jest tygodniem straconym. Masa bibliotek w różnych wersjach, kolejne języki rozbudowujące możliwości JavaScriptu zmieniające to, jak go postrzegamy np.: TypeScript czy CoffeeScript, do tego jeszcze Node.js oraz różne silniki w przeglądarkach. W rezultacie mamy całkiem sporą listę i coraz więcej pracy związanej z zarządzaniem tym wszystkim.Dlatego tym bardziej warto zainteresować się rozwiązaniami, które mają na celu ułatwić nam życie.Jednym z nich jest nvm, czyli Node Version Manager, który ma nam pomóc zarządzać wersjami Node.js.Cel jest prosty - umożliwić nam szybkie i łatwe przeskakiwanie między wersjami Node niewymagające uprawnień administratora.Zalety+ instalacja nie wymaga uprawnień roota,+ dostęp do wielu wersji Node,+ szybkie i wygodne zmiany wersji noda,+ możliwość wykorzystania różnych wersji Node dla różnych projektów.Wady- nvm manipuluje .bashrc,- lag na starcie bash.Podstawowe polecenia nvm      wylistowanie wszystkich dostępnych wersji    nvm ls-remote        Zwróci nam wynik w postaci listy dostępnych wersji z zaznaczoną obecnie używaną.    v0.1.14...v10.15.0-&amp;gt;  v10.15.1v10.15.2...v12.2.0            instalacja najnowszej wersji Node    nvm install Node        w rezultacie otrzymamy informacje jak poniżej    $ nvm install nodeDownloading and installing node v12.2.0...Downloading https://nodejs.org/dist/v12.2.0/node-v12.2.0-linux-x64.tar.xz...######################################################################### 100,0%Computing checksum with sha256sumChecksums matched!Now using node v12.2.0 (npm v6.9.0)            instalacja najnowszej wersji oznaczonej jako Long-term support    nvm install --lts        poniżej rezultat     $ nvm install --lts Installing latest LTS version. Downloading and installing node v10.15.3... Downloading https://nodejs.org/dist/v10.15.3/node-v10.15.3-linux-x64.tar.xz... ######################################################################### 100,0% Computing checksum with sha256sum Checksums matched! Now using node v10.15.3 (npm v6.4.1)            instalacja konkretnej wersji Node (po instalacji aktualnie używaną wersją jest ta ostatnio zainstalowana)    nvm install {VERSION}        tu rezultat wygląda bardzo podobnie    $ nvm install 8.6.0Downloading and installing node v8.6.0...Downloading https://nodejs.org/dist/v8.6.0/node-v8.6.0-linux-x64.tar.xz...######################################################################### 100,0%Computing checksum with sha256sumChecksums matched!Now using node v8.6.0 (npm v5.3.0)            przestawienie domyślnej wersji Node    $ nvm alias default 10.15.1default -&amp;gt; 10.15.1 (-&amp;gt; v10.15.1)            wylistowanie wszystkich zainstalowanych wersji    nvm ls        zwraca nam listę aktualnie zainstalowanych wersji    v10.15.0-&amp;gt;  v10.15.1v10.15.2            użycie konkretnej wersji    nvm use Node {VERSION}        nvm potwierdzi nam zmianę wersji    $ nvm use 12.2.0Now using node v12.2.0 (npm v6.9.0)            uruchomienie aplikacji w wybranej wersji Node    nvm run {VERSION} index.js        tu również nie będziemy mieć wątpliwości z jaką wersją uruchomiliśmy aplikację    $ nvm run 8.6.0 index.js Running node v8.6.0 (npm v5.3.0)            usunięcie wybranej wersji Node    nvm uninstall {VERSION}        potwierdzenie usunięcia    $ nvm uninstall 8.6.0Uninstalled node v8.6.0      Ponadto .nvmrc.Warto pamiętać, że w ramach projektu możemy łatwo ustalić jak wersja Node ma być wykorzystywana, a nvm zajmie się resztą.Do projektu wystarczy dodać plik .nvmrc.  9.0.1i wykonać polecenie  nvm useUwaga na koniecNvm zwiększa objętość pliku .bashrc co spowalnia działanie terminala, trzeba się zastanowić czy na środowisku produkcyjnym jest to koszt, który chcemy ponosić.Link do projektuhttps://github.com/nvm-sh/nvm/blob/master/README.md",
"url": "/2019/05/09/node-version-menager.html",
"author": "Krzysztof Czechowski",
"authorUrl": "/authors/kczechowski.html",
"image": "kczechowski.jpg",
"highlight": "/assets/img/posts/2019-05-09-node-version-menager/Node-Version-Manager.jpg",
"date": "09-05-2019",
"path": "pl/2019-05-09-node-version-menager"
}
,


"2019-05-06-refactoring-html": {
"title": "Przykłady refaktoryzacji na podstawie książki Martina Fowlera i Kenta Becka 'Refactoring'",
"lang": "pl",
"tags": "refaktoryzacja javascript",
"content": "Książka “Refactoring” Martina Fowlera i Kenta Becka została po raz pierwszy wydana w 1999 roku i często określana jest jako pozycja wybitna, ponadczasowa, jako must read każdego programisty. Dodatkowo w tym roku wyszła jej druga edycja. Po co?Martin podkreśla przecież, że notatki z lat dziewięćdziesiątych, które tworzył i ogólnie zasady refaktoryzacji wciąż są aktualne i wciąż sam ich używa. Mimo to wszystkie jej rozdziały zostały przepisane, a przede wszystkim zmienił się język wykorzystany w przykładach - wtedy wybrali Javę, tym razem postawili na JavaScript.Dominująca część tej pozycji to katalog reguł refaktoryzacji wraz z motywacją, sposobem jej wykonania i oczywiście kodem. Zanim jednak tam dotrzemy, Fowler próbuje wytłumaczyć, po co w ogóle refaktoryzować i kiedy to robić.I to, co najbardziej wyniosłam z tej lektury to podejście, które przewija się w niej bardzo często: mianowicie, że refaktoryzacja powinna być “częścią naturalnego flow programisty”.Gdy podchodzimy do zadania, chcemy dodać nową funkcjonalność, powinniśmy móc ją wprowadzić w zastany kod w sposób łatwy i ładny. Jeśli jednak jego stan nam na to nie pozwala, może powinniśmy pokusić się o jego reorganizację, tak żebyśmy mogli później szybko i bez wyrzutów sumienia wprowadzać zmiany. Bardzo trafnie opisuje to cytat:&quot;It&#39;s like I want to go 100 miles east but instead of just traipsing through the woods, I&#39;m going to drive 20 miles north to the highway and then I&#39;m going to go 100 miles east at three times the speed I could have if I just went straight there. When people are pushing you to just go straight there, sometimes you need to say &#39;Wait, I need to check the map and find the quickest route&#39;.&quot;Zanim przejdziemy do przykładów, jeszcze jedna motywacja, czyli to, jak określa sam siebie Kent Beck:&quot;I&#39;m not a great programmer. I&#39;m just a good programmer with great habits.&quot;Przykład pierwszyfunction solveIdealRocketEquation(specificImpulse, initialMass, finalMass) {\tconst exhaustVelocity = specificImpulse * 9.81;\tconst massFraction = initialMass/finalMass;\treturn exhaustVelocity * Math.log(massFraction)}Refaktor powyższego przykładu rozpoczniemy od zastosowania wyodrębnienia metod (Extract Function).function solveIdealRocketEquation(specificImpulse, initialMass, finalMass) {\tconst exhaustVelocity = specificImpulse * 9.81;\tconst massFraction = initialMass/finalMass;\treturn calculateVelocity(exhaustVelocity, massFraction)}function calculateVelocity(exhaustVelocity, massFraction){\treturn exhaustVelocity * Math.log(massFraction)}Następnie wprowadzimy strukturę służącą do komunikacji (Introduce Parameter Object).function solveIdealRocketEquation(specificImpulse, initialMass, finalMass) {\tconst exhaustVelocity = specificImpulse * 9.81;\tconst massFraction = initialMass/finalMass;\tconst velocityData = {exhaustVelocity: exhaustVelocity, massFraction: massFraction}\treturn calculateVelocity(velocityData)}function calculateVelocity(velocityData){\treturn velocityData.exhaustVelocity * Math.log(velocityData.massFraction)}I z tak posprzątanymi parametrami, możemy wydzielić dwie, niezależne fazy naszych obliczeń (Split Phase).function solveIdealRocketEquation(specificImpulse, initialMass, finalMass) {\tconst velocityData = calculateVelocityData(specificImpulse, initialMass, finalMass)\treturn calculateVelocity(velocityData)}function calculateVelocity(velocityData){\treturn velocityData.exhaustVelocity * Math.log(velocityData.massFraction)}function calculateVelocityData(specificImpulse, initialMass, finalMass){\tconst exhaustVelocity = specificImpulse * 9.81;\tconst massFraction = initialMass/finalMass;\treturn {exhaustVelocity: exhaustVelocity, massFraction: massFraction}}Przykład drugifunction aggregateStarsData(stars){    let highestTemperature = stars[0] ? stars[0].temperature : 0;    let totalMass = 0;    for (const s of stars) {\t    if (s.temperature &amp;gt; highestTemperature) {\t\thighestTemperature = s.temperature\t    }\ttotalMass += s.mass    }    return {highestTemperature: highestTemperature, totalMass: totalMass}}W powyższym przykładzie obliczamy łączną masę gwiazd oraz znajdujemy gwiazdę, której temperatura jest najwyższa. Wyliczenia te przetwarzane są w tej samej pętli, chociaż są od siebie niezależne. Minusem tego podejścia jest to, że za każdym razem, gdy będziemy chcieli taką pętlę zmodyfikować, będziemy musieli zrozumieć obie te rzeczy. Podzielmy ją więc na dwie, tak abyśmy mogli próbować zrozumieć tylko tę część, którą musimy zmodyfikować (Split Loop).function aggregateStarsData(stars){    let highestTemperature = stars[0] ? stars[0].temperature : 0;    for (const s of stars) {\t    if (s.temperature &amp;gt; highestTemperature) {\t\t    highestTemperature = s.temperature\t    }    }    let totalMass = 0;        for (const s of stars) {\t        totalMass += s.mass    }    return {highestTemperature: highestTemperature, totalMass: totalMass}}Następnie powydzielajmy metody (Extract Function).function aggregateStarsData(stars){\treturn {highestTemperature: highestTemperature(stars), totalMass: totalMass(stars)}}function highestTemperature(stars){\tlet highestTemperature = stars[0] ? stars[0].temperature : 0;\tfor (const s of stars) {\t\tif (s.temperature &amp;gt; highestTemperature) {\t\t\thighestTemperature = s.temperature\t\t}\t}\treturn highestTemperature;}function totalMass(stars){\tlet totalMass = 0;\tfor (const s of stars) {\t\ttotalMass += s.mass\t}\treturn totalMass;}I użyjmy jeszcze dwóch metod refaktoryzacji: Replace Loop with Pipeline oraz Subsitute Algorithm.function aggregateStarsData(stars){\treturn {highestTemperature: highestTemperature(stars), totalMass: totalMass(stars)}}function highestTemperature(stars){    return Math.max(...stars.map(s=&amp;gt; s.temperature))}function totalMass(stars){    return stars.reduce((totalMass, s) =&amp;gt; totalMass += s.mass, 0);}Podsumowanie“Refactoring” Fowlera i Becka to książka, którą na pewno docenią osoby, które pracują przy dużych i długofalowych projektach. Wtedy bowiem potrzeba refaktorowania jest bezdyskusyjna, jednak jest on czasochłonny i ryzykowny. Łatwo wpaść w pułapkę, w której zmiany, które dodajemy, wprowadzają bałagan do kodu i tłumaczone są pośpiechem. Jednak czas, który zaoszczędzimy przy jej dodawaniu, zostanie zjedzony w całości, a pewnie i przekroczony, gdy kolejna osoba będzie musiała wejść w nasze klasy i zrozumieć, jak to właściwie działa.Podejdźmy więc do problemu zdrowo i po prostu - zawsze zostawiajmy kod choć odrobinę lepszym, niż ten który zastaliśmy;)",
"url": "/2019/05/06/refactoring.html",
"author": "Sylwia Üçüncü",
"authorUrl": "/authors/sucuncu.html",
"image": "sucuncu.jpg",
"highlight": "/assets/img/posts/2019-05-06-refactoring/refactoring.jpg",
"date": "06-05-2019",
"path": "pl/2019-05-06-refactoring"
}
,


"2019-03-27-custom-elements-html": {
"title": "Krótkie wprowadzenie do Custom Elements",
"lang": "pl",
"tags": "frontend custom elements web",
"content": "Custom element, co to takiego?Custom Elements to jedna z zestawu czterech specyfikacji występujących pod wspólną nazwą Web Components - wspólnie pozwalają one na tworzenie własnych typów elementów DOM.Na Web Components składają sie następujące specyfikacje:  Templates - wprowadza element &amp;lt;template&amp;gt;, który pozwala na wyrenderowanie jego zawartości dopiero na żądanie stworzenia kopii. Dzięki temu problem z przedwczesnym ładowaniem danych nie występuje,  HTML imports - tworzone komponenty mogą zawierać szablony (Templates) i kod (Custom elements), specyfikacja ta pozwala wydzielić obie te części do oddzielnego pliku HTML i importować go za pomocą &amp;lt;link rel=&quot;import&quot; href=&quot;plik-komponentu.html&quot; /&amp;gt;,  Shadow DOM - specyfikacja ta pozwala na enkapsulację DOM’u oraz styli. Każdy element może mieć swój shadow root, który jest wyświetlany jako jego zawartość, przy czym zawartość ta jest odseparowana logicznie od pozostałych elementów DOM,  Custom Elements - specyfikuje sposób tworzenia własnych elementów DOM oraz dostarcza obiekty do kontrolowania cyklu życia elementu.W tym artykule skupimy się jedynie na Custom Elements (oraz w mniejszym stopniu na Shadow DOM), który jest minimalnym zestawem narzędzi pozwalającym na dodanie własnego elementu HTML niezależnego od wykorzystywanych (lub nie) frameworków czy bibliotek.Wsparcie przez główne przeglądarkiWsparcie custom components przez główne przeglądarkiźródło: www.webcomponents.org - dostęp: 2019-03-16Custom Elements jest wspierany przez większość najpopularniejszych przeglądarek. Na pozostałych implementację zapewniają polyfille:  https://github.com/webcomponents/custom-elements lub  https://github.com/webcomponents/webcomponentsjsCustomElementRegistryObiekt typu CustomElementRegistry zapewnia metody pozwalające na rejestrowanie oraz pobierania już zarejestrowanych elementów. Instancję klasy CustomElementRegistry otrzymamy odwołując się do window.customElements. W klasie tej znajdziemy następujące metody:CustomElementRegistry.define(localName: string, constructor: Function, options?: {extends: string}): voidPozwala na zdefiniowanie elementu. Pierwszym parametrem jest nazwa tagu, drugim konstruktor klasy elementu. Dodatkowo, można podać trzeci parametr, który zawiera opcje komponentu. W aktualnej wersji specyfikacji dostępna jest jedynie opcja extends, której wartością jest nazwa rozszerzanego elementu (wykorzystujemy ją wyłacznie w przypadku rozszerzania już istniejącego elementu).Przykładowe zastosowanie:customElements.define(&#39;my-element&#39;, class extends HTMLElement {    connectedCallback() {        this.innerHTML = &#39;&amp;lt;strong&amp;gt;hello world&amp;lt;/strong&amp;gt;&#39;;    }});Tak zdefiniowanego elementu można użyć w następujący sposób:&amp;lt;my-element&amp;gt;&amp;lt;/my-element&amp;gt;Ograniczenia nazwy taguNazwa naszego elementu musi spełniać następujące wyrażenie regularne:^[a-z][.0-9_a-z]*-[\\-.0-9_a-z]*$Innymi słowy tag musi zaczynać się od litery, musi zawierać przynajmniej jeden myślnik, a poza tym może zawierać jedynie litery alfabetu łaińskiego oraz następujace znaki: _, . i -. Taka reguła sugeruje aby stosować konwencję nazewniczą kebab-case.Dodatkowo, nazwa nie może kolidować z żadną nazw z następującej listy:  annotation-xml,  color-profile,  font-face,  font-face-src,  font-face-uri,  font-face-format,  font-face-name,  missing-glyph.CustomElementRegistry.get(name: string): Function|undefinedMetoda get zwraca constructor utworzonego custom elementu lub undefined, jeżeli taki nie został znaleziony.Przykład:const myElement = customElements.get(&#39;my-element&#39;);CustomElementRegistry.upgrade(root: Node): voidMetoda upgrade pozwala zainicjować element znajdujący się w DOM po tym jak custom element został zarejestrowany.Przykład:const el = document.createElement(&quot;my-element&quot;);class MyElement extends HTMLElement {}customElements.define(&quot;my-element&quot;, MyElement);console.assert(!(el instanceof MyElement)); // not yet upgradedcustomElements.upgrade(el);console.assert(el instanceof MyElement);    // upgraded!CustomElementRegistry.whenDefined(): Promise&amp;lt;undefined&amp;gt;Zwraca Promise, który rozwiązany jest w momencie, gdy element zostanie zarejestrowany.Przykład:customElements.whenDefined(&#39;my-element&#39;).then(() =&amp;gt; {    // ...});Zarządzanie cyklem życia custom elementuW każdym custom elemencie możemy wykorzystać jeden z predefiniowanych callbacków:  connectedCallback - wywoływany za każdym razem, gdy custom element jest dołączany do dokumentu,  disconnectedCallback - wywoływany zawsze po odłączeniu custom elementu z DOM,  adoptedCallback - wywoływany po przeniesieniu custom elementu do innego dokumentu,  attributeChangedCallback - wywoływany, gdy atrybuty elementu zostaną dodane, usunięte lub zmodyfikowane - jest wywoływany jedynie dla atrybutów, których nazwy zostaną zwrócone ze statycznego pola observedAttributes.Custom elementy w akcjiPrzykładowy prosty custom elementZałóżmy, że w naszej aplikacji chcemy stworzyć komponent obrazka z podpisem. Do tej pory używaliśmy HTMLa o takiej strukturze:&amp;lt;div class=&quot;image-with-caption&quot;&amp;gt;    &amp;lt;img class=&quot;image&quot; src=&quot;obrazek.jpg&quot; /&amp;gt;    &amp;lt;div class=&quot;image-caption&quot;&amp;gt;Podpis obrazka&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;style&amp;gt;img.image {   max-width: 100%;   max-height: 500px;}.image-caption {   font-size: 10px;}&amp;lt;/style&amp;gt;Chcielibyśmy wydzielić taki fragment kodu, do elementu, który będziemy definiować w następujący sposób:&amp;lt;image-with-caption src=&quot;obrazek.jpg&quot;&amp;gt;Podpis obrazka&amp;lt;/image-with-caption&amp;gt;Zacznijmy od stworzenia i zarejestrowania komponentu:class ImageWithCaption extends HTMLElement {    constructor() {        super();    }}window.customElements.define(&#39;image-with-caption&#39;, ImageWithCaption);Na tym etapie po dodaniu &amp;lt;image-with-caption /&amp;gt; zostanie na nim zainicjowany element ImageWithCaption. Zajmijmy się dodaniem obrazka.Dodajmy do klasy pole img typu HTMLImageElement:class ImageWithCaption extends HTMLElement {    private readonly img: HTMLImageElement;    // ...}Następnie do konstruktora dopiszmy:constructor() {    // ...    this.img = document.createElement(&#39;img&#39;);        this.attachShadow({mode: &#39;open&#39;});    this.shadowRoot.appendChild(this.img);}W ten sposób dodaliśmy do elementu shadow root (dzięki temu style elementu będą odseparowane od dokumentu).A potem dodajmy metodę connectedCallback:connectedCallback(): void {    this.img.src = this.getAttribute(&#39;src&#39;);}W ten sposób przypiszemy źródło obrazka z atrybutu src elementu &amp;lt;image-with-caption src=&quot;...&quot;&amp;gt;. Na tym etapie po osadzeniu naszego elementu pojawi się obrazek, który wskażemy w atrybucie src. Niestety jego wartość nie będzie mogła się zmieniać po inicjalizacji elementu. Aby nasłuchiwać na zmiany po inicjalizacji należy zadeklarować, że będziemy nasłuchiwać na zmiany atrybutu src:static get observedAttributes(): string[] {    return [&#39;src&#39;];}Następnie należy zdefiniować metodę attributeChangedCallback, która posłuży do obsługi zmian atrybutu src:attributeChangedCallback(name: string, oldValue: string, newValue: string): void {    if (name === &#39;src&#39;) {        this.img.src = newValue;    }}Dzięki temu możemy zmieniać wartość atrybutu src po zainicjowaniu komponentu. Zajmijmy się teraz dodaniem etykiety do obrazka. Do konstruktora dopiszmy następujący kod:const caption: HTMLDivElement = document.createElement(&#39;div&#39;);caption.innerHTML = &#39;&amp;lt;slot&amp;gt;&amp;lt;/slot&amp;gt;&#39;;this.shadowRoot.appendChild(caption);Dodaliśmy do naszego shadow DOM element &amp;lt;div&amp;gt;, którego zawartość zdefiniowaliśmy jako &amp;lt;slot&amp;gt;&amp;lt;/slot&amp;gt;. Podczas działania aplikacji &amp;lt;slot&amp;gt;&amp;lt;/slot&amp;gt; zostanie zastąpione zawartością elementu &amp;lt;image-with-caption&amp;gt;&amp;lt;/image-with-caption&amp;gt;.Zostało nam dodanie styli do naszego komponentu. Dopiszmy do konstruktora:const style: HTMLStyleElement = document.createElement(&#39;style&#39;);style.innerHTML = `    img {        max-width: 100%;        max-height: 500px;    }    div {        font-size: 10px;    }`;this.shadowRoot.appendChild(style);Dzięki użyciu Shadow DOM style, które właśnie dodaliśmy nie wypływają poza element.Voilà!Mały bonus - Rozszerzanie istniejących elementówPoza możliwością zdefiniowania nowego elementu specyfikacja Custom Elements pozwala na rozszerzenie już istniejących elementów. Załóżmy, że chcemy dokonać prostej modyfikacji elementu &amp;lt;a&amp;gt; polegającej na tym, że przejście do łącza nastąpi dopiero po potwierdzeniu przez użytkownika. Poniżej kod przykładowego elementu:class LinkWithConfirmation extends HTMLAnchorElement {    constructor() {        super();    }        connectedCallback(): void {        this.addEventListener(&#39;click&#39;, (event: MouseEvent) =&amp;gt; {            if (!confirm(&#39;Are you sure?&#39;)) {                event.preventDefault();            }        });    }}customElements.define(&#39;link-with-confirmation&#39;, LinkWithConfirmation, { extends: &#39;a&#39; });Zwróćmy uwagę, że w ostatnim parametrze metody define przekazaliśmy obiekt { extends: &#39;a&#39; }, który informuje, że będziemy rozszerzać element &amp;lt;a&amp;gt;.Aby skorzystać z napisanego elementu musimy użyć elementu &amp;lt;a&amp;gt; z atrybutem is o wartości link-with-confirmation, a nie &amp;lt;link-with-confirmation&amp;gt;:&amp;lt;a is=&quot;link-with-confirmation&quot; href=&quot;https://consdata.com&quot;&amp;gt;consdata.com&amp;lt;/a&amp;gt;Przydatne linki  www.webcomponents.org  Wsparcie dla custom elements  Dobre praktyki z przykładami  O Custom elements na MDN",
"url": "/2019/03/27/custom-elements.html",
"author": "Mateusz Pogorzelski",
"authorUrl": "/authors/mpogorzelski.html",
"image": "mpogorzelski.jpg",
"highlight": "/assets/img/posts/2019-03-27-custom-elements/custom-elements.png",
"date": "27-03-2019",
"path": "pl/2019-03-27-custom-elements"
}
,


"2019-03-27-java-darmowa-czy-nie-html": {
"title": "Java darmowa, czy nie?",
"lang": "pl",
"tags": "java",
"content": "End of Public Updates for Oracle JDK 8Oracle will not post further updates of Java SE 8 to its public download sites for commercial use after January 2019. Customers who need continued access to critical bug fixes and security fixes as well as general maintenance for Java SE 8 or previous versions can get long term support through Oracle Java SE Subscription or Oracle Java SE Desktop Subscription. For more information, and details on how to receive longer term support for Oracle JDK 8, please see the Oracle Java SE Support Roadmap.https://www.oracle.com/technetwork/java/javase/overview/index.html15 stycznia tego roku światło dzienne ujrzał JDK 8u202 - ostatni darmowy update JDK 8. Darmowy do zastosowań komercyjnych. Wersja JDK 8 była pierwszą wersją LTS (Long-Term-Support) i była z nami od marca 2014 roku. Co dalej? Czy można zostać na JDK 8? Czy przesiadać się na kolejną wersję?Oracle JDK vs OpenJDKW obecnej sytuacji szczególnego znaczenia nabierają różnice pomiędzy JDK releasowanym przez Oracle a OpenJDK.  OpenJDK jest projektem open source dostarczającym implementację  Java Platform, Standard Edition. Projekt działa od 2007 roku, a jednym z głównych kontrybutorów jest Oracle.  Oracle JDK natomiast to dystrybucja JDK dostarczana i supportowana przez Oracle w ramach OTN (Oracle Technology Network).Obydwie wersję mają w zasadzie ten sam code base. Techniczne różnice są niewielkie i dotyczą głównie narzędzi i deploymentu. Różnice, które z punktu widzenia ostatnich zmian są najbardziej istotnie to licencjonowanie i cykl wydawniczy.Licencjonowanie i opłatyOpenJDK jest projektem open source licencjonowanym w oparciu o GNU General Public License, version 2 with CE.W Oracle JDK model licencjonowania różni się w zależności od wersji. Wersje sprzed wersji 11 licencjonowane są w oparciu o Oracle BCL (Binary Code Licence). Od wersji 11 Oracle JDK licencjonowane jest w oparciu o Oracle Java SE OTN License, który nie pozwala na komercyjne użycie. Jeżeli chcemy używać tej wersji do zastosowań komercyjnych musimy wykupić subskrypcję “Oracle Java SE subscription”. Subskrypcja jest rozliczana w cyklu miesięcznym, a końcowa cena zależy od:  sposobu użycia (desktop/serwer),  liczby rdzeni,  mnożnika Oracle Processor Core Factor.Według aktualnego cennika za jeden, obliczony na podstawie mnożnika, core zapłacimy 25$ przy założeniu, że corów jest mniej niż 99.Cykl wydawniczyBiorąc pod uwagę powyższe informacje wydaje się, że naturalnym krokiem dla tych, którzy nie chcą płacić za support jest przesiadka na wersję OpenJDK, jest tu jednak pewien haczyk. Aby go odkryć musimy wiedzieć jaki jest cykl wydawniczy poszczególnych wersji.Jakiś czas temu Oracle postanowił nadać pewien rygor czasowy kolejnym wydawanym wersjom. Kolejne wersje Javy będą ukazywały się co pół roku. Niektóre z nich będą wersjami LTS, a pozostałe będą zastępowane kolejnymi i nie będą dalej rozwijane. Aktualnie wersje LTS to:  wersja 8 - wspierana do marca 2025 roku,  wersja 11 - wspierana do września 2026 roku.Oracle będzie publikował nowe wydania wersji raz na kwartał. Dotyczy to zarówno wersji LTS jak i non-LTS. Niestety w przypadku OpenJDK Oracle nie będzie wydawał więcej niż dwóch wersji również dla wersji LTS. Oznacza to, że jeżeli wybieramy wersję LTS oczekując długiego czasu wsparcia nie możemy polegać na releasach OpenJDK dostarczanych przez Oracle (https://openjdk.java.net/).Skąd wziąć darmową wersję?Czy zatem wybierając wersję LTS jesteśmy skazani na “własnoręczne” budowanie OpenJDK? Na szczęście nie. Istnieje jeszcze kilku dostawców, którzy będą publikować kolejne wydania bazujące na OpenJDK. Z tych bardziej obiecujących warto wymienić:  AdoptOpenJDK - warte zainteresowania chociażby ze względu na zróżnicowany zbiór sponsorów (IBM, Microsoft Azure, Azul Systems),  Amazon Corretto - ta wersja będzie używana w chmurze AWS można więc liczyć na solidny support,  Azul - dostarczają darmową implementację JDK z opcją płatnego wsparcia,  RedHat - planuje dostarczać pakiety z aktualizacjami JDK dla wersji 8 na RHEL 6 i RHEL7 do 2023 roku, a dla wersji 11 na RHEL7 do 2024 roku.Przydatne linki  Java is still free  Oracle Java SE Support Roadmap  Update and FAQ on the Java SE Release Cadence",
"url": "/2019/03/27/java-darmowa-czy-nie.html",
"author": "Jakub Wilczewski",
"authorUrl": "/authors/jwilczewski.html",
"image": "jwilczewski.jpg",
"highlight": "/assets/img/posts/2019-03-22-java-darmowa-czy-nie/java-darmowa.png",
"date": "27-03-2019",
"path": "pl/2019-03-22-java-darmowa-czy-nie"
}
,


"2019-02-26-pozycjonowanie-zale-ne-od-scrolla-html": {
"title": "Pozycjonowanie zależne od scrolla",
"lang": "pl",
"tags": "frontend javascript",
"content": "Czasem zachodzi potrzeba uzależnienia pozycji elementu od scrolla okna, czy to na potrzeby przyklejenia w widocznym obszarze, czy też stworzenia efektu paralaksy lub niestandardowego flow nawigacji. Temat wydawałby się oczywisty, gdyby nie to, że celowo wprowadzimy sobie dodatkowe ograniczenia (co wcale nie jest takie niecodziennie, uwzględniając fantazję działów UI/UX ;-)).Podejście 1: tylko CSSMamy dwa sposoby przyklejenia elementu do ekranu wykorzystujące tylko CSS. Oba opierają się o zmianę pozycjonowania:  position: fixed,  position: sticky.Oba też mają swoje problemy i ograniczenia.Stosując pozycjonowanie fixed:  musimy uwzględnić pozostawione przez niego miejsce w oryginalnym fragmencie drzewa DOM,  jeżeli element będzie wyższy niż viewport to nie będziemy mieli możliwości obejrzeć niemieszczącej się zawartości,  fixed zawsze tworzy nowy stacking context.Stosując pozycjonowanie sticky:  jeżeli element będzie wyższy niż viewport, to nie będziemy mieli możliwości obejrzeć niemieszczącej się zawartości,  sticky zawsze tworzy nowy stacking context,  sticky czasem może zaskoczyć swoim działaniem (przykładowo issue w3c).O ile uwzględnienie oderwanego przez fixed elementu w layoucie nie stanowi wyzwania, o tyle brak wsparcia dla przewijania treści i zmiana stacking context (co wpłynie np. na liczenie kolejności na osi z) mogą stanowić już zbyt duże ograniczenia.W wielu przypadkach fixed lub sticky załatwią problem. Jeśli jednak potrzebujesz czegoś więcej, czytaj dalej.Podejście 2: JavaScript„Nie ma takiej rzeczy, której bym nie napisał w JavaScript.” 😉Przeglądarki oferują nam zdarzenie związane ze scrollowaniem treści. Na zdarzenie możemy nasłuchiwać przez zdefiniowanie własności target.onscroll, czy też bardziej elastycznie, dodając listener przez target.addEventListner(’scroll’). Teoretycznie wystarczyłoby już tylko przeliczać pozycję przyklejanego elementu, obsłużyć przewijanie w dwóch kierunkach i nie zapomnieć o użyciu najmniej obciążającej metody przesuwania elementów po ekranie. Co może pójść źle? Sprawdźmy prosty przykład.Implementujemy proste przeliczanie pozycji nasłuchując na zdarzenie scroll:Uzyskany efekt:Okazuje się, że funkcjonalnie możemy uzyskać wszystko, czego potrzebujemy, jednak jakość rozwiązania nie jest zadowalająca. Gdy przyjrzymy się sprawie bliżej, zauważymy, że na różnych przeglądarkach mamy różne problemy z płynnym rysowaniem UI. Obserwujemy lekki pościg naszego elementu względem reszty strony - to stanowczo nie jest efekt, z którym chcemy być kojarzeni.Możemy jeszcze raz przeanalizować nasze kody, przekonać się, że ani throttlowanie zdarzeń, ani przesuwanie transformem, ani nawet wymyślne funkcje wygładzające nic nie dają. Okazuje się, że odpowiedź jest równocześnie dobra i zła, dobra - bo z naszym kodem nie ma większych problemów; zła - bo tak po prostu działają przeglądarki, na co niespecjalnie mamy wpływ!Całe zamieszanie wynika z tego, że większość nowoczesnych przeglądarek obsługuje rysowanie oraz scrollowanie w osobnych wątkach. W praktyce oznacza to, że pozycja strony oraz jej zawartość liczone są w różnych momentach. Brak synchronizacji na tych operacjach objawia się skakaniem przesuwanego elementu. O ile takie rozwiązanie ułatwia przeglądarkom uzyskiwać upragnione 60 fps przy renderowaniu, o tyle dla nas oznacza skreślenie tego rozwiązania z listy wartościowych.Co dalej?Czy to oznacza, że jeśli rozwiązanie z pozycjonowaniem CSS oferuje za mało funkcjonalności, a na lag przy rysowaniu z JavaScript nie możemy sobie pozwolić, to musimy rozłożyć ręce? Oczywiście, że nie! Na początek chwyćmy się wyjaśnienia z poprzednich akapitów - problemem jest, że scroll viewportu i DOM strony rysowane są niezależnie, w różnych momentach czasu. Gdybyśmy jednak potrafili zapewnić, że obie te rzeczy będą się działy synchronicznie? O ile nie możemy do tego zmusić przeglądarki, o tyle możemy ją oszukać 😉Załóżmy że:  to nie przeglądarka odpowiada za przewijanie treści strony,  scroll przeglądarki wyraża jedynie intencję, w którym miejscu strona ma się znajdować,  faktyczne przesuwanie treści odbywa się w naszym kodzie,  również w naszym kodzie znajduje się obsługa przesuwania przyklejonych elementów,  obliczenia wykonujemy co żądanie klatki animacji.Przy takich założeniach możliwe okazuje się uzyskanie płynnego przewijania i przyklejania elementów. Dodatkowo, proponowane rozwiązanie poza przyklejaniem pierwszy raz oferuje opcję realizacji paralaksy czy niestandardowych przejść strony (kto powiedział, że kolejne ekrany nie mają być po skosie lub na spirali ;-)).RozwiązaniePrzykładowe rozwiązanie może wyglądać następująco:  dotychczasową strukturę DOM opakowujemy we wrapper,  wrapper pozycjonujemy jako fixed na cały ekran (top, bottom, left, right na 0),          to będzie nadrzędny element strony odpowiedzialny za prezentowanie viewport, w tym obsługę przewijania,        obok wrappera definiujemy sztuczny element replikujący wysokość wrappera,          to będzie element odpowiedzialny za symulowanie wysokości strony, dzięki niemu przeglądarka będzie wyświetlała prawidłowy pasek przewijania i poprawnie rozgłaszała związane z nim zdarzenia,        definiujemy metodę renderującą ekran co klatkę animacji,          odpowiada za faktycznie rysowanie pozycji elementów, zarówno standardowo przewijanej zawartości, jak i przyklejonych elementów,        zawartość wrappera przewijamy zgodnie z bieżącym scrollem,  przyklejony element przewijamy odwrotnie, kompensując przesunięcie wrappera.Przykładowa implementacjaPrzedstawione rozwiązanie jest najprostszym z możliwych potwierdzających teoretyczne założenia.W docelowym rozwiązaniu na pewno warto pomyśleć o rozdzieleniu funkcji pętli od faktycznego rysowania, wygładzaniu przesunięcia scrolla, dorzuceniu wskazówki will-change dla przesuwanych elementów, czy ogólnym sposobie na nasłuchiwanie na zmiany scrolla globalnie.Po wprowadzeniu zmian nasz rozwiązanie prezentuje się znacznie lepiej:Sukces?Samodzielna obsługa scrollowania może być kusząca przy realizacji niestandardowych przepływów ekranów, animacji, czy skomplikowanych interfejsów użytkownika. Zawsze jednak należy pamiętać, że przerzucamy na siebie ciężar obsługi czegoś, co jest robione dobrze przez każdą przeglądarkę. Czasem lepszym rozwiązaniem będzie znalezienie uproszczeń w wymaganiach, a czasem będziemy mogli wziąć na siebie taki trade-off 🙂Czy ktoś stosuje takie podejścia? Tak, przykładem niech będzie apple.com, gdzie przewijane początkowo jest pionowe, następnie poziome i na końcu znowu pionowe 😉Przydatne linki  Scroll-linked effects @ MDN  The stacking context  Własności pozycjonowania elementów drzewa DOM @ MDN  What No One Told You About Z-Index",
"url": "/2019/02/26/pozycjonowanie-zale-ne-od-scrolla.html",
"author": "Grzegorz Lipecki",
"authorUrl": "/authors/glipecki.html",
"image": "glipecki.jpg",
"highlight": "/assets/img/posts/2019-02-26-pozycjonowanie-zale-ne-od-scrolla/pozycjonowanie-scroll.jpeg",
"date": "26-02-2019",
"path": "pl/2019-02-26-pozycjonowanie-zale-ne-od-scrolla"
}
,


"2018-12-13-podsumowanie-consdata-tech-odpowiedzi-na-pytanie-html": {
"title": "Podsumowanie Consdata Tech - odpowiedzi na pytania",
"lang": "pl",
"tags": "consdata.tech event sourcing",
"content": "Czy używacie platformy Kafka Connect? Jeśli tak, czy pozwalacie connectorowi na sterowanie schematem bazy danych? Jeśli nie, jakie macie podejście do ewolucji schematów?Nie używamy platformy Kafka Connect, w związku z czym nie jestem w stanie odnieść się do pierwszej części drugiego pytania. Odnośnie części drugiej, schemat naszej bazy danych ewoluuje wyłącznie przyrostowo. Nigdy nie usuwamy pól ze schematu, a jedynie dodajemy nowe, niewymagane pola - dzięki temu nie łamiemy wstecznej kompatybilności z istniejącymi już eventami oraz obiektami znajdującymi się na state storze. Nasza domena pozwala na taki rozwój schematu, jednak oczywiście należy mieć na uwadze, że nie zawsze będzie to możliwe. W takich sytuacjach należy rozważyć na przykład wersjonowanie eventów.Czy nowe podejście w oparciu o ES miało wpływ na wydajność systemu?Pośrednio tak, jednak należy w pierwszej kolejności zaznaczyć, że wydajność nie była dla nas celem samym w sobie, gdyż ta, którą osiągaliśmy jeszcze przed wprowadzeniem Event Sourcingu była w zupełności wystarczajaca. Co więcej, w systemach, w których event store (np. Kafka) staje się źródłem prawdy, a state store zasilany jest wyłącznie za jego pomocą, bezpośredni wpływ na wydajność ma głównie state store, a więc na ogół baza danych - gdyż to na niej wykonywana jest większość operacyjnych zapytań w systemie. Natomiast nie wprost, ale Kafka pozwoliła nam poprawić utylizację zasobów, pośrednio przyczyniając się do ogólnej poprawy wydajności - w szczególności poprzez asynchroniczną obsługę wywołań zmieniających stan systemu, co pozwoliło na znacznie lepsze gospodarowanie pulami wątków w systemie.Czy Kafka to na pewno dobry wybór do ES?Na dokładnie to pytanie odpowiada wcześniejszy artykuł na naszym blogu: Czy Apache Kafka nadaje się do Evnet Sourcingu?Final, Mass oraz External to faktyczne topici, czy tylko kategorie topiców z jakimi pracujecie?Są to rozłączne topiki - każdy ze specyficzną dla siebie konfiguracją partycjonowania, retencji, obsługi błędów itp.Kafka jako event store. Jak dużo danych ma Kafka w waszym mailboxie?Działamy w skali około miliarda eventów, co przekłada się to w tej chwili na około kilka terabajtów danych, biorąc oczywiście pod uwagę replikację.Czy topic final to trochę taki ESB?W tym konkretnym ujęciu - nie. Topic final jest dla nas wyłącznie źródłem prawdy w systemie, a nie elementem integrującym różne jego moduły.Jak dużo macie sytuacji, w których trzeba ręcznie obsługiwać DLQ na produkcji?Jak dotąd, po kilkumiesięcznym działaniu na produkcji, trafiliśmy tylko na jeden przypadek, kiedy wiadomości trafiły na DLQ. Większość błędów, które finalnie mogłyby trafić do DLQ wyłapaliśmy na poziomie topiców Retry, skąd automatycznie schodziły w wyniku udrożnienia zależnych komponentów lub systemów.Co zrobić, gdy trzeba usunąć eventy np. że względu na RODO? Event store, jako audyt jest niezmienny. Jeżeli usuniemy eventy, to możemy też stracić ciągłość stanUsuwanie eventów nie jest jedynym sposobem na spełnienie wymagań narzuconych przez RODO. Jednym z możliwych rozwiązań, bardzo naturalnie wynikającym z idei Event Sourcingu, jest dodanie kolejnych eventów wyrażających usunięcie newralgicznych danych. W ten sposób, na aktualnym state storze newralgiczne dane zostaną usunięte, a tym samym przestaną być dostępne w ramach operacyjnego działania aplikacji. Natomiast w przypadku odtwarzania stanu systemu ze strumienia zdarzeń zostaną one ponownie przetworzone w ten sposób, a więc finalnie usunięte - spełniając tym samym zasadę utrzymywania listy wyjątków (1).Jeśli zaciekawił Cię temat Event Sourcingu koniecznie zapoznaj się z naszymi materiałami na YouTube, a jeśli chcesz zobaczyć jak wyglądała ta edycja w praktyce lub wziąć udział w kolejnej edycji Consdata Tech to koniecznie dołącz do grupy Consdata Tech, gdzie opublikujemy informację o terminie kolejnego spotkania.",
"url": "/2018/12/13/podsumowanie-consdata-tech-odpowiedzi-na-pytanie.html",
"author": "Marcin Mergo",
"authorUrl": "/authors/mmergo.html",
"image": "mmergo.webp",
"highlight": "/assets/img/posts/2018-12-13-podsumowanie-consdata-tech-odpowiedzi-na-pytanie/consdatatech.jpg",
"date": "13-12-2018",
"path": "pl/2018-12-13-podsumowanie-consdata-tech-odpowiedzi-na-pytanie"
}
,


"2018-11-15-czy-apache-kafka-nadaje-sie-do-event-sourcingu-html": {
"title": "Czy Apache Kafka nadaje się do Event Sourcingu?",
"lang": "pl",
"tags": "event sourcing apache kafka",
"content": "Nietrudno jest natknąć się na głosy mówiące, że Apache Kafka nie nadaje się do implementacji wzorca, jakim jest Event Sourcing [1]. Czy jest tak w istocie? W artykule tym postaram się przedstawić swój punkt widzenia na tę sprawę.Aby móc debatować nad przydatnością (bądź nie) Apache Kafki do implementacji Event Sourcingu należy najpierw odpowiedzieć na pytanie - czym tak właściwie jest Event Sourcing? Z miejsca natrafiamy na problem, bowiem nie istnieje jedna, uniwersalna definicja Event Sourcingu. W zależności od źródła, natrafimy na różne definicje, nierzadko kładące nacisk na zupełnie odmienne aspekty tego modelu. W tej sytuacji najlepszym wyjściem będzie zwrócenie się do uznanych autorytetów, a konkretniej niezawodnego Martina Fowlera. W wolnym tłumaczeniu definiuje on Event Sourcing następująco:  Każda zmiana stanu systemu reprezentowana jest przez ciąg uszeregowanych eventów. [2]Z czym to się je?Jednak na pierwszy rzut oka definicja ta może wydawać się co najmniej enigmatyczna. Czym jest stan systemu? Czym są, i jak mają się do niego eventy?W tym ujęciu jako stan systemu należy rozumieć aktualną postać obiektów domenowych, utrwaloną w specyficzny dla danej aplikacji sposób - w pamięci, na dysku w postaci plików, w bazie danych. W praktyce stan systemu najczęściej przechowywany jest w bazie danych.Jak wobec tego eventy mają się do stanu? Weźmy na warsztat klasyczny przykład sklepu internetowego, w ramach którego istnieje obiekt zamówienia, na chwilę zapominając o Event Sourcingu.Standardowy przepływ w takim przykładzie mógłby wyglądać następująco:  klient rozpoczyna składanie zamówienia - na bazie danych tworzona jest krotka zamówienia - przy pomocy bezpośredniego bazodanowego inserta,  klient finalizuje zamówienie - aktualizujemy krotkę zamówienia na bazie, zmieniając status zamówienia - przy pomocy bazodanowego update’a,  klient porzuca zamówienie - usuwamy obiekt zamówienia z bazy - bezpośrednio na bazie wywołując delete’a.Jest to do bólu klasyczny przepływ, który można odnieść do większości systemów webowych. Jak jednak wyglądałoby to w świecie Event Sourcingu? Każda z powyższych akcji nie kończyłaby się bezpośrednim uderzeniem do bazy danych. Zamiast tego, generowany byłby odpowiedni event. A więc:  klient rozpoczyna składanie zamówienia - tworzony jest event mówiący, że klient utworzył zamówienie,  klient finalizuje zamówienie - tworzony jest event mówiący, że klient sfinalizował zamówienie,  klient porzuca zamówienie - tutaj analogicznie tworzymy event wyrażający akcję biznesową.Jak wobec tego, dysponując jedynie eventami, poznać aktualny stan obiektu zamówienia?Wyróżniamy tutaj dwa podejścia:  pierwsze z nich mówi, że chcąc poznać stan danego obiektu, należy zebrać wszystkie dotyczące go eventy i je zagregować - to znaczy przetworzyć po kolei, “nałożyć na siebie”, aż do uzyskania wynikowego stanu obiektu. Takie podejście w praktyce jest jednak często niepraktyczne, szczególnie w dużych systemach, przetwarzających dużą liczbę eventów;  w praktyce częściej wykorzystuje się rozwiązanie zwane state storage lub state store. Jest to odzwierciedlenie aktualnego stanu systemu, np. w formie bazy danych. Powyższe eventy byłyby w związku z tym “tłumaczone” na odpowiednie wywołania na bazie - odpowiednio insert, update oraz delete.Należy tutaj zwrócić uwagę na dwie bardzo ważne cechy eventów. Po pierwsze, eventy powinny być domenowe, a więc abstrahować od technicznych szczegółów wybranego przez nas state store’a. Stąd też eventy niosą za sobą informację domenową - “zamówienie zostało sfinalizowane”, a nie - “wykonaj w bazie danych update na polu status, ustawiając je na final”. Po drugie, eventy powinny być idempotentne, tj. niezależnie od tego ile razy dany event zostanie przetworzony (o ile nie zostanie utracona kolejność eventów) system powinien znajdywać się w tym samym stanie.Wracając do przykładu sklepu - po odzwierciedleniu eventów na bazie danych, będącej naszym state storem, otrzymujemy w wyniku identyczny stan bazy danych jak w przykładzie bez event sourcingu - obiekt zamówienia nie istnieje na bazie, ponieważ został usunięty. Na usta ciśnie się więc pytanie: jaki jest wobec tego sens Event Sourcingu? Po co zaprzątać sobie głowę kolejną abstrakcją i dokładać kolejne elementy do systemu, kiedy finalnie dochodzimy do identycznego stanu?Na szczęście, istnieje szereg zalet takiego podejścia:  ponowne przetworzenia strumienia zdarzeń - ponieważ każda zmiana stanu systemu reprezentowana jest przez event, możemy w bardzo prosty sposób ponownie ten strumień przetworzyć - kiedy chcemy wymienić state store (np. zamienić jedną bazę danych na inną), dołożyć nowy mechanizm raportowy do systemu, który musi od nowa przeanalizować aktywność naszej aplikacji, czy po prostu naprawić błędy lub rozbieżności, które wkradły się do state store’a. W ujęciu tym strumień eventów staje się źródłem prawdy w systemie,  łatwe odpytanie o stan każdego z obiektów w dowolnym momencie - nietrudno jest sobie wyobrazić sytuację, w której chcemy poznać stan jednego z obiektów w systemie w konkretnym punkcie czasu. Bez Event Sourcingu mamy do dyspozycji jedynie aktualny stan obiektu lub jesteśmy skazani na analizę logów w celu odtworzenia cyklu życia obiektu. Wykorzystując Event Sourcing staje się to proste i bezproblemowe,  możliwość zrzutu stanu z dowolnego momentu - chcąc przeanalizować aplikację w konkretnym momencie jej działania wystarczy przetworzyć eventy aż do tego momentu w czasie, “zrzucając” je do stojącej na boku bazy danych - tym samym, nie przeszkadzając w żaden sposób produkcyjnej bazie danych, można włączyć oraz zdebugować aplikację znajdującą się w perfekcyjnie tym samym stanie co np. w momencie wystąpienia błędu na produkcji,  wiele różnych odzwierciedleń stanu aplikacji - pewne reprezentacje stanu aplikacji są lepsze do konkretnych zastosowań niż inne. Np. chcąc łatwo przeszukiwać nasze obiekty pod kątem tekstu wybierzemy Solra lub Elastic Search. Jednak chcąc wykonywać klasyczne zapytania bazodanowe na naszym stanie, lepszy będzie PostgreSQL. Dzięki Event Sourcingowi łatwe staje się odzwierciedlanie stanu w różnych technologiach - sprowadza się to jedynie do “przetłumaczenia” eventów na odpowiednią technologię i ponownego przetworzenia strumienia zdarzeń,  prostsze debugowanie oraz audyt systemu - mając jawnie zdefiniowaną i zapisaną każdą zmianę każdego obiektu, debugowanie oraz audyt systemu stają się prostsze - prześledzenie zmian sprowadza się do prześledzenia eventów, bez konieczności odszyfrowywania nierzadko zawiłych logów aplikacji.Pozostawiając za nami przykład ze sklepem internetowym, świetną ilustracją Event Sourcingu oraz jego zalet są systemy kontroli wersji, np. git:  każda zmiana reprezentowana jest w postaci eventu (commit),  mamy możliwość powrotu do dowolnego punktu na strumieniu zdarzeń,  jeśli przydarzy nam się pomyłka w kodzie, możemy w każdej chwili powrócić do poprawnego stanu,  tworzenie wynikowego stanu (w postaci plików) jest niczym innym jak agregacją pojedynczych eventów,  debugowanie jest uproszczone, dzięki możliwości prześledzenia zmian każdej linijki kodu.Wejście KafkiPozostaje więc pytanie - gdzie przechowywać eventy? Jedną z możliwych opcji jest wykorzystanie Apache Kafki jako event store’a, a więc składnicy eventów.Czym jednak jest Kafka? U absolutnych podstaw można o Kafce myśleć jako o brokerze komunikatów - o funkcjonalności zbliżonej do ActiveMQ lub RabbitMQ. Po bliższym przyjrzeniu się tym technologiom okazuje się jednak, że Kafka jest brokerem o diametralnie innej filozofii działania niż te wcześniej wspomniane.Klasyczne systemy, jak ActiveMQ, dostarczając wiadomości do konsumentów wykorzystują model Smart Broker / Dumb Consumer. Oznacza to, że broker bierze na siebie ciężar obsługi dostarczania wiadomości do konsumentów - a więc wybiera i przesyła wiadomości do odpowiednich konsumentów, oczekuje na ACK, a w przypadku błędów zajmuje się obsługą DLQ. Kafka jednak wychodzi z dokładnie odwrotnego założenia, przyjmując filozofię Dumb Broker / Smart Consumer, przerzucając na konsumentów konieczność obsługi dostarczania wiadomości.Jak działa to w praktyce? Kiedy do Kafki wpływa nowy rekord, zostaje on umieszczony na topiku, gdzie otrzymuje swój offset - w najprostszym ujęciu jest to numer porządkowy danego rekordu na topiku. Kiedy konsument chce skonsumować jeden z rekordów musi, w uproszczeniu, wskazać konkretny offset tego rekordu - jest to więc zupełne odwrócenie klasycznego modelu, w którym to broker decyduje, który rekord należy dostarczyć do konsumenta. Takie odwrócenie kontroli jest niezwykle istotne z punktu widzenia Event Sourcingu, którego jedną z podstaw jest możliwość ponownego przetworzenia strumienia zdarzeń. Ponieważ to konsument wskazuje, który rekord chce odczytać, może ustawić się na początku strumienia i przetworzyć ten strumień od nowa. Jednak możliwości są zdecydowanie większe, na przykład konsument może rozpocząć przetwarzanie od połowy strumienia w dowolnym kierunku. Co więcej, z punktu widzenia Kafki pobranie dowolnego rekordu, niekoniecznie w naturalnej kolejności, jest bardzo tanią operacją. Jest to pierwsza duża zaleta Kafki w kontekście Event Sourcingu - natywne wsparcie dla ponownego przetwarzania strumienia zdarzeń.Kolejną zaletą jest obsługa pokaźnych wolumenów danych. Duże systemy mogą generować miliardy eventów. Nawet jeśli rozmiar pojedynczych eventów jest bardzo mały, to efekt skali powoduje, że ich sumaryczna wielkość może swobodnie przekraczać terabajty. Również tutaj Kafka nie zawodzi, obsługując liniowo zarówno wolumen danych o wielkości 50 KB jak i 50 TB [3]. Jest to kolejna duża zaleta bezpośrednio związana z Event Sourcingiem - Kafka świetnie sprawdza się w charakterze event store’a.Na tym zalety Kafki się nie kończą, a jedną z największych jest ujednolicenie modeli kolejek oraz publisher-subscriber. Niestety, jest to już poza zakresem niniejszego artykułu.ZarzutyJak wobec tego prezentują się zarzuty wobec Kafki w kontekście Event Sourcingu?Pierwszy z nich dotyczy trudności z odczytem eventów dla konkretnego obiektu. Postuluje on konieczność przetworzenia całego strumienia zdarzeń celem odnalezienia eventów dotyczących interesującej nas instancji obiektu [1]. Jest to jednak błędny argument - z co najmniej kilku powodów. Po pierwsze, w dobie rozwiązań takich jak KSQL, odczytanie konkretnych eventów jest jak najbardziej możliwe [4]. Po drugie, argument ten rozmija się z faktyczną ideą Event Sourcingu, którego podstawą nie jest sposób wyznaczania wynikowego stanu, a sam fakt odzwierciedlania wszystkich zmian stanu w postaci eventów. Kwestia agregacji eventów do wynikowego (lub pośredniego) stanu jest już drugorzędna, w sporej mierze zależy od konkretnego rozwiązania i jako taka nie definiuje Event Sourcingu.Kolejne zarzuty dotyczą spójności zapisu [1] - kiedy po odczytaniu stanu konkretnego obiektu w wyniku logiki biznesowej generujemy kolejny event zmieniający stan tego obiektu, może dojść do sytuacji, w której inny wątek w międzyczasie nadpisze lub usunie obiekt, na którym działamy. Zarzut dotyczy w praktyce braku blokad na obiektach, znanych z klasycznych baz danych. W ogólności rozwiązaniem tego problemu jest zapewnienie jednowątkowych zapisów, jednak autor przywołanego artykułu sugeruje, że będzie to miało drastyczny wpływ na wydajność zapisu. I tutaj Kafka przychodzi nam z pomocą - topiki na Kafce mogą zostać podzielone na partycje, co w połączeniu z łatwym umieszczaniem eventów dotyczących pojedynczego obiektu na jednej partycji, pozwala bezproblemowo zrównoleglić przetwarzanie, nie naruszając zasady jednowątkowego zapisu - tym samym obalając niniejszy zarzut.Podsumowując, nie mam wątpliwości, że Kafka nadaje się do implementacji Event Sourcingu. Z jednej strony, dzięki natywnemu wsparciu dla ponownego przetwarzania eventów oraz radzeniu sobie z dużym wolumenem danych, jest to naturalny wybór jako event store. Prostą pochodną tych dwóch właściwości są pozostałe zalety Event Sourcingu - zrzuty stanu z dowolnego momentu, łatwe odzwierciedlanie stanu w innej technologii, czy też prostsze debugowanie. Z drugiej strony, dzięki bardziej zaawansowanym możliwościom przetwarzania eventów - jak streamy, KSQL, czy zrównoleglanie przetwarzania topików - Kafka staje się świetną podstawą do implementacji szeroko rozumianego Event Sourcingu.Na zakończenie należy jeszcze zauważyć, że Event Sourcing jest wzorcem technologicznie agnostycznym - nieprzywiązanym do żadnej konkretnej technologii. Istnieją rozwiązania, jak Apache Kafka, które znacząco ułatwiają implementację Event Sourcingu, jednak nic nie stoi na przeszkodzie, żeby zaimplementować go w oparciu o MySQL, MongoDB lub jeszcze inną technologię pozwalającą na przechowywanie uszeregowanych eventów. Ostatecznie idea Event Sourcingu jest prosta: każda zmiana stanu systemu reprezentowana jest przez ciąg uszeregowanych eventów - a technologia jest już sprawą drugorzędną.Jeśli chcecie dowiedzieć się więcej na temat Event Sourcingu, zapraszamy na pierwszą edycję Consdata Tech! Więcej informacji na temat całego wydarzenia znajdziecie pod tym linkiem: https://consdata.tech/. Co ważne, podczas tego meetupu będzie streaming, na którym warto być: http://consdata.tech/streaming.Źródła:  [1] https://medium.com/serialized-io/apache-kafka-is-not-for-event-sourcing-81735c3cf5c  [2] https://martinfowler.com/eaaDev/EventSourcing.html  [3] https://kafka.apache.org/intro  [4] https://www.confluent.io/product/ksql/",
"url": "/2018/11/15/czy-apache-kafka-nadaje-sie-do-event-sourcingu.html",
"author": "Marcin Mergo",
"authorUrl": "/authors/mmergo.html",
"image": "mmergo.webp",
"highlight": "/assets/img/posts/2018-11-15-czy-apache-kafka-nadaje-sie-do-event-sourcingu/kafka-apache.png",
"date": "15-11-2018",
"path": "pl/2018-11-15-czy-apache-kafka-nadaje-sie-do-event-sourcingu"
}
,


"2018-08-07-algorytmy-rekomendacyjne-przyklad-implementacji-w-pythonie-html": {
"title": "Algorytmy rekomendacyjne - przykład implementacji w Pythonie",
"lang": "pl",
"tags": "algorytmy rekomendacyjne python",
"content": "Gdy robimy zakupy w internecie, często zdarza się, że przez kolejne dni pokazują nam się propozycje produktów, podobnych do tych, które wcześniej oglądaliśmy. Atrakcyjne ceny powodują, iż czasem ulegamy presji reklamy, decydujemy się na zapoznanie się z ofertą i…dokonujemy zakupu. Jak to się dzieje, że przeglądarka wie, co chcemy kupić, zobaczyć czy posłuchać? Za to wszystko odpowiadają systemy rekomendacyjne.Systemy rekomendacyjne widzimy wszędzie tam, gdzie użytkownik ma styczność z ogromnymi katalogami danych, np. Amazon podpowiada nam, jakie produkty powinniśmy kupić, Netflix - jakie filmy oglądać, a Spotify - które utwory na pewno nam się spodobają. Wykorzystywane są przez coraz większą ilość usług, a ich popularność stale rośnie. Odpowiada za to koncepcja “Long Tail”.W 1988 roku brytyjski alpinista Joe Simpson napisał książkę pod tytułem „Touching the Void”, w której opisał swoje zmagania w peruwiańskich Alpach. Publikacja cieszyła się zainteresowaniem, wkrótce jednak zostałaby zapomniana, gdyby nie Jon Krakauer, który dekadę później napisał „Into Thin Air”, czyli kolejną pozycję o wspinaczkowych starciach. Po jej wydaniu książka Joe Simspona nagle wróciła do sprzedaży, nastąpiły dodruki, aby nadążyć za popytem. Co więcej, przez czternaście tygodni znajdowała się na liście bestsellerów NewYork Times i sprzedano jej dwukrotnie więcej niż dzieło Krakauera! Dlaczego tak się stało? Otóż Amazon zarekomendował książkę „Touching the Void” podczas zakupu „Into Thin Air”, jako książkę, która również może spodobać się kupującemu. Teoria Long Tail sugeruje, że na rynku internetowym sumaryczny dochód ze sprzedaży pojedynczych towarów niszowych, może generować wyższe zyski, niż oferowane masowo produkty popularne, tzw. bestsellery.Najciekawszym rodzajem rekomendacji są te bazujące na danych o konkretnych użytkownikach. Możemy tu wyróżnić dwa podejścia:  Content Based Filtering, który opiera się na cechach, typach produktów, szukając podobnych do tych, które użytkownik już ocenił pozytywnie lub kupił,  Collaborative Filtering, czyli rekomendacje na podstawie ocen i zakupów użytkownika. Gdy dwóch użytkowników kupuje podobne produkty, możemy stwierdzić, że mają zbliżone upodobania i polecać im wzajemnie sprawdzone dla nich artykuły.Collaborative Filtering – przykład implementacjiCollaborative Filtering dzieli się na jeszcze dwa rodzaje:  Item-based, który bazując tylko na ocenach, wyszukuje podobne przedmioty i je rekomenduje,  User-based, którego przykład implementacji szerzej omówimy.Zasada jest prosta. Wyobraźmy sobie, że mamy użytkownika X. Dla tego użytkownika znajdujemy grupę innych użytkowników, którzy są do siebie podobni, np. zgodnie oceniają te same filmy. Jest to tzw. sąsiedztwo użytkownika X. W tej grupie znajdujemy zbiór filmów, które również są przez nią wysoko oceniane, a użytkownik X ich nie oglądał i rekomendujemy mu je. Dane, z których skorzystałam, pochodzą ze strony MovieLens. W zbiorze danych znajduje się m.in. 100.000 ocen użytkowników w skali 1-5.import pandas as pdratings_columns = [&#39;user_id&#39;, &#39;movie_id&#39;, &#39;rating&#39;, &#39;timestamp&#39;]ratings = pd.read_csv(&#39;u.data&#39;, sep=&#39;\\t&#39;, names=ratings_columns, encoding=&#39;latin-1&#39;)ratings.drop( &quot;timestamp&quot;, inplace = True, axis = 1 )print ratings.shaperatings.head(10)Na podstawie tej tabelki utworzymy macierz ocen filmów userId x movieId.user_movies = ratings.pivot( index=&#39;user_id&#39;, columns=&#39;movie_id&#39;, values = &quot;rating&quot; ).reset_index(drop=True)user_movies.fillna( 0, inplace = True )user_movies=pd.DataFrame(user_movies)print user_movies.shapeuser_movies.head()Kolejnym krokiem jest wyznaczenie n-tego sąsiedztwa użytkownika X, czyli grupy najbardziej podobnych mu osób. Aby to zrobić, potrzebujemy miary podobieństwa między użytkownikami, których istnieje mnogość, m.in. miarę cosinusową, współczynnik Jaccarda. Ja użyję współczynnika korelacji Pearsona. Miara ta mieści się w przedziale zamkniętym [-1, 1], gdzie 1 będzie oznaczała stuprocentowe podobieństwo.Nie pokuszę się jednak o implementację tego wzoru, a wykorzystam gotową funkcję z biblioteki SciPy.from sklearn.metrics.pairwise import pairwise_distancesusers_similarity = 1 - pairwise_distances( user_movies.as_matrix(), metric=&quot;correlation&quot; )users_similarity_df = pd.DataFrame( users_similarity )print users_similarity.shapeusers_similarity_df.head()Otrzymaliśmy macierz userId x userId z wyliczonymi wartościami podobieństwa między każdym użytkownikiem. Możemy też z niej zauważyć, że użytkownicy są najbardziej podobni do… samych siebie (wartość 1 po przekątnej macierzy). Do dalszych obliczeń wypełnilibyśmy przekątną macierzy zerami, aby nie zakłamywać wyników. Tak to wygląda krok po kroku. Tak naprawdę Python ułatwia nam obliczenia jeszcze bardziej, oferując metodę NearestNeighbors z biblioteki sklearn, w której to możemy podać np. wielkość szukanego sąsiedztwa, czy metodę obliczania podobieństwa.from sklearn.neighbors import NearestNeighborsdef find_neighborhood(user_id, n):    model_knn = NearestNeighbors(metric = &quot;correlation&quot;, algorithm = &quot;brute&quot;)    model_knn.fit(user_movies)    distances, indices = model_knn.kneighbors(user_movies.iloc[user_id-1, :].values.reshape(1, -1), n_neighbors = n+1)    similarities = 1-distances.flatten()    print &#39;{0} most similar users for user with id {1}:\\n&#39;.format(n, user_id)    for i in range(0, len(indices.flatten())):        # pomiń, jeśli ten sam użytkownik        if indices.flatten()[i]+1 == user_id:            continue;        else:            print &#39;{0}: User {1}, with similarity of {2}&#39;.format(i, indices.flatten()[i]+1, similarities.flatten()[i])    return similarities,indicesPowyższa funkcja posłuży nam do wylistowania sąsiedztwa danego użytkownika. Przyjmuje ona id użytkownika, którego sąsiedztwa szukamy oraz jego rozmiar. Wykorzystując współczynnik korelacji Pearsona, na podstawie naszej pierwszej tabelki z ocenami użytkowników otrzymamy odległości między użytkownikiem „wejściowym” a każdym pozostałym oraz odpowiadające im id userów. Następnie w pętli możemy wylistować sąsiedztwo użytkownika, pomijając przy tym jego samego.Następnym krokiem będzie przywidywanie ocen użytkownika.import numpy as npdef predict_rate(user_id, item_id, n):    similarities, indices=find_neighborhood(user_id, n)    neighborhood_ratings =[]    for i in range(0, len(indices.flatten())):        if indices.flatten()[i]+1 == user_id:            continue;        else:            neighborhood_ratings.append(user_movies.iloc[indices.flatten()[i],item_id-1])    weights = np.delete(indices.flatten(), 0) #delete weight for input user    prediction = round((neighborhood_ratings * weights).sum() / weights.sum())    print &#39;\\nPredicted rating for user {0} -&amp;gt; item {1}: {2}&#39;.format(user_id,item_id,prediction)Za pomocą funkcji find_neighborhood znajdziemy dla niego sąsiedztwo n użytkowników, którzy ocenili film, którego ocenę będziemy przewidywać. Z taką wiedzą, w pętli tworzymy listę ocen danego filmu przez podobnych mu użytkowników. Jak teraz obliczyć prawdopodobną ocenę? Najprostszym z rozwiązań jest policzenie średniej ocen całego sąsiedztwa użytkownika dla tego filmu i zaprezentować to jako nasze przewidywanie. Niestety to rozwiązanie ignoruje wartość podobieństwa między użytkownikami, więc lepszym podejściem policzenie średniej ważonej. Jako wagi przyjmiemy „odległości” między użytkownikami, które zwróciła nam funkcja w zmiennej indices. Otrzymując spodziewany oceny użytkowników dla danych filmów, możemy zarekomendować mu listę filmów, które będą bliskie jego upodobaniom.PodsumowaniePowyższy przykład implementacji jest oczywiście bardzo podstawowym. Duże portale korzystają raczej z hybrydowych rozwiązań, próbując jak najbardziej trafnie oszacować nasze upodobania. Ich metody rekomendacji bazują na tak dużych danych i są tak czasochłonne, że rekomendacje nie są obliczane na bieżąco, po każdej ocenie użytkownika, czy po każdym zakupie, a są uaktualniane raz na kilka dni.",
"url": "/2018/08/07/algorytmy-rekomendacyjne-przyklad-implementacji-w-pythonie.html",
"author": "Sylwia Üçüncü",
"authorUrl": "/authors/sucuncu.html",
"image": "sucuncu.jpg",
"highlight": "/assets/img/posts/2018-08-07-algorytmy-rekomendacyjne-przyklad-implementacji-w-pythonie/algorytmy-rekomendacyjne.jpg",
"date": "07-08-2018",
"path": "pl/2018-08-07-algorytmy-rekomendacyjne-przyklad-implementacji-w-pythonie"
}
,


"2018-04-23-practical-applications-of-webhooks-html": {
"title": "Praktyczne zastosowanie webhook",
"lang": "pl",
"tags": "sonarqube webhook",
"content": "Każdy programista prędzej czy później ma do czynienia z jakąś formą API (ang. application programming interface). API to określony interfejs, którym dwie niezależne aplikacje mogą porozumiewać się między sobą. W tym artykule chciałbym przedstawić Wam nieco inne podejść do takiej komunikacji między aplikacjami.Czym są webhooki?Przyjmijmy, że napisaliśmy aplikację A. Chcielibyśmy także, aby wyświetlała ona pewne dane z aplikacji B - niech będzie to lista aktywnych użytkowników. W tym celu aplikacja B udostępnia nam API pozwalające na pobranie listy aktywnych użytkowników. W konwencjonalnym podejściu musielibyśmy odpytywać aplikację B o aktualną listę aktywnych użytkowników. Znacznie lepiej byłoby, gdyby to aplikacja B poinformowała naszą aplikację o zmianie na liście aktywnych użytkowników. W tym miejscu z pomocą przychodzą nam webhooki.Webhook - jest to nieco inne podejście do komunikacji między aplikacjami. Mechanizm webhooków jest często nazywany Reverse API, ponieważ zwykle nie wymaga interakcji ze strony klienckiej (w naszym przykładzie aplikacji A). Prościej rzecz ujmując, webhooki pozwalają na powiadomienie aplikacji klienckiej o wystąpieniu pewnych zdarzeń.Od strony implementacyjnej, webhooki są nieco prostsze, gdyż polegają de facto na wysłaniu request po zadanej strukturze, pod zadany adres, w wyniku wystąpienia jakiegoś zdarzenia. Integracja aplikacji klienckiej polega na wskazaniu adresu, na który ma zostać wysłany request.Podsumowując, webhooki są bardzo przydatne w sytuacjach, gdy oczekujemy od aplikacji zewnętrznej informacji w wystąpieniu zdarzenia, gdyż pozwalają nam na uniknięcie aktywnego czekania po stronie naszej aplikacji.Jeśli chodzi o wady webhooków, można wyróżnić dwie:  w momencie wystąpienia błędu w aplikacji możemy stracić zewnętrzne dane, gdyż nie mamy gwarancji, że zewnętrzna aplikacja w jakikolwiek sposób zareaguje na zgłoszony przez nas błąd - w przypadku klasycznego API moglibyśmy ponownie odpytać aplikację zewnętrzną,  przy obsłudze webhooków musimy wziąć po uwagę, iż zdarzenia, o których jesteśmy powiadamiani, mogą występować bardzo często - zwykle nie mamy nad tym kontroli.Przykład integracjiZintegrujemy SonarQube Compoaniona z komunikatorem Slack. Efektem integracji będzie wiadomość na kanale w komunikatorze z informacją, jak w ciągu ostatniego dnia zmieniła się liczba naruszeń zespołu.Wymagania wstępne:  zapoznanie się z artykułem o SonarQube Companionie,  skonfigurowany SonqrQube Companion,  komunikator Slack.Na początek musimy skonfigurować webhooka po stronie komunikatora. Aby to zrobić, przechodzimy do ustawień integracji Slack’a. Dodajemy konfigurację aplikacji „Incoming WebHooks” i definiujemy, na jaki kanał chcemy wysyłać wiadomości:Po pomyślnej konfiguracji, w widoku wybranego kanału, powinno ukazać nam się powiadomienie o pomyślnej konfiguracji:Kolejnym krokiem będzie skonfigurowanie webhooka po stronie SonarQube Companiona.W SonarQube Companionie mamy nieco bardziej rozbudowany model definiowania webhooków. Każda ze zdefiniowanych grup może zawierać swoją własną definicję webhooków. Definicja każdego webhooka składa się z trzech podstawowych elementów:  action – akcje to określone zdarzenie / zachowanie, jakie ma zostać wykonane,  trigger – triggery definiują, kiedy akcja ma zostać wykonana,  callback – w jakiej formie aplikacja kliencka ma zostać poinformowana o wynikach akcji.Pełna dokumentacja dostępnych akcji, triggerów i callbacków znajduje się na: https://github.com/Consdata/sonarqube-companion/wiki/Webhooks.W ramach przykładu, zdefiniujemy webhooka, który sprawdzi, jak zmieniła się liczba naruszeń w obrębie grupy, w ciągu ostatniego dnia. W zależności od wyniku, wyślemy stosowny komunikat na kanał komunikatora.Na początku definiujemy, w wyniku jakiej akcji mamy wysłać wiadomość. Aby to zrobić, w węźle webhooks definiujemy akcję dla nowego webhooka:&quot;webhooks&quot;: [{    &quot;action&quot;: {    &quot;type&quot;: &quot;NO_IMPROVEMENT&quot;,    &quot;period&quot;: &quot;DAILY&quot;,    &quot;severity&quot;: [&quot;blockers&quot;, &quot;criticals&quot;, &quot;majors&quot;]    }}]Zdefiniowaliśmy w ten sposób akcję, która sprawdzi, czy w przeciągu ostantiego dnia poprawie uległa ilość naruszeń w obrępie projektów grupy. Dodatkowo, pod uwagę weźmie tylko naruszenia o priorytetach: blocker, critical oraz major.Kolejnym krokiem będzie zdefiniowanie triggera akcji. W naszym przykładzie, dla ułatwienia, chcielibyśmy, aby akcja wykonywana była co minutę. Definiujemy trigger typu CRON:&quot;trigger&quot;: {    &quot;type&quot;: &quot;CRON&quot;,    &quot;definition&quot;: &quot;0 */1 * * * *&quot;},Na koniec definicja faktycznej integracji z komunikatorem. W tym celu definiujemy callback typu POST, w definicji którego podajemy URL pozyskany z panelu konfiguracyjnego Slack’a w polu “Webhook URL”:&quot;callbacks&quot;: [    {        &quot;type&quot;: &quot;POST&quot;,        &quot;url&quot;: &quot;https://hooks.slack.com/services/*/*&quot;,        &quot;body&quot; : {            &quot;no_improvement&quot;: &quot;{ &#39;text&#39;: &#39;http://gph.is/1RFg2r3 Brak poprawy&#39;}&quot;,            &quot;improvement&quot;: &quot;{ &#39;text&#39;: &#39;http://gph.is/1a7RlDR Poprawiono ${diff}&#39;&quot;,            &quot;clean&quot;: &quot;{ &#39;text&#39;: &#39;https://gph.is/1IH3RW6 Czysto&#39;}&quot;        }    }]W sekcji body definiujemy treść wiadomości, jaka ma zostać wysłana w zależności od stanu grupy. Przykładowo, w przypadku, gdy stan naruszeń uległ poprawie (odpowiedź “improvement”), wyślemy gifa wraz z krótkim komentarzem zawierającym predefiniowaną zmienną akcji - ${diff}. W ten sposób, na kanale pojawi się informacja o liczbie poprawionych naruszeń w ciągu ostatniego dnia.Całość konfiguracji przedstawia się następująco:&quot;webhooks&quot;: [    {        &quot;action&quot;: {        &quot;type&quot;: &quot;NO_IMPROVEMENT&quot;,        &quot;period&quot;: &quot;DAILY&quot;,        &quot;severity&quot;: [&quot;blockers&quot;, &quot;criticals&quot;, &quot;majors&quot;]        },        &quot;trigger&quot;: {            &quot;type&quot;: &quot;CRON&quot;,            &quot;definition&quot;: &quot;0 */1 * * * *&quot;        },        &quot;callbacks&quot;: [          {            &quot;type&quot;: &quot;POST&quot;,            &quot;url&quot;: &quot;https://hooks.slack.com/services/*/*&quot;,            &quot;body&quot; : {                &quot;no_improvement&quot;: &quot;{ &#39;text&#39;: &#39;http://gph.is/1RFg2r3 Brak poprawy&#39;}&quot;,                &quot;improvement&quot;: &quot;{ &#39;text&#39;: &#39;http://gph.is/1a7RlDR Poprawiono ${diff}&#39;&quot;,                &quot;clean&quot;: &quot;{ &#39;text&#39;: &#39;https://gph.is/1IH3RW6 Czysto&#39;}&quot;            }          }        ]    }]W ten sposób, co minutę SonarQube Companion sprawdzi stan naruszeń grupy i w zależności od wyniku, wyśle określoną wiadomość na kanał komunikatora.W rezultacie otrzymujemy wiadomość na kanale o braku poprawy jakichkolwiek naruszeń:Po poprawie jednego naruszenia, w kolejnej minucie otrzymujemy stosowne powiadomienie:W ten sposób zintegrowaliśmy dwie, niezależne aplikacje za pomocą mechanizmu webhooków, bez potrzeby pisania choćby linijki kodu.PodsumowanieW artykule przedstawiłem koncepcję webhooków oraz pokazałem Wam, jak w prosty i szybki sposób można dzięki nim zintegrować dwie niezależne aplikacje. Zamierzamy stopniowo rozszerzać SonarQube Companiona o nowe akcje, np. przesyłanie cotygodniowego raportu o naruszeniach, poszczególnym użytkownikom - dlatego zalecam regularne odwiedzanie strony projektu :)Przydatne linki:  https://github.com/Consdata/sonarqube-companion  https://github.com/Consdata/sonarqube-companion/wiki/Webhooks",
"url": "/2018/04/23/practical-applications-of-webhooks.html",
"author": "Bartosz Radliński",
"authorUrl": "/authors/bradlinski.html",
"image": "bradlinski.webp",
"highlight": "/assets/img/posts/2018-04-23-praktyczne-zastosowanie-webhook/webhook.png",
"date": "23-04-2018",
"path": "pl/2018-04-23-practical-applications-of-webhooks"
}
,


"2018-03-26-terraform-czyli-o-tym-jak-okielznac-chmure-od-amazona-html": {
"title": "Terraform - czyli o tym, jak okiełznać chmurę od Amazona",
"lang": "pl",
"tags": "terraform aws",
"content": "“Get your clouds right.” Dwight Schrute, The OfficeNie tak dawno temu, Jakub Wilczewski opublikował wyczerpujący wstęp do programowania w AWS. Jakub użył webowej konsoli AWS do definiowania zasobów. Jest to metoda, która sprawdza się w przypadku prezentacji oraz nauki. Jednak w przypadku regularnej pracy z kodem, potrzebujemy narzędzi, które umożliwią nam zautomatyzowanie procesu tworzenia a także niszczenia zasobów AWS. Wykorzystam zaproponowaną przez niego aplikację, aby pokazać, jak można to zrobić, korzystając z rozwiązania Terraform od firmy Hashicorp.Konfiguracja i pierwsza terraformacja chmuryFirmę Hashicorp powinni kojarzyć wszyscy, którzy mieli do czynienia z rozwiązaniami chmurowymi. Stoi ona za takimi rozwiązaniami jak Vault, Consul czy Vagrant. Terraform jest ich odpowiedzią na jeden z paradygmatów kultury devops, jakim jest “infrastructure as code”.Zanim przystąpimy do terraformacji chmury AWS potrzebne będą dwie binarki:  https://aws.amazon.com/cli/  https://www.terraform.io/downloads.htmlAWS CLI jest co prawda opcjonalny, ale pozwala uwolnić skrypty Terraform od danych logowania użytkownika. Żeby to osiągnąć, wystarczy jednorazowo zalogować się do konsoli AWS, wydając polecenie:$ aws configureAWS Access Key ID [None]: xxxAWS Secret Access Key [None]: xxxDefault region name [None]: eu-central-1Default output format [None]:Jedną z najbardziej elementarnych czynności związanych z pracą z chmurą Amazonu jest utworzenie nowej “wirtualki”, czyli instancji usługi EC2. Oto minimalny skrypt Terraform, który tę czynność automatyzuje:provider &quot;aws&quot; {  version = &quot;~&amp;gt; 0.1&quot;  region = &quot;eu-central-1&quot;}resource &quot;aws_instance&quot; &quot;step0&quot; {  ami = &quot;ami-13b8337c&quot;  instance_type = &quot;t2.micro&quot;}Pierwsza sekcja wskazuje dostawcę usług, druga jest dyrektywą uruchomienia instancji EC2 z obrazu o identyfikatorze ami-13b8337c. Numer taki można podejrzeć w konsoli webowej AWS albo u autora konkretnej dystrybucji systemu operacyjnego, na przykład Ubuntu.W celu uruchomienia procesu tworzenia zasobów, należy wykonać sekwencje poleceń$ terraform init$ terraform plan$ terraform applyPierwsze polecenie uruchamiamy raz w katalogu ze skryptem i spowoduje ono pobranie bibliotek koniecznych do komunikacji z dostawcą usług. Drugie polecenie wydrukuje na wyjściu plan wykonania skryptu, który zostanie uruchomiony trzecim poleceniem. Po kilkunastu sekundach, instancja powinna być uruchomiona i gotowa do pracy. Na zakończenie polecenie, które jest wisienką na torcie, czyli wycofanie wszystkich zmian wprowadzonych w poprzednim kroku.$ terraform destroyW tym miejscu warto się na chwilę zatrzymać i wyjaśnić, w jaki sposób Terraform śledzi stan zmian. Po wykonaniu polecenia ‘apply’, w katalogu bieżącym powstanie plik terraform.tfstate oraz jego kopia terraform.tfstate.backup. Plik ten jest zapisem stanu środowiska i od tego momentu możliwe jest wprowadzanie zmian wyłącznie za pomocą Terraforma. Jeśli wprowadzimy zmiany z konsoli AWS, to Terraform nie będzie ich śledził. Nie ma żadnego mechanizmu odpytywania aktualnego stanu chmury. Warto o tym pamiętać, pracując z Terraformem.Skrypt dla kompletnej aplikacjiPo tym krótkim wstępie mamy niezbędną wiedzę pozwalającą nam rozpocząć pracę z automatyzacją konfiguracji usług w chmurze. Na stronach HashiCorp dokumentujących Terraform, można znaleźć wyczerpujący opis wszystkich dyrektyw.Teraz możemy przejść do opisu automatyzacji przykładowej aplikacji opisanej przez Jakuba. Zdaję sobie sprawę, że niektóre elementy mogą być dość skomplikowane na pierwszy rzut oka. Musicie mi jednak uwierzyć na słowo, że jak tylko zaczniecie korzystać z tego sposobu konfiguracji, nigdy nie wrócicie już do ręcznego wyklikiwania z konsoli AWS. Postaram się zachować kolejność, w której Jakub dodawał zasoby za pomocą konsoli. Zrezygnuję jednak z rozbicia na poszczególne przepływy, czyli odczyt i zapis notatek będę realizował równocześnie. Zaczynamy zatem od kodu źródłowego, który umieszczamy w plikach note-find-lambda.js i note-add-lambda.js. Przygotuję sobie także pomocniczny skrypt w Bash, który spakuje kod do archiwum:#!/usr/bin/env bashrm *.zipzip -R note-add-lambda.zip ./note-add-lambda.jszip -R note-find-lambda.zip ./note-find-lambda.jsZaczynamy od zdefiniowania dostawcy, tabeli DynamoDB oraz dwóch Lambd:provider &quot;aws&quot; {  version = &quot;~&amp;gt; 0.1&quot;  region = &quot;eu-central-1&quot;}resource &quot;aws_dynamodb_table&quot; &quot;notes-table&quot; {  name = &quot;notes&quot;  read_capacity = 1  write_capacity = 1  hash_key = &quot;userName&quot;  range_key = &quot;timestamp&quot;  attribute {    name = &quot;userName&quot;    type = &quot;S&quot;  }  attribute {    name = &quot;timestamp&quot;    type = &quot;N&quot;  }}resource &quot;aws_lambda_function&quot; &quot;note-add-lambda&quot; {  filename = &quot;note-add-lambda.zip&quot;  function_name = &quot;note-add-lambda&quot;  role = &quot;${aws_iam_role.iam_for_lambda.arn}&quot;  handler = &quot;note-add-lambda.handler&quot;  source_code_hash = &quot;${base64sha256(file(&quot;note-add-lambda.zip&quot;))}&quot;  runtime = &quot;nodejs6.10&quot;  timeout = &quot;10&quot;  memory_size = &quot;256&quot;}resource &quot;aws_lambda_function&quot; &quot;note-find-lambda&quot; {  filename = &quot;note-find-lambda.zip&quot;  function_name = &quot;note-find-lambda&quot;  role = &quot;${aws_iam_role.iam_for_lambda.arn}&quot;  handler = &quot;note-find-lambda.handler&quot;  source_code_hash = &quot;${base64sha256(file(&quot;note-find-lambda.zip&quot;))}&quot;  runtime = &quot;nodejs6.10&quot;  timeout = &quot;10&quot;  memory_size = &quot;256&quot;}Następnie musimy wprost zdefiniować coś, co w przypadku tworzenia z poziomu konsoli web zadziało się “automagicznie”, czyli uprawnienie dla Lambd do korzystania z tabeli w DynamoDB. Możemy zrobić to w następujący sposób.resource &quot;aws_iam_role&quot; &quot;iam_for_lambda&quot; {  name = &quot;iam_for_lambda&quot;  assume_role_policy = EOF{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [    {      &quot;Action&quot;: &quot;sts:AssumeRole&quot;,      &quot;Principal&quot;: {        &quot;Service&quot;: &quot;lambda.amazonaws.com&quot;      },      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Sid&quot;: &quot;&quot;    }  ]}EOF}resource &quot;aws_iam_role_policy&quot; &quot;role_policy_for_dynamodb_access&quot; {  name = &quot;role_policy_for_dynamodb_access&quot;  role = &quot;${aws_iam_role.iam_for_lambda.id}&quot;  policy = EOF{    &quot;Version&quot;: &quot;2012-10-17&quot;,    &quot;Statement&quot;: [        {            &quot;Sid&quot;: &quot;AccessAllNotes&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: [                &quot;dynamodb:*&quot;            ],            &quot;Resource&quot;: [                &quot;*&quot;            ]        }    ]}EOF}Na tym etapie skrypt jest gotowy do uruchomienia i testowania z poziomu samych Lambd. Kolejnym elementem będzie wystawienie usług na świat, czyli wygenerowanie całej otoczki związanej z usługą API Gateway. Elementów jest niemało, ale wszystkie one składają się na elegancki opis API wystawionego w przypadku prostej aplikacji zarządzającej notatkami.W pierwszej kolejności tworzymy elementy ścieżki URI://COMMON APIresource &quot;aws_api_gateway_rest_api&quot; &quot;NotesAPI&quot; {  name = &quot;NotesAPI&quot;}resource &quot;aws_api_gateway_resource&quot; &quot;notes-resource&quot; {  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  parent_id = &quot;${aws_api_gateway_rest_api.NotesAPI.root_resource_id}&quot;  path_part = &quot;notes&quot;}resource &quot;aws_api_gateway_resource&quot; &quot;notes-userName-resource&quot; {  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  parent_id = &quot;${aws_api_gateway_resource.notes-resource.id}&quot;  path_part = &quot;{userName}&quot;}Teraz kolej na usługę GET oraz jej integrację z note-find-lambda://GET IMPLEMENTATIONresource &quot;aws_api_gateway_method&quot; &quot;notes-get-method&quot; {  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  resource_id = &quot;${aws_api_gateway_resource.notes-userName-resource.id}&quot;  http_method = &quot;GET&quot;  authorization = &quot;NONE&quot;  api_key_required = true}resource &quot;aws_api_gateway_method_response&quot; &quot;notes-get-method-response-ok-200&quot; {  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  resource_id = &quot;${aws_api_gateway_resource.notes-userName-resource.id}&quot;  http_method = &quot;${aws_api_gateway_method.notes-get-method.http_method}&quot;  status_code = &quot;200&quot;}resource &quot;aws_api_gateway_method_response&quot; &quot;notes-get-method-response-not-found-error-404&quot; {  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  resource_id = &quot;${aws_api_gateway_resource.notes-userName-resource.id}&quot;  http_method = &quot;${aws_api_gateway_method.notes-get-method.http_method}&quot;  status_code = &quot;404&quot;}resource &quot;aws_api_gateway_integration&quot; &quot;notes-get-integration&quot; {  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  resource_id = &quot;${aws_api_gateway_resource.notes-userName-resource.id}&quot;  http_method = &quot;${aws_api_gateway_method.notes-get-method.http_method}&quot;  integration_http_method = &quot;POST&quot;  type = &quot;AWS&quot;  uri = &quot;arn:aws:apigateway:eu-central-1:lambda:path/2015-03-31/functions/${aws_lambda_function.note-find-lambda.arn}/invocations&quot;  request_templates {    &quot;application/json&quot; = EOF    {       &quot;userName&quot;: &quot;$input.params(&#39;userName&#39;)&quot;    }    EOF  }}resource &quot;aws_api_gateway_integration_response&quot; &quot;notes-get-integration-response&quot; {  depends_on = [    &quot;aws_api_gateway_integration.notes-get-integration&quot;]  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  resource_id = &quot;${aws_api_gateway_resource.notes-userName-resource.id}&quot;  http_method = &quot;${aws_api_gateway_method.notes-get-method.http_method}&quot;  status_code = &quot;${aws_api_gateway_method_response.notes-get-method-response-ok-200.status_code}&quot;  response_templates {    &quot;application/json&quot; = EOF#set($inputRoot = $input.path(&#39;$&#39;))$inputRoot.ItemsEOF  }}resource &quot;aws_api_gateway_integration_response&quot; &quot;notes-get-integration-response-user-does-not-exist&quot; {  depends_on = [    &quot;aws_api_gateway_integration.notes-get-integration&quot;]  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  resource_id = &quot;${aws_api_gateway_resource.notes-userName-resource.id}&quot;  http_method = &quot;${aws_api_gateway_method.notes-get-method.http_method}&quot;  status_code = &quot;${aws_api_gateway_method_response.notes-get-method-response-not-found-error-404.status_code}&quot;  selection_pattern = &quot;.*errorCode\\&quot;:\\&quot;USER_DOES_NOT_EXIST.*&quot;  response_templates {    &quot;application/json&quot; = EOF#set ($errorMessageObj = $util.parseJson($input.path(&#39;$.errorMessage&#39;))){  &quot;errorCode&quot; : &quot;$errorMessageObj.errorCode&quot;,  &quot;message&quot; : &quot;$errorMessageObj.message&quot;}EOF  }}resource &quot;aws_lambda_permission&quot; &quot;notes-get-lambda-permision&quot; {  statement_id = &quot;AllowExecutionFromAPIGatewayNotesGet&quot;  action = &quot;lambda:InvokeFunction&quot;  function_name = &quot;${aws_lambda_function.note-find-lambda.arn}&quot;  principal = &quot;apigateway.amazonaws.com&quot;}analogicznie dla POST://POST IMPLEMENTATIONresource &quot;aws_api_gateway_request_validator&quot; &quot;notes-request-validator&quot; {  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  name = &quot;NotesRequestValidator&quot;  validate_request_body = true}resource &quot;aws_api_gateway_model&quot; &quot;notes-model&quot; {  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  name = &quot;NotesRequestModel&quot;  content_type = &quot;application/json&quot;  schema = EOF{  &quot;$schema&quot;: &quot;http://json-schema.org/draft-04/schema#&quot;,  &quot;description&quot;: &quot;&quot;,  &quot;type&quot;: &quot;object&quot;,  &quot;properties&quot;: {    &quot;userName&quot;: {      &quot;type&quot;: &quot;string&quot;,      &quot;minLength&quot;: 1    },    &quot;content&quot;: {      &quot;type&quot;: &quot;string&quot;,      &quot;minLength&quot;: 1    }  },  &quot;required&quot;: [    &quot;userName&quot;,    &quot;content&quot;  ]}EOF}resource &quot;aws_api_gateway_method&quot; &quot;notes-post-method&quot; {  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  resource_id = &quot;${aws_api_gateway_resource.notes-resource.id}&quot;  http_method = &quot;POST&quot;  authorization = &quot;NONE&quot;  api_key_required = true  request_models = {    &quot;application/json&quot; = &quot;NotesRequestModel&quot;  }  request_validator_id = &quot;${aws_api_gateway_request_validator.notes-request-validator.id}&quot;  depends_on = [    &quot;aws_api_gateway_model.notes-model&quot;]}resource &quot;aws_api_gateway_method_response&quot; &quot;notes-post-method-response-created-201&quot; {  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  resource_id = &quot;${aws_api_gateway_resource.notes-resource.id}&quot;  http_method = &quot;${aws_api_gateway_method.notes-post-method.http_method}&quot;  status_code = &quot;201&quot;}resource &quot;aws_api_gateway_method_response&quot; &quot;notes-post-method-response-create-400&quot; {  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  resource_id = &quot;${aws_api_gateway_resource.notes-resource.id}&quot;  http_method = &quot;${aws_api_gateway_method.notes-post-method.http_method}&quot;  status_code = &quot;400&quot;}resource &quot;aws_api_gateway_integration&quot; &quot;notes-post-integration&quot; {  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  resource_id = &quot;${aws_api_gateway_resource.notes-resource.id}&quot;  http_method = &quot;${aws_api_gateway_method.notes-post-method.http_method}&quot;  integration_http_method = &quot;POST&quot;  type = &quot;AWS&quot;  uri = &quot;arn:aws:apigateway:eu-central-1:lambda:path/2015-03-31/functions/${aws_lambda_function.note-add-lambda.arn}/invocations&quot;  request_templates {    &quot;application/json&quot; = EOF{  &quot;userName&quot; : $input.json(&#39;$.userName&#39;),  &quot;content&quot; : $input.json(&#39;$.content&#39;)}EOF  }}resource &quot;aws_api_gateway_integration_response&quot; &quot;notes-post-integration-response&quot; {  depends_on = [    &quot;aws_api_gateway_integration.notes-post-integration&quot;]  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  resource_id = &quot;${aws_api_gateway_resource.notes-resource.id}&quot;  http_method = &quot;${aws_api_gateway_method.notes-post-method.http_method}&quot;  status_code = &quot;${aws_api_gateway_method_response.notes-post-method-response-created-201.status_code}&quot;  response_templates {    &quot;application/json&quot; = EOF#set($inputRoot = $input.path(&#39;$&#39;))$inputRootEOF  }}resource &quot;aws_api_gateway_integration_response&quot; &quot;notes-post-integration-response-error-400&quot; {  depends_on = [    &quot;aws_api_gateway_integration.notes-post-integration&quot;]  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  resource_id = &quot;${aws_api_gateway_resource.notes-resource.id}&quot;  http_method = &quot;${aws_api_gateway_method.notes-post-method.http_method}&quot;  status_code = &quot;${aws_api_gateway_method_response.notes-post-method-response-create-400.status_code}&quot;  selection_pattern = &quot;.*errorCode.*&quot;  response_templates {    &quot;application/json&quot; = EOF#set ($errorMessageObj = $util.parseJson($input.path(&#39;$.errorMessage&#39;))){  &quot;errorCode&quot; : &quot;$errorMessageObj.errorCode&quot;,  &quot;message&quot; : &quot;$errorMessageObj.message&quot;}EOF  }}resource &quot;aws_lambda_permission&quot; &quot;notes-post-lambda-permision&quot; {  statement_id = &quot;AllowExecutionFromAPIGatewayNotesPost&quot;  action = &quot;lambda:InvokeFunction&quot;  function_name = &quot;${aws_lambda_function.note-add-lambda.arn}&quot;  principal = &quot;apigateway.amazonaws.com&quot;}Na zakończenie sekcja związana z osadzeniem API i wystawieniem na zewnątrz. Ponieważ API w chwili zakończenia wykonywania skryptu zostanie wystawione na świat, warto zatroszczyć się o jego podstawowe chociaż zabezpieczenia. W dyrektywie aws_api_gateway_api_key możemy wskazać token, który będzie niezbędny do korzystania z usług. W poniższym przykładzie jest on ustawiony na stałe, ale nic nie stoi na przeszkodzie, żeby go przekazać w parametrach.resource &quot;aws_api_gateway_deployment&quot; &quot;notes-deployment&quot; {  depends_on = [    &quot;aws_api_gateway_integration.notes-get-integration&quot;,    &quot;aws_api_gateway_integration.notes-post-integration&quot;  ]  rest_api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;  stage_name = &quot;test&quot;}resource &quot;aws_api_gateway_usage_plan&quot; &quot;notes-usage-plan&quot; {  name         = &quot;NotesUsagePlan&quot;  description  = &quot;Notes usage plan&quot;  api_stages {    api_id = &quot;${aws_api_gateway_rest_api.NotesAPI.id}&quot;    stage  = &quot;${aws_api_gateway_deployment.notes-deployment.stage_name}&quot;  }}resource &quot;aws_api_gateway_api_key&quot; &quot;notes-api-key&quot; {  name = &quot;NotesApiKey&quot;  value = &quot;NtMLWD6CG49mgtbpcWTmd5jCtkSyUvow9LMV5KMf&quot;}resource &quot;aws_api_gateway_usage_plan_key&quot; &quot;notes-usage-plan-key&quot; {  key_id        = &quot;${aws_api_gateway_api_key.notes-api-key.id}&quot;  key_type      = &quot;API_KEY&quot;  usage_plan_id = &quot;${aws_api_gateway_usage_plan.notes-usage-plan.id}&quot;}Tym samym dotarliśmy do końca przykładu. Uruchomienie skryptu spowoduje wygenerowanie całego środowiska w przeciągu sekund. Warto również przypomnieć, że jednym poleceniem możemy posprzątać po sobie, usuwając wszystkie zasoby z chmury.Uważny czytelnik mógłby w tym momencie zapytać - “a co w przypadku, gdy mamy wiele środowisk?”. I będzie to słuszne pytanie - powyższy przykład nie zadziała poprawnie w takiej sytuacji. Podobnie jak w przypadku wielu developerów współdzielących jedno konto AWS. Albo, co jest najczęstszym przypadkiem, kombinacją obu powyższych. W tej sytuacji, z pomocą przychodzi nam mechanizm zmiennych środowiskowych, wspierany przez Terraforma. Wystarczy taką zmienną zdefiniować, a następnie użyć w nazwach wszystkich nazwanych zasobów. Trzeba także przekazać ją do wnętrza funkcji Lambda tak, aby funkcja wiedziała, z którą tabelą DynamoDB ma rozmawiać. Żeby nie powielać i tak długiego skryptu, pozwolę sobie zastosować tryb zmian, żeby zobrazować sposób wprowadzania takiej zmiennej:--- a/instance.tf+++ b/instance.tf+variable &quot;env&quot; {+  default = &quot;&quot;+}+-  name = &quot;notes&quot;+  name = &quot;${var.env}-notes&quot;-  name = &quot;iam_for_lambda&quot;+  name = &quot;${var.env}-iam_for_lambda&quot;-  name = &quot;role_policy_for_dynamodb_access&quot;+  name = &quot;${var.env}-role_policy_for_dynamodb_access&quot;-  function_name = &quot;note-add-lambda&quot;+  function_name = &quot;${var.env}-note-add-lambda&quot;+  environment {+    variables = {+      environment_name = &quot;${var.env}&quot;+    }+  }-  function_name = &quot;note-find-lambda&quot;+  function_name = &quot;${var.env}-note-find-lambda&quot;+  environment {+    variables = {+      environment_name = &quot;${var.env}&quot;+    }+  }-  name = &quot;NotesAPI&quot;+  name = &quot;${var.env}-NotesAPI&quot;-  stage_name = &quot;test&quot;+  stage_name = &quot;${var.env}&quot;-  name         = &quot;NotesUsagePlan&quot;+  name         = &quot;${var.env}-NotesUsagePlan&quot;-  name = &quot;NotesApiKey&quot;-  value = &quot;NtMLWD6CG49mgtbpcWTmd5jCtkSyUvow9LMV5KMf&quot;+  name = &quot;${var.env}-NotesApiKey&quot;+  value = &quot;${var.env}-NtMLWD6CG49mgtbpcWTmd5jCtkSyUvow9LMV5KMf&quot;--- a/note-add-lambda.js+++ b/note-add-lambda.js+const env = process.env.environment_name;-        TableName: &#39;notes&#39;,+        TableName: env + &#39;-notes&#39;,--- a/note-find-lambda.js+++ b/note-find-lambda.js+const env = process.env.environment_name;-        TableName: &#39;notes&#39;,+        TableName: env + &#39;-notes&#39;,PodsumowanieAutomatyzacja procesów tworzenia środowiska wykonywalnego dla naszych komponentów w chmurze, nie tylko pozwala zaszczędzić czas, ale stanowi żywą dokumentację oraz przepis na to, jak takie środowisko powielić w razie potrzeb. Mam nadzieję, że wpis ten, wraz z artykułem Jakuba, stanowią wystarczające wprowadzenie do pracy w środowisku Amazon AWS.Bibliografia  https://www.terraform.io/docs/index.html",
"url": "/2018/03/26/terraform-czyli-o-tym-jak-okielznac-chmure-od-amazona.html",
"author": "Jacek Grobelny",
"authorUrl": "/authors/jgrobelny.html",
"image": "jgrobelny.jpg",
"highlight": "/assets/img/posts/2018-03-26-terraform-czyli-o-tym-jak-okielznac-chmure-od-amazona/terraform.png",
"date": "26-03-2018",
"path": "pl/2018-03-26-terraform-czyli-o-tym-jak-okielznac-chmure-od-amazona"
}
,


"2018-02-22-monitorowanie-zespolowych-trendow-jakosci-kodu-html": {
"title": "Monitorowanie zespołowych trendów jakości kodu",
"lang": "pl",
"tags": "sonarqube sonarqube companion jakość kodu",
"content": "W jednym z wcześniejszych wpisów omawialiśmy już zalety i zasadność statycznej analizy kodu z pomocą SonarQube (tutaj).Przyjmijmy więc, że jesteśmy już przekonani do całego przedsięwzięcia, inwestujemy w infrastrukturę i zaczynamy mierzyć. Pierwsze sukcesy zapewne już są na naszym koncie, jakość projektów wzrosła, naruszeń o krytycznych priorytetach mamy coraz mniej, a cały zespół jest zadowolony z dbania o własne podwórko. Powstaje jednak pytanie, jak monitorować trend zmian? Jak ocenić czy prowadzone przez nas prace przynoszą faktyczne efekty? Dodatkowo powstaje problem, jak monitorować statystyki naruszeń na poziomie zespołu utrzymującego wiele projektów?SonarQube CompanionW odpowiedzi na zadane pytania, stworzyliśmy aplikację dashboardu dla SonarQube i udostępniliśmy ją wszystkim na GitHub!Projekt na pierwszym miejscu stawia zespoły i ich potrzeby. Monitorowane projekty są agregowane w zespoły, zgodnie ze strukturą firmy. Dodatkowo naruszenia możemy śledzić na wykresach z konfigurowalnymi zakresami sprintów, czy nawet naniesionymi ważnymi datami z życia zespołu.W zależności od oczekiwań, możesz śledzić stan bazy kodu na różnych poziomach. Przykładowo, drzewo projektów pokazuje szybki rzut na stan wszystkich projektów oraz sumaryczny stan firmy / zespołu. Alternatywnie, przeglądając ekran konkretnego zespołu, czy nawet projektu, widzisz obszary na które warto zwrócić uwagę przy codziennej pracy.Drzewo zespołówDrzewo zespołów przedstawia hierarchiczną reprezentację organizacji na projekty oraz pracujące nad nimi zespoły.Poszczególne kafle projektów/zespołów przedstawiają ich bieżącą kondycję oraz krótkie podsumowanie statystyk. Celem nadrzędnym jest dążyć do bezwzględnej zielenie na całym ekranie ;-)Poszczególne kolory są ustalane zgodnie z zasadą:  kolor czerwony (unhealthy) - występują naruszenia o priorytecie bloker, bezpośrednio w grupie lub jednej z podgrup, wymagana natychmiastowa akcja,  kolor pomarańczowy (warning) - występują naruszenia o priorytecie krytyczny, bezpośrednio w grupie lub jednej z podgrup, należy planować rozwiązanie problemów,  kolor zielony (healthy) - brak naruszeń o priorytecie bloker oraz krytyczny, mogą znajdować się naruszenia o niższych priorytetach, wystarczające są codzienne akcje porządkowe.Zasady kolorystyczne będą się powtarzały przy takich samych ograniczeniach na innych ekranach. Same akcje/interakcje to już wypracowana przez nas indywidualnie polityka i w różnych zespołach może zostać zrealizowana inaczej.Dodatkowo, dla każdego kafla widzimy szybką statystykę liczby projektów, ogólnej oceny zdrowia i podsumowanie liczby naruszeń.Przegląd zespołuStrona zespołu pozwala na bieżąco śledzić stan projektów nad którymi pracuje zespół, bez dodatkowych rozproszeń związanych z działalnością innych zespołów.W ramach ekranu zespołu możemy śledzić:  szybki przegląd stanu projektu, liczbę projektów, podgrup, itp,  wykres zmian liczby naruszeń w wybranym przedziale czasu:          uwzględniający filtrowanie po różnych typach naruszeń,      zaznaczanie na osi wykresu dat i nanoszenie sprintów zespołu,        przegląd grup, z których składa się bieżąca grupa (w przypadku głębszych hierarchii z dodatkowymi grupami agregującymi),  listę projektów składających się na grupę:          w grupie znajdują się projekty zdefiniowane w niej oraz w grupach pod nią w drzewie,      listę projektów można filtrować na podstawie ich statusu, tj. zmiany liczby naruszeń, poprawienia, pogorszenia jakości projektów      dla każdego projektu, kolorem zaprezentowana jest jego bieżąca kondycja,      dla każdego projektu prezentowana jest bieżąca liczba naruszeń oraz zmian tej wartości w oknie czasowym wybranym na wykresie.      Przegląd projektuMożliwe jest również przeglądanie stanu konkretnego projektu, niezależnie od jego przynależności do grup czy zespołów.Widok pojedynczego projektu jest uproszczonym widokiem zespołu prezentującym jedynie podzbiór danych zasadnych dla tego projektu.Realny wpływ na pracę zespołówWprowadzenie dodatkowych narzędzi historycznego śledzenia jakości projektów pozwoliło utrzymać motywację zespołów do zmniejszania liczby naruszeń.Miły dla oka interfejs i możliwość szybkiego porównania wprowadziły mały aspekt rywalizacji oraz pogoni za zielonym wśród programistów.Wszystko to pozytywnie przełożyło się zarówno na poprawę istniejących, jak i zwiększenie świadomości i zmniejszenie liczby nowych naruszeń.Wprowadzone narzędzie pozwoliło nam długofalowo monitoroawć zespoły pracujące nad wieloma małymi projektami (co jest nieuniknione przy architekturach typu microservices), jak i utrzymać zaangażowanie oraz zainteresowanie programistów ponad początkową euforię, również w kolejnych, bardziej mozolnych miesiącach.Uruchomienie aplikacjiKompletne kody projektu dostępne są dla każdego na GitHub. Dodatkowo, zbudowane wersje kodów wrzucamy w postaci gotowego do użycia obrazu na Docker Hub.Nic nie stoi na przeszkodzie, żebyś też skorzystał z naszych doświadczeń. Szczegółowe informacje, jak uruchomić projekt, zarówno produkcyjnie, jak i lokalnie, znajdują się w README projektu na GitHub.Zachęcamy do skorzystania, zgłaszania uwag i wsparcia przy dalszych pracach rozwojowo/utrzymaniowych.Przydatne linki:  Projekt GitHub  Obraz w Docker Hub",
"url": "/2018/02/22/monitorowanie-zespolowych-trendow-jakosci-kodu.html",
"author": "Grzegorz Lipecki",
"authorUrl": "/authors/glipecki.html",
"image": "glipecki.jpg",
"highlight": "/assets/img/posts/2018-02-22-monitorowanie-zespolowych-trendow-jakosci-kodu/monitorowanie-kodu.jpg",
"date": "22-02-2018",
"path": "pl/2018-02-22-monitorowanie-zespolowych-trendow-jakosci-kodu"
}
,


"2018-01-17-aws-serverless-programming-html": {
"title": "AWS - serverless programming",
"lang": "pl",
"tags": "aws serverless",
"content": "Trudno dziś wyobrazić sobie branżę IT bez chmury obliczeniowej. Przeniesienie naszych serwerów do clouda i zastosowanie Infrastructure as Code (IaC) pozwala często na zoptymalizowanie kosztów, a także na zapewnienie lepszej dostępności i skalowalności rozwiązania. Architektura serverless przenosi nas jeszcze poziom wyżej - przestajemy myśleć o maszynach wirtualnych, skupiamy się na kodzie rozwiązującym konkretne potrzeby biznesowe. Wymusza ona również pewną zmianę mentalną w środowisku inżynierów IT - praca z arkuszem kalkulacyjnym i liczenie kosztów przygotowywanego rozwiązania staje się nieodłączną częścią procesu produkcji. Wraz z wprowadzeniem przez Amazon usługi AWS Lambda w 2014 roku programowanie w architekturze serverless stało się jeszcze prostsze. W artykule przedstawię przykład serwisu do przechowywania notatek, przygotowany w oparciu o architekturę serverless, dostarczaną przez AWS. Pokażę również koszty takiego rozwiązania.Wymagania biznesowe:  Użytkownik może zapisywać notatki tekstowe.  Użytkownik może pobrać zapisane wcześniej notatki tekstowe.Środowisko deweloperskie:  Potrzebne będzie aktywne konto w Amazon AWS. Założenie takiego konta jest dość proste i dla większości usług darmowe przez pierwszy rok użytkowania. Konto możemy założyć tutaj: https://aws.amazon.com/free. Podczas tworzenia konta konieczne jest podanie danych karty kredytowej, ale przy okazji tworzenia opisanego tu serwisu będziemy używali jedynie usług wchodzących w skład rocznego free tier. W każdym momencie istnieje również możliwość sprawdzenia jakie są nasze należności dla Amazona. Niestety nie ma możliwości ustalenia granicy kwotowej, której nie chcemy przekroczyć, po której nastąpi wyłączenie naszej usługi. Można za to zdefiniować alerty, które powiadomią nas w przypadku przekroczenia ustalonej należności. Konfigurując konto w AWS warto włączyć Multi-Factor Authentication (MFA), ustrzeże nas to przed najgorszym możliwym scenariuszem, czyli przejęciem konta przez nieuprawnioną osobę. To może pociągnąć za sobą spore koszty obciążające nasz rachunek.Implementacja odczytu notatekWarstwa persystencjiAby zaimplementować odczyt notatek będziemy potrzebowali jakąś warstwę persystencji, gdzie przechowywane będą dane naszych notatek, komponent który wystawi nam restowe API i coś w czym zaimplementujemy “logikę biznesową” reagującą na wywołanie API pobraniem danych z warstwy persystencji.AWS udostępnia warstwę persystencji na różne sposoby. Mamy do dyspozycji storage plikowy S3, bazy relacyjne RDS a także bazę NoSql DynamoDB. Jeżeli chcemy być w 100% serverless, nie używamy RDS, gdyż wymaga ona utrzymywania ciągle działającej instancji (nadal nie musimy się w tym przypadku przejmować vm’kami - baza jest uruchamiana jako niezależny serwis, ale płacimy za cały czas, w którym dostępna jest baza). Do naszego zastosowania dobrze nadaje się baza DynamoDB. Aby utworzyć instancję bazy, logujemy się do konsoli AWS https://aws.amazon.com/console/ i przechodzimy do serwisu DynamoDB. Zanim utworzymy jakikolwiek zasób w konsoli AWS ważne jest określenie regionu, w którym chcemy działać. Mapa przedstawia jakie regiony są dostępne (pomarańczowe okręgi) i ile “availability zones” znajduje się w każdym regionie. Zielone okręgi oznaczają dodatkowe regiony, które będą dostępne w przyszłości.Najlepiej wybrać region, znajdujący się najbliżej lokalizacji, dla której oferujemy nasze rozwiązanie (mniejsze opóźnienia w komunikacji). Wszystkie komponenty składające się na nasze rozwiązanie powinny w miarę możliwości znajdować się w tym samym regionie. Możliwa jest oczywiście komunikacja, między różnymi regionami, ale pociąga to za sobą dodatkowe koszty. Region wybieramy klikając nazwę miasta w prawym górnym rogu konsoli:Po wybraniu regionu klikamy przycisk “Create table” i przechodzimy do widoku tworzenia tabeli. Tutaj musimy określić nazwę tabeli, np.: “notes” oraz klucz. Klucz tabeli DynamoDB może składać się z jednej lub dwóch kolumn. Pojedynczy klucz zawiera tylko tzw. partition key - kolumna, którą wybierzemy będzie używana do rozkładu danych w różnych partycjach bazy danych. Wybierając partition key pamiętajmy, że pożądane jest, aby podczas działania systemu odwoływać się do różnych partycji - to zapewnia lepsze skalowanie. W przypadku aplikacji do przechowywania notatek dobrym wyborem wydaje się nazwa użytkownika. Nie możemy jednak poprzestać na jednoczęściowym kluczu, gdyż moglibyśmy zapisać tylko jedną notatkę dla użytkownika - podobnie jak w RDS wartości klucza muszą być unikalne. Dodamy więc drugą część klucza tzw. sort key. W naszym przypadku będzie to timestamp dodania notatki:Wybór klucza tabeli jest bardzo istotnym krokiem - decyzja jest wiążąca i później nie można zmienić już klucza. Co ważne, wyszukiwanie w tabeli DynamoDB możliwe jest jedynie po kluczu lub indeksie. W innym przypadku wyszukiwanie realizowane jest jako scan tabeli. Warto tu napisać, że indeksy wyglądają inaczej niż indeksy w relacyjnej bazie danych. Indeks w DynamoDB jest de facto kopią tabeli (określamy nawet jakie kolumny znajdą się w kopii i jedynie one będą dostępne dla wyszukanego rekordu). Kopia ta jest synchronizowana w sposób asynchroniczny! z oryginalną tabelą. Przy okazji: DynamoDB zapewnia transakcyjność jedynie na poziomie pojedynczego wiersza tabeli - możliwe są tzw. conditional updates, czyli modyfikacja wiersza jeżeli spełnia on określone kryteria. Klasyczna transakcyjność typu przelew z konta A na konto B nie jest wspierana!W tej chwili dodamy kilka rekordów do naszej tabeli, aby możliwe było przetestowanie funkcjonalności odczytu notatek. Przechodzimy na zakładkę items i klikamy przycisk “Create item”. Wpisujemy nazwę użytkownika oraz timestamp. Dodajemy również nową kolumnę o nazwie “content” i typie “String”:Wpisujemy przykładową treść notatki. Dodajmy jeszcze kilka przykładowych notatek:Logika biznesowaWarstwę persystencji mamy gotową, zajmiemy się teraz częścią biznesową. Zaimplementujemy ją w postaci lambd udostępnianych przez AWS. Wybieramy w lewym górnym rogu konsoli Services i przechodzimy do serwisu Lambda. Tutaj klikamy przycisk “Create a function” i przechodzimy do określania parametrów naszej lambdy. Określamy nazwę funkcji np.:”getNotes” oraz wybieramy język, w którym tworzona jest nasza funkcja. Do wyboru mamy C#, Javę 8, Node.js 4.3, Node.js 6.10, Python 2.7, Python 6.10. Wybieramy Node.js 6.10.Za wyborem języka idą nie tylko określone cechy danego języka, ale także sposób jego działania w AWS. Przykładowo implementacja w Javie będzie zdecydowanie najszybciej działającym kodem na AWS, z drugiej jednak strony powołanie nowej instancji lambdy napisanej w Javie będzie trwało dość długo, bo aż kilka sekund. Przy założeniu, że nasz system będzie doświadczał peek’ów wywołań może to oznaczać, że w jednym momencie AWS będzie próbował zainstancjonować tysiąc (w zależności od liczby jednoczesnych wywołań) lambd w tym samym czasie, a dalej przełoży się to na kilkusekundowe opóźnienie w realizacji tych requestów. Będziemy musieli również ponieść koszt za czas jaki został poświęcony na wystartowanie każdej instancji lambdy. Analogiczny przypadek dla lambd zaimplementowanych w którymś z języków skryptowych może mymagać znacząco mniejszej liczby instancji, gdyż lambdy będą wcześniej dostępne do obsługi requestów (kilkaset milisekund w porównaniu do kilku sekund). Ciekawe porównianie szybkości działania dla poszczególnych języków można znaleźć na stronie: https://read.acloud.guru/comparing-aws-lambda-performance-when-using-node-js-java-c-or-python-281bef2c740f.W polu Role wybieramy “Create a custom role” i przechodzimy do tworzenia nowej roli dla tworzonej przez nas lambdy. Rola ta będzie przydatna do definiowania uprawnień, które umożliwią lambdzie dostęp do różnych serwisów AWS. Pozostawiamy domyślnie wypełnione wartości i klikamy przycisk Allow. Następnie na ekranie tworzenia lambdy wybieramy w polu Existing role* utworzoną przez nas rolę, powinna to być rola o nazwie “lambda_basic_execution”:Klikamy przycisk “Create function”. Funkcja zostaje utworzona, a my widzimy ekran konfiguracji utworzonej przez nas funkcji. W polu “Function code” znajduje się inline’owy edytor, w którym możemy edytować kod naszej lambdy.Oczywiście tworzenie kodu w konsoli pozwala jedynie na tworzenie prostych funkcji. Bardziej skomplikowane lambdy tworzymy w zewnętrznych narzędziach i upload’ujemy w postaci archiwów zip. W polu “Code entry type” wybieramy “Upload a .ZIP file”.Implementacja lambdy w Node.js 6.10 polega na implementacji trój-argumentowej funkcji. Pierwszy argument zawiera dane event’a, który uruchomił lambdę, drugi to informacje kontekstowe dot. środowiska uruchomieniowego, trzeci to metoda callbackowa pozwalająca na zwrócenie wyniku lub zaraportowanie błędu. Poniżej znajduje się kod lambdy umożliwiający pobieranie notatek z DynamoDB dla użytkownika przekazanego w argumencie event:// dołączamy sdk AWSconst AWS = require(&#39;aws-sdk&#39;);exports.handler = (event, context, callback) =&amp;gt; {    console.log(&quot;event=&quot;, event);    // tworzymy instancję DocumentClient, która umożliwia zapytanie bazy danych    let documentClient = new AWS.DynamoDB.DocumentClient();    // definicja parmetrów zapytania do bazy danych    // szukamy rekordów, dla których userName jest równe userName przekazanemu w event    let params = {        TableName: &#39;notes&#39;,        KeyConditionExpression: &#39;userName = :userName&#39;,        ExpressionAttributeValues: {            &#39;:userName&#39;: event.userName        }    }    // wykonanie zapytania do bazy    documentClient.query(params, (err, data) =&amp;gt; {        if (err) {            // w przypdaku błędu logujemy błąd i zwracamy błąd w funkcji callback            console.log(err);            callback(err);        } else {            // logujemy odpowiedź z bazy u zwracamy odpowiedź w funkcji callback            console.log(data);            callback(null, data);        }    });};Po dodaniu kodu określimy jeszcze kilka parametrów konfiguracyjnych lambdy. W polu “Basic setting” określamy timeout dla wywołania lambdy, warto zauważyć, że maksymalny timeout to 5 minut, co oznacza, że żadna lambda nie może wykonywać się dłużej niż 5 minut. Dodatkowo możemy określić rozmiar pamięci i co ciekawe przekłada się to również na moc procesora, którą dostanie nasza lambda (rozmiar pamięci przekłada się oczywiście na koszt usługi). W polu concurrency możemy określić maksymalną liczbę instancji naszej lambdy.Ten wprowadzony niedawno parametr ma bardzo ważne znaczenie dla lambd napisanych w języku Java. Pozwala bowiem obejść problem uruchomienia bardzo dużej liczby instancji lambdy w przypadku peek’u - spowolni obsługę wszystkich requestów (co i tak by nastąpiło ze względu na długi czas tworzenia nowej instancji lambdy w Javie), ale jednocześnie pozwoli na zmniejszenie kosztów usługi - następne wywołania będą już działały na zainstancjonowanych lambdach i będą szybkie.Zapisujemy lambdę przyciskiem “Save”. Teraz możemy przetestować napisany przez nas kod. Klikamy przycisk “Test” i definiujemy json’a, który zostanie przekazany w evencie podczas uruchamiania naszej lambdy.{    &quot;userName&quot;: &quot;user1&quot;}Po uruchomieniu testu powinniśmy otrzymać błąd oznaczający brak autoryzacji zdefiniowanej przez nas lambdy do odczytu danych z bazy DynamoDB:Aby nadać odpowiednie uprawnienia musimy przejść do usługi IAM (Services-&amp;gt;IAM). Wybrać z menu po lewej stronie “Roles”, a następnie wybrać rolę, którą zdefiniowaliśmy podczas tworzenia lambdy (lambda_basic_execution). Klikamy przycisk “Attach policy”:W polu wyszukiwania “policy type” wpisujemy DynamoDB i po wyszukaniu dołączamy policy “AmazonDynamoDBFullAccess”:Po ponownym wywołaniu testu lambdy powinniśmy otrzymać w wyniku dane pobrane z bazy DanamoDB:Rest APIPozostaje nam udostępnienie naszego serwisu do notatek w formie API restowego. Komponent AWS, który umożliwi nam zrobienie tego to API Gateway. W konsoli AWS przechodzimy do Serices-&amp;gt;API Gateway i po załadowaniu strony klikamy przycisk “Get started”. W polu wyboru wybieramy New API i wpisujemy nazwę np.: “notes”:Klikamy przycik “Create API”. W polu “Actions” wybieramy “Create Resource”:W polu “Resource Name” wpisujemy “userName”, a w “Resource Path” “{username}”:Klikamy przycisk “Create Resource”. Będąc na nowoutworzonym resource z menu “Actions” wybieramy “Create Method”:Z listy rozwijanej wybieramy metodę GET i zatwierdzamy:W polu wyboru “Integration type” wybieramy “Lambda function”. Poniżej w polu “Lambda region” wybieramy region, w którym utworzyliśmy lambdę getNotes. W polu “Lambda Function” podajemy nazwę funkcji np.: “getNotes” (po wpisaniu pierwszej litery nazwa powinna się podpowiedzieć):Klikamy przycisk “Save” i zatwierdzamy nadanie uprawnienia wykonywania lambdy (popup). Ponieważ chcemy, aby nazwa użytkownika, dla którego chcemy pobrać notatki przekazywana była w ścieżce potrzebujemy jeszcze dodatkowej konfiguracji. Kliknij na link “Integration Request”. Rozwiń “URL Path Parameters” i kliknij “Add path”. W polu name wpisz “username”, w “mapped from” “method.request.path.username” i zatwierdź:Rozwiń “Body Mapping Templates” i kliknij “Add mapping template”. W polu “Content-Type” wpisz “application/json” i zatwierdź. W polu edycji dodaj mapowanie:{   &quot;userName&quot;: &quot;$input.params(&#39;username&#39;)&quot;}Zapisz mapowanie przyciskiem “Save”.Kliknij w drzewie “Resources” metodę “GET”. Po prawej stronie powinien pojawić się przepływ danych łączący API Gateway i lambdę. Kliknij przycisk “Test” aby przetestować konfigurację API Gateway. Jako parametr “{username}” podaj “user1” i kliknij przycisk “Test”:API Gateway powinno zwrócić notatki znajdujące się w bazie danych DynamoDB:Aby wystawić naszą usługę na świat musimy jeszcze wdrożyć konfigurację API Gateway. W tym celu z menu “Actions” wybieramy “Deploy API”. W polu “Deployment stage” wybieramy “[New Stage]”, w polu “Stage name” wpisujemy nazwę naszego środowiska np.: “test” i klikamy przycisk “Deploy”:Od tej chwili nasz serwis dostępny jest w sieci. Aktualnie nie jest on niczym zabezpieczony, więc każdy znający jego adres może go wywołać. Aby sprawdzić działanie z zewnętrznej aplikacji, przechodzimy na zakładkę “Export” i pobieramy json’a dla postmana:Pobraną kolekcję importujemy do Postmana. W ścieżce wywołania zamieniamy “:username” na faktyczną nazwę użytkownika i wysyłamy żądanie. W odpowiedzi powinniśmy dostać dane z bazy DynamoDB:Dostęp do logów możliwy jest za pomocą serwisu CloudWatch. Aby dotrzeć do logów konkretnej lambdy należy wejść na jej konfigurację, np.: Lambda-&amp;gt;Functions-&amp;gt;getNotes i przejść na zakładkę “Monitoring”. Dalej na jednym z paneli klikamy link “Jump to Logs”. W to samo miejsce możemy również dotrzeć z poziomu serwisów: Services-&amp;gt;CloudWatch-&amp;gt;Logs.Implementacja zapisu notatekWarstwę persystencji mamy już przygotowaną i ona nie wymaga żadnych modyfikacji. Aby umożliwić zapis notatek, musimy zaimplementować dodatkową lambdę. Tworzymy ją w taki sam sposób jak lambdę do odczytu podając inną nazwę, np.: addNote i wybierając utworzoną wcześniej rolę “lambda_basic_execution”. Kod lambdy powinien wyglądać mniej więcej tak:const AWS = require(&#39;aws-sdk&#39;);exports.handler = (event, context, callback) =&amp;gt; {    console.log(&quot;event=&quot;, event);    // tworzymy instancję DocumentClient, która umożliwia wstawienie do bazy danych    let documentClient = new AWS.DynamoDB.DocumentClient();    // definicja parmetrów wstawienia do bazy    // nazwę użytkownika i treść notatki pobieramy z przekazanego event&#39;a    let params = {        TableName: &#39;notes&#39;,        Item: {            userName: event.userName,            timestamp: Date.now(),            content: event.content        }    }    // próba wstawienia do bazy    documentClient.put(params, (err, data) =&amp;gt; {        if (err) {            console.log(err);            callback(err);        } else {            console.log(data);            callback(null, data);        }    });};Aby przetestować lambdę definiujemy nowy event testowy w formie json’a:{  &quot;userName&quot;: &quot;user1&quot;,  &quot;content&quot;: &quot;nowo dodana notatka&quot;}i klikamy przycisk test. Notatka powinna być zwrócona podczas pobierania notatek użytkownika “user1”.Pozostaje jeszcze konfiguracja API Gateway. W tym celu wchodzimy na Services-&amp;gt;API Gateway i odnajdujemy utworzone wcześniej API “notes”. W drzewie “Resources” wybieramy “/” i w menu “Actions” wybieramy akcję “Create Method”.Z listy metod wybierramy “POST” i zatwierdzamy wybór. W polu “Integration type” wybieramy “Lambda Function”. Określamy region naszej lambdy i podajemy jej nazwę w polu “Lambda Function”:Klikamy przycisk “Save” i zatwierdzamy dodanie uprawnienia do funkcji. I to tyle z konfiguracji API Gateway. Podobnie jak w przypadku odczytu również na zapisie możemy przetestować konfigurację API Gateway. Klikamy przycisk “Test” i w polu “Request Body” podajemy treść eventa:W odpowiedzi powinniśmy otrzymać status http 200, a nowododana notatka powinna być widoczna w pobieranych notatkach użytkownika “user1”. Aby nowa metoda była dostępna z zewnątrz, należy zdeployować jeszcze raz nasze api wybierając z menu “Actions” “Deploy API” i podać utworzony wcześniej stage “test”. Teraz ponownie możemy wyeksportować konfigurację do postmana i wykonać test:Diagram użytych przez nas komponentów AWS wygląda następująco:Ile to kosztujePraca w cloudzie AWS daje nam możliwość dokładnego policzenia kosztów naszego rozwiązania. Trzeba przy tym pamiętać również o tym, że AWS daje nam tzw. “Free tier” czyli pewną pulę zasobów, która jest dostępna bezkosztowo.Koszty DynamoDBI tak zaczynając od warstwy persystencji na koszty DynamoDB składają się koszty przestrzeni, które zajmują dane oraz koszty operacji odczytu i zapisu wykonywane w jednostce czasu. Przy kosztach zapisu i odczytu AWS posługuje się jednostkami:  RCU (read capacity unit) - jeden RCU umożliwia wykonywanie dwóch odczytów na sekundę,  WCU (write capacity unit) - jeden WCU umożliwia wykonywanie jednego zapisu na sekundę.Jeżeli nasza aplikacja obsługuje ruch, w którym maksymalnie wykonywane są 2 odczyty na sekundę przez cały czas jej działania to AWS naliczy opłatę za 1 RCU.W przypadku DynamoDB Free Tier wynosi:  25 RCU,  25 WCU,  25 GB zaindeksowanych danych.Oznacza to, że aplikacja generująca 25 zapisów na sekundę i 50 odczytów na sekundę, której dane mieszczą się w 25 GB nie generuje kosztów. Free tier dla DynamoDB jest stały, nie ma ograniczenia czasowego. Po przekroczeniu wartości Free tier koszt wynosi odpowiednio:  1 RCU: $0.09 miesięcznie,  1 WCU: $0.47 miesięcznie,  1 GB: $0.25 miesięcznie.Szczegóły naliczania kosztów dla DynamoDB można znaleźć na stronie https://aws.amazon.com/dynamodb/pricing/.Koszty LambdyW przypadku lambd płacimy za liczbę wykonanych żądań oraz za tzw. gigabajto-sekundy (GB-seconds). W konfiguracji lambdy możemy określić ile maksymalnie pamięci chcemy jej przydzielić. Minimalnie to 128 MB, maksimum 3008 MB. Liczbę GB-s zużytych przez lambdę określamy jako:-liczba skonfigurowanej pamięci w MB / 1024 * czas wykonania lambdy w sekundach (zaokrąglony do 100 ms w górę).Czas wykonania lambdy, za który AWS naliczy nam opłatę jest zawsze podawany w logu po wykonaniu danej lambdy, np.:REPORT RequestId: 31aaeaec-ef26-11e7-9892-57823db0c303    Duration: 316.42 ms    Billed Duration: 400 ms Memory Size: 128 MB    Max Memory Used: 34 MBW tym przypadku nasza lambda zużyła 128/1024 GB * 0.4 s = 0.125 GB * 0.4 s = 0.05 GB-s. Free tier dla lambd wynosi:  1 000 000 żądań miesięcznie,  400 000 GB-s miesięcznie.Po przekroczeniu tych wartości płacimy:  za każde 1 000 000 żądań $0.20,  za 1 GB-s $0.00001667.Szczegóły możemy znaleźć na stronie https://aws.amazon.com/lambda/pricing/.Koszty API-GatewayZdecydowanie najdroższym komponentem, którego użyliśmy w naszym przykładzie jest API-Gateway. W tym przypadku płacimy za liczbę żądań oraz dane, które transferujemy przez api. Warstwa darmowa obejmuje 1 000 000 żądań miesięcznie, ale jest dostępna przez pierwsze 12 miesięcy używania API-Gateway. Po tym czasie koszty wyglądają następująco:  1 000 000 żądań: $3.70,  pierwsze 10 TB transferowanych danych $0.09 za 1 GB,  kolejne 40 TB transferowanych danych $0.085 za 1 GB,  kolejne 100 TB transferowanych danych $0.07 za 1 GB,  kolejne 350 TB transferowanych danych $0.05 za 1 GB.Szczegóły dot. kosztów API-Gateway dostępne są na stronie https://aws.amazon.com/api-gateway/pricing/.Koszty przykładowej aplikacjiProponuję teraz ćwiczenie polegające na policzeniu kosztów przygotowanej przez nas aplikacji. Załóżmy, że chcemy obsługiwać milion użytkowników i każdy z nich będzie zapisywał 10 notatek dziennie i odczytywał 30. Zakładamy że jedna notatka ma wielkość 1 KB.Koszty DynamoDBMamy więc 1 000 000 użytkowników zapisujących 10 notatek każdego dnia. Policzmy ile zapisów na sekundę potrzebujemy:  1 000 000 użytkowników * 10 notatek * 31 dni = 310 000 000 zapisywanych notatek miesięcznie,  310 000 000 / (31 dni * 24h * 60m * 60s) = 310 000 000 / 2678400s = 116 zapisów na sekundę,czyli potrzebujemy 116 WCU.Każdy z użytkowników odczytuje 30 notatek każdego dnia:  1 000 000 użytkowników * 30 notatek * 31 dni = 930 000 000 odczytywanych notatek miesięcznie,  930 000 000 / (31 dni * 24h * 60m * 60s) = 930 000 000 / 2678400s = 348 odczytów na sekundę,ponieważ jeden RCU pozwala na 2 odczyty na sekundę potrzebujemy 348 / 2 = 174 RCU.Maksymalna liczba notatek jaką będziemy przechowywać w systemie po upływie 1 roku to:  12 miesięcy * 310 000 000 notatek = 3 720 000 000 * 1 KB = 3720 GB danych.Koszty wyniosą odpowiednio:  koszt zapisów, czyli WCU to 116 - 25 (free tier) = 91 WCU * $0.47 = $42.77 miesięcznie,  koszt odczytów, czyli RCU to 174 - 25 (free tier) = 149 RCU * $0.09 = $13.41 miesięcznie,  koszt przechowywania danych to 3720 GB * $0.25 = $930 miesięcznie.W sumie za użycie DynamoDB zapłacimy miesięcznie $986.18.Koszty lambdLambda getNotes:  będzie wywoływana 930 000 000 razy miesięcznie,  możemy odjąć od tej liczby 1 000 000 requestów (free tier),  zostaje 929 000 000 requestów,  za każdy 1 000 000 requestów płacimy $0.20,  w sumie mamy więc 929 * $0.20 = $185.8 miesięcznie.Lambda addNote:  będzie wywoływana 310 000 000 razy miesięcznie,  koszt wynosi więc 310 * $0.20 = $62 miesięcznie.Oprócz opłat za liczbę requestów płacimy jeszcze za gigabajto-sekundy. Załóżmy w przybliżeniu, że wywołanie lambdy odczytującej będzie mieściło się w 100 ms, a lambdy zapisującej w 200 ms.Dla getNotes mamy więc:  930 000 000 * 0.1s * 128 / 1024 MB = 11 625 000 GB-s,  od tej wartości możemy odjąć 400 000 GB-s (free tier),  koszt wynosi: 11 225 000 GB-s * $0.00001667 = $187.12 miesięcznie.Odpowiednio dla addNote:  310 000 000 * 0.2s * 128 / 1024 MB = 7 750 000 GB-s,  koszt wynosi 7 750 000 * $0.00001667 = $129.19 miesięcznie.Sumaryczny koszt lambd wynosi $316.31 miesięcznie.Koszty API-GatewayKoszty API-Gateway podzielone są na koszty wykonanych żądań oraz koszty transferu danych. Miesięcznie wykonujemy:  1 000 000 użytkowników * (10 zapisów + 30 odczytów) * 31 dni = 1 240 000 000 wywołań API-Gateway,  za 1 000 000 wywołań płacimy $3.70, koszt wynosi więc 1 240 * $3.70 = $4 588.Przy założeniu, że pojedyncza notatka ma średnio 1 KB transferujemy:  1 000 000 użytkowników * (10 zapisów + 30 odczytów) * 31 dni * 1KB = 1.24 TB danych,  koszt transferu dla pierwszy 10 TB to $0.09 za 1 GB,  koszt wynosi więc 1 240 GB * $0.09 = $111.6 miesięcznie.W sumie koszty API-Gateway to $4 699.6 miesięcznie.PodsumowaniePrzy przyjętych założeniach całkowity koszt naszej przykładowej aplikacji wyniesie $6 002.09 miesięcznie. Wydaje się to sporą kwotą, jeżeli jednak podzielimy to przez liczbę użytkowników, dostajemy koszt około $0.006 na jednego użytkownika miesięcznie. Wynika z tego, że roczne utrzymanie jednego użytkownika zamknie się w $0.08 - jeżeli zaoferujemy użytkownikom cenę $0.99 za roczny dostęp do usługi na jednym użytkowniku zarobimy $0.91 rocznie :-)Wady i zalety rozwiązaniaRozwiązanie oparte o AWS wydaje się być dość proste do wykonania, a także atrakcyjne cenowo. Płacimy tylko za faktyczne zużycie zasobów nie ma więc niebezpieczeństwa, że w przypadku małego zainteresowania naszą aplikacją poniesiemy straty związane z kosztami utrzymania infrastruktury. Dodatkowo w standardzie dostajemy dużą skalowalność rozwiązania. Wady? - W takiej implementacji bardzo mocno uzależniamy się od komponentów AWS, przeniesienie rozwiązania na inną infrastrukturę wiąże się praktycznie z jego przepisaniem.Już wkrótce na naszym blogu pojawi się artykuł omawiający automatyzację tworzenia zasobów na AWS w oparciu o narzędzie terraform. Zapraszamy.",
"url": "/2018/01/17/aws-serverless-programming.html",
"author": "Jakub Wilczewski",
"authorUrl": "/authors/jwilczewski.html",
"image": "jwilczewski.jpg",
"highlight": "/assets/img/posts/2018-01-17-aws-serverless-programming/aws.jpg",
"date": "17-01-2018",
"path": "pl/2018-01-17-aws-serverless-programming"
}
,


"2017-12-01-monitoring-serwerow-i-uslug-html": {
"title": "Jak szybko stworzyć system monitoringu serwerów i usług",
"lang": "pl",
"tags": "monitoring icinga2 grafana influxdb",
"content": "Bieżące informacje o stanie serwerów i działających na nich usług są ważne dla każdego dostawcy rozwiązań IT, z którego korzysta szersza rzesza użytkowników. Najważniejsze oczywiście jest zapewnienie, że wszystkie usługi działają. Jednak samo działanie to jeszcze nie wszystko. Ważny jest także czas odpowiedzi z tych usług. Jeśli czas odpowiedzi wzrósł, lub nawet usługa przestała odpowiadać, przydatna może okazać się znajomość stanu maszyny w zadanym czasie. W tym celu przydatne będą takie dane, jak obciążenie procesora w danym momencie, zużycie pamięci RAM oraz wolnej przestrzeni na dysku. Aby możliwie szybko zareagować, a być może także uniknąć sytuacji, w której usługa przestaje odpowiadać, istotne są ostrzeżenia, które pozwolą zareagować na czas. Niniejszy artykuł ma na celu zaprezentować rozwiązanie, które pozwoli w miarę szybko stworzyć system monitoringu oraz ostrzegania na systemach wyposażonych w system Linux, oparty o takie narzędzia jak “Grafana”, “Icinga2” oraz “InfluxDB”.Przykładowy dashboard prezentujący stan maszyny (obciążenie, zużycie ramu i dysku), jak i działających na nim usługWymagania wstępneNa maszynie, która pełnić będzie rolę serwera monitoringu należy:  Zainstalować Dockera (https://docs.docker.com/engine/installation/)  Zainstalować Docker-Compose (https://docs.docker.com/compose/install/)  Należy upewnić się, że sieci tworzone przez Dockera będą miały dostęp do maszyn, które chcemy monitorować. Przypadkiem kiedy takiego dostępu może nie być, jest sytuacja w której monitorowane maszyny znajdują się za siecią VPN. Jednym z rozwiązań w takim przypadku, które odbywa się kosztem słabszej separacji kontenera od hosta, jest edycja pliku docker-compose.yml z punktu piątego niniejszego rozdziału, tak aby kontenery korzystały z sieci hosta.  W kolejnym kroku powinniśmy wygenerować parę kluczy SSH, która posłuży do uwierzytelniania się na monitorowanych maszynach.  Na sam koniec pozostaje pobranie następującego projektu: https://github.com/aswarcewicz/monitoringNa maszynach, które będą monitorowane należy dodać do zaufanych klucz publiczny SSH wygenerowany na serwerze monitoringu w kroku czwartym, tak aby serwer monitorujący mógł logować się przez SSH bez podawania hasła. Logowanie przez SSH wykonywane jest celem pobrania stanu maszyny (obciążenie, zużycie RAMu oraz dysku).Uruchamiamy kontenery  Po pobraniu projektu z githuba, należy umieścić klucz prywatny serwera wewnątrz struktury pobranego projektu w katalogu “icinga2/ssh_keys”. Klucz prywatny zazwyczaj nosi nazwę id_rsa i nie posiada rozszerzenia *.pub.  Tworzymy i uruchamiamy wszystkie kontenery następującym poleceniem wykonanym wewnątrz pobranego projektu: “docker-compose up”  W terminalu obok wykonujemy polecenie “docker exec -it influxdb bash”, dzięki któremu znajdziemy się wewnątrz kontenera influxdb.          W celu utworzenia bazy danych pod zbierane metryki uruchamiamy w kontenerze następujący program “/opt/influxdb/usr/bin/influx”      Następnie wykonujemy polecenie tworzące bazę danych, która będzie przechowywała dane przez dwa tygodnie. Długość przechowywania danych można dobrać dowolnie, w tym także pominąć ich usuwanie po zadanym czasie. “CREATE DATABASE icinga2 WITH DURATION 2w”. Po wykonaniu tego polecenia można już opuścić kontener influxdb.        W tym kroku należy przejść do konfiguracji Icinga2, tak aby zapisywała zebrane dane do wcześniej utworzonej bazy InfluxDB. Uruchomienie polecenia z punktu drugiego stworzyło w katalogu projektu folder o nazwie “data”, wewnątrz którego znajdują się niezbędne konfiguracje oraz bazy danych. W celu konfiguracji Icinga2 należy wyedytować plik “./data/config/icinga2/features-enabled/influxdb.conf” tak, aby wskazywał na właściwą bazę danych. Jeśli korzystamy z sieci utworzonej przez dockera, w linijce odpowiedzialnej za hosta powinien znaleźć się wpis “influxdb”, gdyż pod takim aliasem dostępny będzie kontener z bazą. Natomiast, jeśli korzystamy z sieci host’a, w tej samej linijce powinien znaleźć się wpis localhost. Pozostałe linijki wystarczy odkomentować.  Uruchamiamy ponownie całość. Można dokonać tego poprzez użycie kombinacji klawiszy “CTRL+C” na terminalu z uruchomionym poleceniem “docker-compose up” lub poprzez uruchomienie innego terminala w katalogu projektu oraz wykonanie polecenia “docker-compose stop”. Po zatrzymaniu kontenerów należy oczywiście uruchomić je ponownie za pomocą “docker-compose up” lub w tle “docker-compose start”.Konfiguracja zbierania danychW tej części skonfigurujemy Icingę tak, aby zbierała dane potrzebne dla naszego monitoringu. Wszystkie konfiguracje powinny znaleźć się w katalogu projektu “./data/config/icinga2/conf.d” lub w jego podkatalogach, aby zostały zaczytane przez Icingę. Warto zwrócić uwagę na domyślną konfigurację komend zbierających dane o obciążeniu, zużyciu pamięci RAM czy dysku. Komendy te domyślnie wykorzystują użytkownika “root” do logowania po SSH, jeśli klucze wygnerowane w podpunkcie 4. “wymagań wstępnych” przeznaczone są dla innego użytkownika, to warto dokonać tej zmiany globalnie już w samej konfiguracji komend.Aby rozpocząć zbierane danych przez Icingę należy najpierw skonfigurować usługi, przykładowa konfiguracja poniżej:apply Service &quot;router_version_https&quot; {  import &quot;generic-service&quot;  display_name = &quot;Router (version) - HTTPS&quot;  assign where &quot;router&quot; in host.vars.services  ignore where host.vars.router.https.port == &quot;&quot;  check_command = &quot;http&quot;  vars.http_uri = &quot;/router/version/getVersionNumber&quot;  vars.http_vhost =   vars.http_port =   vars.http_ssl = &quot;true&quot;}apply Service &quot;servicemix_tcp_https&quot; {  import &quot;generic-service&quot;  display_name = &quot;ServiceMix - TCP (https)&quot;  assign where &quot;router&quot; in host.vars.services  ignore where host.vars.router.https.port == &quot;&quot;  check_command = &quot;tcp&quot;  vars.tcp_address =   vars.tcp_port = }A następnie dodać konfigurację maszyn (hostów), które z tych usług korzystają, przykład poniżej:object Host &quot;Consdata (formdev)&quot; {    import &quot;generic-host&quot;    address = &quot;formdev.eximee.consdata.local&quot;    vars.os = &quot;Linux&quot;    vars.services = [&quot;check-load&quot;, &quot;check-memory&quot;, &quot;check-disk-space&quot;, &quot;router&quot;]    /* Router */    vars.router.https.port = &quot;9002&quot;}Po ponownym uruchomieniu Icinga powinna zaczytać konfigurację i co około 30 sekund zapisywać metryki do InfluxDB. Ewentualne problemy powinny znaleźć się w logach, do których dostęp można uzyskać poprzez komendę “docker logs -f icinga2”.Wizualizacja danychKolejnym etapem jest skonfigurowanie Grafany, aby zaczęła wyświetlać dane zbierane przez Icingę. Grafanę znajdziemy na porcie 3000 na maszynie, na której uruchomiono pobrany z GitHuba projekt. Domyślny login to admin, domyślne hasło to także admin. Po zalogowaniu, podobnie jak w przypadku Icingi, konfigurujemy najpierw źródło danych, jeśli korzystamy z sieci Docker’a źródło danych będzie znajdowało się na hoście o nazwie influxdb, w przypadku korzystania z sieci hosta będzie to localhost. Następnie możemy przejść do konfiguracji Dashboard’ów czyli tablic agregujących wykresy. Najpierw dodajemy wiersz, później wykres, a następnie przechodzimy do edycji (konfiguracji) wykresu.Konfiguracja wyświetlania metryk dla wykresuTak naprawdę wszystko tutaj powinno dać się wyklikać, jeśli brakuje jakiejś metryki, a minęło więcej niż 30-60 sekund od startu Icingi z nową konfiguracją, oznacza to błąd w konfiguracji. W takim przypadku istnieje szansa, że znajdziemy jakieś informacje po wykonaniu komendy “docker logs -f icinga2”. W przypadku konfiguracji alertów istotne może okazać się wypełnienie miejsc, w których brakuje danych. Na powyższym screenie wybrano “none” i taką opcję zalecałbym na początek.Powiadomienia w przypadku problemu z usługamiKolejnym etapem będzie konfiguracja powiadomień. Dla odmiany od najczęściej wykorzystywanych powiadomień mailowych w tym przypadku skorzystamy z komunikatora Slack.  Na początku musimy skonfigurować SlackBota, a dokładniej “Incoming Webhook”. W przypadku Consdaty można tego dokonać pod następującym adresem: https://consdata.slack.com/apps/manage/custom-integrations  Po otrzymaniu adresu URL należy go dodać w Menu “Alerting-&amp;gt;Notification Channels”. W razie potrzeb można swobodnie wykorzystać jeden URL, aby skonfigurować kilka kanałów:    Ekran edycji kanału powiadomień. W tym przypadku jako kanał dostarczania powiadomień wybrano komunikator Slack    Po tym pozostaje już tylko konfiguracja powiadomień dla każdego z wykresów z osobna    Konfiguracja wywołania powiadomienia na wykresie, dla którego wcześniej skonfigurowano zbieranie metryk    Konfiguracja wywołania powiadomienia na wykresie - wybór sposobu wysyłki alertu    Od tej chwili, w przypadku wystąpienia problemów, możemy spodziewać się komunikatów podobnych do tego poniżej    Przykładowy alert dostarczany na komunikator Slack  PodsumowaniePołączenie Grafany, jako warstwy prezentacji i ostrzegania, oraz Icingi, jako narzędzia do zbierania metryk, jest dosyć łatwym i szybkim w zestawieniu systemem do monitorowania serwerów. Warto zwrócić uwagę także na wiele opcji budowania własnych tablic z wykresami. Poza grupowaniem wykresów w wiersze z opcjami zwijania, ustalania różnych rozmiarów czy umieszczania na tablicach kontrolek, które prezentują dane w innych formach niż wykresy (np. w formie tabel lub pojedynczych wartości), wartą uwagi jest funkcjonalność zmiennych, których można używać na wykresach. Zmienne można definiować za pomocą wartości wpisanych na stałe, lub w formie zapytań. Jednym z zastosowań takiej funkcjonalności jest dynamiczna tablica zasilana metrykami z różnych serwerów w zależności od wybranej opcji.Dashboard, na którym zaprezentowano przykładowe użycie zmiennych. W tym wypadku jeden Dashboard jest w stanie przełączać serwer, z którego wyświetlane są metrykiNiestety opcja ta nie jest pozbawiona wad i w bieżącej wersji Grafany (4.6.2), chcąc skorzystać ze zmiennych, pozbawiamy się możliwości definiowania powiadomień.",
"url": "/2017/12/01/monitoring-serwerow-i-uslug.html",
"author": "Adrian Swarcewicz",
"authorUrl": "/authors/aswarcewicz.html",
"image": "aswarcewicz.jpg",
"highlight": "/assets/img/posts/2017-12-01-monitoring-serwerow-i-uslug/monitoring-systemow.jpg",
"date": "01-12-2017",
"path": "pl/2017-12-01-monitoring-serwerow-i-uslug"
}
,


"2017-08-01-gatling-html": {
"title": "Gatling! “Odłamkowym ładuj!”, czyli jak strzelać do aplikacji",
"lang": "pl",
"tags": "gatling tests",
"content": "W każdym projekcie pojawia się moment, w którym pada stwierdzenie: “A co z wydajnością? Damy radę na produkcji?”. I wtedy cały zespół zastanawia się, ile tak naprawdę zniesie owoc ich wielomiesięcznych prac. Kod po wnikliwych review, poprawkach architektonicznych i projektowych wygląda wspaniale w repozytorium, ale analizując go, trudno stwierdzić czy aplikacja wytrzyma ruch na produkcji oraz gdzie są najsłabsze ogniwa. Trzeba to sprawdzić w praktyce. Przygotowujemy środowisko zasobami zbliżone do produkcji albo odpowiednio wyskalowane i klikamy. A co tam, zespół duży, zacięcie klika i osiągamy ruch 5 użytkowników. Szybka analiza logów i wszystko jest OK. Ale zaraz, to nic nam nie daje. Nadal nie zidentyfikowaliśmy najsłabszego ogniwa. Zautomatyzujmy zatem użytkowników w systemie i wykonajmy symulację bardzo dużej liczby użytkowników. Pierwsze co przychodzi na myśl, to…Nasz stary znajomy - Apache JMeter. Aplikacja bardzo rozbudowana, gdzie można tworzyć skomplikowane przepływy użytkowników. Narzędzie bogate w liczne funkcje, ale jak dla mnie dość toporne. Pisanie w nim nowych scenariuszy zawsze idzie jakoś ciężko. Praca jest lżejsza, jeśli można oprzeć się na starych testach. Trudno jednak czeprać przyjemność z pracy tym narzędziem.Również w moim zespole, gdy pojawiła się kwestia testów wydajnościowych, pierwszą sugestią było użycie JMeter’a. Głównie z racji dostępnych scenariuszy dla podobnego przepływu użytkownika. W trakcie dyskusji na temat wydajności padło, że w firmie, jeden z zespołów użył narzędzia Gatling, które sprawdziło się dużo lepiej, niż narzędzie Apacha. Powód był bardzo prosty, scenariusze się programuje, a nie klika w GUI. Przez co są czytelniejsze i łatwiejsze do utrzymania w repo. Co prawda, w Gatlingu koduje się w języku scala, ale nawet jeśli, ktoś nigdy nie programował w tym języku, to bez problemu sobie poradzi. Podstawy w zupełności wystarczą. Nie jest potrzebna skomplikowana wiedza, o czym przekonują nawet twórcy na swojej stronie.Poniżej spróbuję przybliżyć owe narzędzie posługując się prostym przykładem.Co by tu potestować?Na początku musimy przygotować środowisko do testów. Wystawimy jakiś prosty serwis z kilkoma metodami. Najszybciej będzie skorzystać ze springboot’a i zainicjować projekt. Wchodzimy na https://start.spring.io, klikamy co nas interesuje i ściągamy projekt. Importujemy i możemy zaczynać. Dodajemy prosty serwis.package pl.consdata.server.controller;import org.springframework.web.bind.annotation.*;import java.security.SecureRandom;@RestControllerpublic class SimpleWaitController {  private final String GENERIC_RESPONSE = &quot;{\\&quot;result\\&quot;: %1$d}&quot;;  @GetMapping(path = &quot;/getWaitTime&quot;)  @ResponseBody  public String getWaitTime() {     SecureRandom random = new SecureRandom();     return String.format(GENERIC_RESPONSE, 5000 + random.nextInt(5500));  }  @PostMapping(path = &quot;/wait&quot;)  @ResponseBody  public String waitForNextStep(Integer waitTime) {     try {        Thread.sleep(waitTime);     } catch (InterruptedException e) {        e.printStackTrace();     }     return String.format(GENERIC_RESPONSE, waitTime);  }  @GetMapping(path = &quot;/stop/{waitTime}&quot;)  public String stop(@PathVariable Integer waitTime) {     return &quot;user end after: &quot; + waitTime;  }}Funkcjonalność poszczególnych metod nie jest skomplikowana, ale do testu wystarczy. Zakładamy następujący przepływ użytkownika (każdy użytkownik postępuje tak samo):  Pobiera czas przetwarzania  Wywołuje przetwarzanie  Czeka 5 sekund - po stronie użytkownika  Opuszcza ten skomplikowany procesSkoro aplikacja jest gotowa, to spróbujmy zastanowić się: co jest słabym ogniwem? Bystre oko zauważy, że endpoint ‘/wait’ może sprawiać problemy, gdyż zawiera metodę ‘sleep’. OK, zgodzę się, ale pomimo tego - jaki ruch ta aplikacja wytrzyma i co pierwsze padnie? Tutaj odpowiedź już nie jest już taka oczywista. Przygotujmy zatem test wydajnościowy i sprawdźmy.Zaprogramujmy scenariusz wydajnościowyPrzejdźmy zatem do naszego zadania, czyli napisania scenariusza pokrywającego flow użytkownika. Zanim jednak do tego przystąpimy, musimy pobrać naszego frameworka. Skorzystamy z mojego ulubionego sposobu, czyli zaprzęgniemy mavena, aby się tym zajął. W głównym pom’ie dodajemy następujące konfiguracje:  biblioteka wykorzystywana przez Gatlinga      &amp;lt;dependencies&amp;gt;     &amp;lt;dependency&amp;gt;         &amp;lt;groupId&amp;gt;io.gatling.highcharts&amp;lt;/groupId&amp;gt;         &amp;lt;artifactId&amp;gt;gatling-charts-highcharts&amp;lt;/artifactId&amp;gt;         &amp;lt;version&amp;gt;2.2.4&amp;lt;/version&amp;gt;         &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;     &amp;lt;/dependency&amp;gt;  &amp;lt;/dependencies&amp;gt;        plugin odpowiedzialny za uruchamianie z poziomu mavena      &amp;lt;plugin&amp;gt;     &amp;lt;groupId&amp;gt;io.gatling&amp;lt;/groupId&amp;gt;     &amp;lt;artifactId&amp;gt;gatling-maven-plugin&amp;lt;/artifactId&amp;gt;     &amp;lt;version&amp;gt;2.2.4&amp;lt;/version&amp;gt;     &amp;lt;configuration&amp;gt;         &amp;lt;simulationsFolder&amp;gt;src/main&amp;lt;/simulationsFolder&amp;gt;         &amp;lt;simulationClass&amp;gt;scala.simulation.SimpleWaitSimulation&amp;lt;/simulationClass&amp;gt;     &amp;lt;/configuration&amp;gt;     &amp;lt;executions&amp;gt;         &amp;lt;execution&amp;gt;             &amp;lt;goals&amp;gt;                 &amp;lt;goal&amp;gt;execute&amp;lt;/goal&amp;gt;             &amp;lt;/goals&amp;gt;         &amp;lt;/execution&amp;gt;     &amp;lt;/executions&amp;gt;  &amp;lt;/plugin&amp;gt;      Dodatkowo, w konfiguracji dodajemy ścieżkę ze źródłami oraz domyślną klasę naszej symulacji. Klasa ta musi dziedziczyć po obiekcie ‘Simulation’ z pakietu Gatlinga. Po stworzeniu klasy ‘SimpleWaitSimulation’, uruchamiamy symulację w konsoli poleceniem:mvn gatling:executePowinniśmy dostać błąd:Caused by: java.lang.IllegalArgumentException: requirement failed: No scenario set upBłąd ten informuje nas, że nasza symulacja nie zawiera scenariusza, co oczywiście jest prawdą na tym etapie, ale potwierdza nam działanie szkieletu symulacji. Możemy zatem przejść dalej i skupić się na zakodowaniu naszego testu. Uruchamiamy wcześniej napisaną aplikację do testowania i ruszamy.W pierwszej kolejności definiujemy podstawy komunikacji naszego testu z serwerem.Aplikacja uruchamia się na standardowym adresie i porcie, zatem takie wartości przekazujemy. W Gatlingu za konfigurację komunikacji odpowiada klasa ‘http’. Ustawiamy adres bazowy, pod którym szukać należy naszej aplikacji. Nasz test będzie symulował uderzenia z przeglądarki, zatem ustawiamy odpowiednie nagłówki wykorzystywane przy requestach. Powstaje następujący kod:val httpConf = http    .baseURL(&quot;http://localhost:8080&quot;)    .acceptHeader(&quot;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&quot;)    .acceptEncodingHeader(&quot;gzip, deflate&quot;)    .acceptLanguageHeader(&quot;en-US,en;q=0.5&quot;)    .userAgentHeader(&quot;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:50.0) Gecko/20100101 Firefox/50.0&quot;)Teraz możemy zaprogramować scenariusz, czyli krok po kroku wykonać to, co wykonuje symulowany użytkownik. Za tworzenie scenariusza odpowiada polecenie ‘scenario’. Na początku powołajmy instancję tego obiektu.val scn = scenario(&quot;Simple Wait simulation&quot;)W tym momencie mamy zalążek scenariusza i możemy zacząć dodawać poszczególne wywołania. W pierwszym kroku użytkownik wywołuje metodą GET endpointa ‘/getWaitTime’, a w odpowiedzi otrzymuje dane, które musi zapamiętać i wykorzystać w kolejnym etapie. Praca z Gatlingiem jest bardzo wygodna. Dostajemy masę użytecznych poleceń, dzięki którym możemy wykonywać różne operacje. Zarówno na wejściowych, jak i wyjściowych danych z testu. Na początku dodajemy wywołanie GET:.exec(http(&quot;Take waiting time&quot;)    .get(&quot;/getWaitTime&quot;))Całkiem proste. Powyższy kod wywoła tylko wskazanego endpointa. Brakuje nam pobrania odpowiedzi i zapamiętania wartości. Dodatkowo powinniśmy dostać kod statusu http równy 200. W naszym przypadku oznacza to, że wywołanie powiodło się, a serwer wyśle do nas jsona z polem o nazwie ‘result’. Wartość tę musimy zapamiętać i wykorzystać do dalszego procesowania użytkownika. Uzupełnijmy nasze proste wywołanie o niezbędne elementy:val scn = scenario(&quot;Simple Wait simulation&quot;)    .exec(http(&quot;Take waiting time&quot;)        .get(&quot;/getWaitTime&quot;)        .check(status.is(200))        .check(jsonPath(&quot;$.result&quot;).saveAs(&quot;waitTime&quot;))    )Podobnie jak wcześniej skorzystaliśmy z wbudowanych narzędzi Gatlinga. ‘Check’ operuje na odpowiedzi wszystkich requestów. Metodą ‘status’ sprawdzamy kod odpowiedzi, czyli oczekujemy wartości 200. Metodami ‘jsonPath’ wyciągamy niezbędne informacje z odpowiedzi, a poleceniem ‘saveAs’ zapamiętamy te dane. Warto wspomnieć, iż Gatling do każdego użytkownika tworzy sesję użytkownika, do której możemy zarówno zapisywać, jak i odczytywać wartości. Istnieje także możliwość odczytywania wielu wartości w ramach jednego żądania, wówczas musimy tylko skopiować polecenie ‘check’ i odpowiednio ustawić, co i jak chcemy zapisać.Idąc dalej, użytkownik wywołuje endpoint ‘/wait’ przekazując wartość z pierwszego żądania, a otrzymując inną wartość, którą także zapamiętujemy. Postępując zgodnie z opisem, kodujemy:.exec(http(&quot;Wait for&quot;)    .post(&quot;/wait&quot;)    .formParam(&quot;&quot;&quot;waitTime&quot;&quot;&quot;, &quot;${waitTime}&quot;)    .check(status.is(200))    .check(jsonPath(&quot;$.result&quot;).saveAs(&quot;realWaitTime&quot;))    )Nowym elementem, jaki tutaj wykorzystaliśmy, jest polecenie ‘formParam’. Jak nazwa sugeruje, służy ono do przekazywania parametrów do wywołania. Poza nazwą, w formularzu podajemy wartość: bezpośrednio lub za pomocą zmiennej. W tym przypadku odczytaliśmy z sesji wcześniej zapisaną wartość.W kolejnym etapie użytkownik czeka kilka sekund, zanim wywoła następną operację. Aby to osiągnąć, w prosty sposób możemy wykorzystać wbudowaną metodę ‘pause’. Jako parametr przyjmujemy wartość, oznaczającą ilość czasu, którą symulowany użytkownik ma czekać przed wykonaniem następnego kroku. Polecenie proste, ale użyteczne. Możemy w ten sposób symulować np. wypełnianie formularza czy klikanie w jakiś przycisk..pause(5 seconds)W ostatnim kroku użytkownik wywołuje tak, jak na początku polecenie GET, ale tym razem przekazując wartość. W odpowiedzi nie otrzymuje JSONa, ale prosty ciąg znaków. Rezultat odpowiedzi sprawdzimy czy zawiera interesującą nas wartość przekazaną w żądaniu..exec(http(&quot;User stop&quot;)    .get(&quot;/stop/${realWaitTime}&quot;)    .check(status.is(200), bodyBytes.saveAs(&quot;endMessageBody&quot;))    .check(substring(&quot;${realWaitTime}&quot;).exists))Poza wykorzystywanymi poprzednio poleceniami, użyliśmy tutaj kolejnej metody Gatlinga, tym razem do operacji na stringach. ‘Substring’ pozwala w prosty sposób operować na ciągach znakó,w np. znajdując podaną na wejściu frazę. Serwer powinien wysłać nam “stringa” zawierającego przekazany na wejściu parametr. Dlatego też weryfikujemy, czy otrzymana odpowiedź zawiera ów wartość.Cały przepływ użytkownika mamy już zakodowany. Na końcu, zbierzmy wszystkie kroki w jednym miejscu. Każde polecenie do wykonania operacji użytkownika zwraca instancję scenariusza, zatem w prosty sposób możemy agregować polecenia. W efekcie nasz wynikowy scenariusz wygląda następująco:val scn = scenario(&quot;Simple Wait simulation&quot;)    .exec(http(&quot;Take waiting time&quot;)        .get(&quot;/getWaitTime&quot;)        .check(status.is(200))        .check(jsonPath(&quot;$.result&quot;).saveAs(&quot;waitTime&quot;))    )    .exec(http(&quot;Wait for&quot;)        .post(&quot;/wait&quot;)        .formParam(&quot;&quot;&quot;waitTime&quot;&quot;&quot;, &quot;${waitTime}&quot;)        .check(status.is(200))        .check(jsonPath(&quot;$.result&quot;).saveAs(&quot;realWaitTime&quot;)))        .pause(5 seconds)    )    .exec(http(&quot;User stop&quot;)        .get(&quot;/stop/${realWaitTime}&quot;)        .check(status.is(200), bodyBytes.saveAs(&quot;endMessageBody&quot;))        .check(substring(&quot;${realWaitTime}&quot;).exists)    )W następnym kroku zajmiemy się konfiguracją i przygotowaniem ruchu użytkowników.Pierwsze strzałyMamy gotowy scenariusz, aby jednak nasza symulacja została poprawnie uruchomiona, trzeba ją jeszcze skonfigurować. W tym celu musimy wywołać metodę ‘setUp()’, która połączy nam wszystkie elementy tzn.: komunikację, scenariusz i ruch użytkowników. Nie jest to skomplikowana operacja. W jej efekcie powstanie jedna linijka kodu:setUp(scn.inject(atOnceUsers(1)).protocols(httpConf))Na początek, w celu sprawdzenia czy wszystko poprawnie działa, uruchomimy test tylko z jednym użytkownikiem. Użyjemy tutaj polecenia ‘atOnceUsers(1)’, które symuluje właśnie określoną liczbę użytkowników na start testu. Wszystko gotowe, zatem możemy uruchamiać. Odpalamy konsolę i uruchamiamy polecenie:mvn gatling:executeJeśli wszystko poszło OK, to dostajemy:[INFO] BUILD SUCCESSPrzeglądamy logi i weryfikujemy czy otrzymaliśmy następujące wyniki:--- Requests ------------------------------------------------------------------Global (OK=3 KO=0 )Take waiting time (OK=1 KO=0 )Wait for (OK=1 KO=0 )User stop (OK=1 KO=0 )Jeśli mamy takie wypiski, to wszystko jest w jak najlepszym porządku.Przy takiej konfiguracji testu mamy w sumie 3 uderzenia do aplikacji, zatem ciężko wnioskować i mówić tu o jakichś wynikach do analizy. Testowo możemy spojrzeć sobie na raport, który wygenerowany został automatycznie na koniec testu. Wystarczy poszukać ‘Please open the following file:’ gdzie znajduje się strona html z wszystkimi wynikami naszego testu. Dokładniej przyjrzymy się temu, gdy uderzeń będzie o wiele więcej. Ustalmy zatem większy ruch.“Och, ilu tych użytkowników”Ruch jednego użytkownika nie jest w stanie zbadać nam wydajności aplikacji. Dlatego musimy zastanowić się, jaki ruch chcemy wygenerować. Oczywiście, jeśli mowa o prawdziwych aplikacjac,h to musimy wziąć pod uwagę charakterystykę działania naszej aplikacji w środowisku produkcyjnym. Czy jest to ruch liniowy, skokowy, impulsowy itp. I to właśnie rozkład ruchu na produkcji powinien determinować, w jaki sposób generujemy ruch użytkowników w systemie w trakcie testu.Gatling ma kilka sposobów generowania ruchu. Ponieważ metod jest wiele, warto spojrzeć do dokumentacji - http://gatling.io/docs/current/general/simulation_setup/. My zakładamy, że w każdej sekundzie do systemu będzie wpadać określona liczba nowych użytkowników, postępujących według poprzednio opisanego scenariusza. Przy konfiguracji symulacji wykorzystamy dwie metody:  ’nothingFor()’ - do rozładowania poprzedniego ruchu  ‘constantUsersPerSec() during() randomized’ - do symulowania określonej liczby użytkowników przez ustalony czas.W początkowej fazie należy wykonać wstępne testy i określić wartości w jakich będziemy się poruszać. Nie ma sensu przeprowadzać godzinnego testu, jeśli aplikacja “wywali się” w ciągu pierwszych sekund. Po wykonaniu takich szybkich testów, możemy przystąpić do finalnej konfiguracji.Dobrym rozwiązaniem jest przeprowadzanie testów w ramach kilku iteracji. Każda z nich składa się z fazy obciążenia aplikacji użytkownikami, a następnie czasu potrzebnego na zakończenie przetwarzania wszystkich użytkowników. Wówczas możemy uruchomić następne powtórzenie, ale zwiększając liczbę użytkowników. O tym jak przeprowadzać testy wydajnościowe, można by napisać następny artykuł czy nawet książkę.Wróćmy zatem do naszej symulacji. Wykonamy pięć iteracji, każda będzie się składać z obciążenia użytkownikami trwającym 100 sekund, następnie przez 60 sekund będziemy czekać zanim uruchomimy następne powtórzenie. Zaczniemy od 20 nowych użytkowników na sekundę i będziemy zwiększać ich liczbę o kolejnych 10 w następnych iteracjach. Podobnie jak w przypadku scenariusza, możemy agregować poszczególne fazy testu. Dla powyższych założeń mamy następującą konfigurację:setUp(scn.inject(constantUsersPerSec(20) during(100 seconds) randomized,nothingFor(60 seconds),constantUsersPerSec(30) during(100 seconds) randomized,nothingFor(60 seconds),constantUsersPerSec(40) during(100 seconds) randomized,nothingFor(60 seconds),constantUsersPerSec(50) during(100 seconds) randomized,nothingFor(60 seconds),constantUsersPerSec(60) during(100 seconds) randomized).protocols(httpConf))Konfiguracja gotowa, czas na uruchomienie.Wielkie strzelanieW końcu dotarliśmy do momentu, w którym możemy sprawdzić, jak zachowuje się nasza aplikacja przy dużym obciążeniu. Wszystko gotowe, uruchamiamy symulację i idziemy na kawę. Jak już wypijemy, jest szansa, że nasz test się zakończył. Wówczas w logach, tak jak poprzednio, otrzymamy:[INFO] BUILD SUCCESSNastępnie szukamy, gdzie wygenerowany został raport: ‘Please open the following file:’. Odpalamy i naszym oczom pojawia się automatycznie wygenerowany raport.Przejrzyjmy otrzymane wyniki. Pierwsze błędy zaczęły się pojawiać przy ruchu 50 nowych użytkowników na sekundę. Całkiem nieźle. Widać jednak, że aplikacja przetwarzała cały ruch tylko przy 20 nowych użytkownikach na sekundę. Już przy kolejnej iteracji widać tendencję wzrostową aktywnych użytkowników. Oznacza to, że aplikacja nie radziła sobie z całym ruchem, jaki był generowany i część żądań była kolejkowana. Przy kolejnych fazach widać, że tendencja się jeszcze zaostrzała i przy 50 użytkownikach czas oczekiwania na aplikację był tak długi, że “leciały” timeouty. Wiemy oczywiście z czego to wynika. Użycie “sleep” z dużymi wartościami jest mało wydajne. Jeśli to usuniemy, otrzymamy dużo większe wartości.Czy nie jest wspaniałe, że Gatling wygenerował za nas wyniki i pokazał w graficznej formie? Jak dla mnie super. Możemy sprawdzić podstawowe dane, takie jak liczbę aktywnych użytkowników, czasy odpowiedzi, liczbę żądań i odpowiedzi w czasie. Dodatkowo, możemy wybrać jeden krok i przeanalizować w izolacji w ten sam sposób. Poza tym, przedstawione mamy statystyki w liczbach, takie jak, minimalny i maksymalny czas, średnia, odchylenie standardowe czy liczba requestów z podziałem na zakończone sukcesem i błędem. Gatling przedstawia także czasy poszczególnych udziałów procentowych obsługi żądań. Dostajemy zatem informacje ile trwało 50%, 75%, 95% i 99% wywołań. Ten prosty raport naprawdę sprawia, że praca z Gatlingiem i testami wydajnościowymi jest przyjemnością.Testowanie wydajności poprzez GatlingPodsumowując moje doświadczenia w pracy z Gatlingiem, mogę stwierdzić, że wybór tego narzędzia był “strzałem w dziesątkę”. Jego istotną zaletą jest to, że scenariusze testowe są pisane jako zwykły kod. Nie ma problemu, aby scenariusz przygotowywało więcej osób. Problemy z “mergowaniem” zazwyczaj są minimalne. Kod jest czytelny i nie ma potrzeby pisania dodatkowej dokumentacji.Gatling zawiera ponadto masę pomocniczych poleceń. Na stronie http://gatling.io/ dostępna jest użyteczna dokumentacja, w której w przejrzysty sposób wyjaśniono dostępne funkcje. Możemy korzystać bezpośrednio z poleceń JMX’a naszego serwera, czy tworzyć komunikaty na kolejkach JMS. Produkt jest rozwijany. Powstają nowe wersje, a błędy są rozwiązywane. W przypadku problemów, można skorzystać z pomocy społeczności odpowiedzialnej za rozwój narzędzia.Jak każde narzędzie, również Gatling ma ograniczoną liczbę funkcji. Jeśli w dokumentacji nie znajdziemy potrzebnych poleceń, możemy spróbować zapisać np. dane w sesji i posiłkować się kodem napisanym przez nas w scali. Daje nam to ogrom możliwości. Zapewnia elastyczność w rozwiązywaniu specyficznych przypadków, które niestety spotykamy praktycznie codziennie.Kolejnym ważnym ułatwieniem wykonywania testów jest raport końcowy. Automatycznie generowane wykresy i podstawowe statystyki bardzo przydają się w początkowej i końcowej fazie testu. Jeśli chcemy, szybko przetestować aplikację z ogromnym ruchem, nie ma problemu -zmieniamy 2-3 linijki i odpalamy. Po kilku chwilach mamy dostępne wyniki i wiemy, czy aplikacja daje radę, czy już nie. Tej funkcjonalności mi zawsze brakowało np. w JMeterze, gdzie raportem było trzeba zająć się samemu po testach. W przypadku Gatlinga wyniki możemy bez problemu dołączyć do dokumentacji produktu, jako potwierdzenie naszej ciężkiej i dobrze wykonanej pracy przy pisaniu aplikacji i testach wydajnościowych. Polecam każdemu wypróbowanie Gatlinga do testów wydajnościowych. Mi narzędzie to bardzo mocno przypadło do gustu.",
"url": "/2017/08/01/gatling.html",
"author": "Błażej Baron",
"authorUrl": "/authors/bbaron.html",
"image": "bbaron.jpg",
"highlight": "/assets/img/posts/2017-08-01-gatling/wydajnosc_aplikacji.jpg",
"date": "01-08-2017",
"path": "pl/2017-08-01-gatling"
}
,


"2017-02-17-absolutne-importowanie-zaleznosci-w-angular-cli-html": {
"title": "Absolutne importowanie zależności w Angular CLI",
"lang": "pl",
"tags": "angular",
"content": "Odpowiedzialny programista tworząc aplikacje przestrzega powszechnie uznanych zasad tworzenia oprogramowania. Jedną z takich zasad jest Single Responsibility Principle, która uczy nas, że każdy moduł powinien mieć jedno, jasno zdefiniowane zadanie. Przenosząc tę zasadę na strukturę aplikacji powinniśmy, co najmniej, wydzielić każdą tworzoną klasę do osobnego pliku. O ile świat JavaScript’u próbował wielokrotnie zmierzyć się z problemem modularyzacji aplikacji, to jednak dopiero standard ES6 pozwolił w pełni zaadaptować takie podejście.Modularyzacja w ES6W ramach standardu ES6 wprowadzono pojęcie modułu oraz standardowego sposobu wyrażania zależności za pomocą polecenia import.Za moduł przyjmuje się pojedyczny plik. Wszystkie elementy w nim zdefiniowane są jego własnością i nie zabrudzają globalnej przestrzeni aplikacji. Elementy, które chcemy udostępnić innym modułom należy jawnie wskazać przez oznaczenie ich poleceniem export. Wyeksportowane elementy możemy importować i używać w modułach zależnych poleceniem import.Przykładowo, możemy zdefiniować zależność pomiędzy modułami:service.tsexport class Service {}component.tsimport {Service} from &#39;./service&#39;;export class Component {\tprivate service: Service;}Problemy z importowaniem zależnościStandardowo importowane moduły są wyszukiwane względem importującego pliku. W przypadku nietrywialnej struktury może to doprowadzić do pogroszenia czytelności aplikacji.Wyobraźmy sobie strukturę kodu:src  \\- user    \\- details      user-details.component.ts  \\- authentication    \\- authentication.service.tsWówczas zawartość pliku user-details.component.ts mogłaby wyglądać:component.tsimport {AuthenticationService} from &#39;../../authentication/authentication.service&#39;;export class UserDetailsService {\tprivate service: AuthenticationService;}Utrzymywanie względnych ścieżek importowanych modułów jest trudne. Czytelność istniejących importów jest wątpliwa, a dopisywanie nowych niewygodne. Rozwiązaniem tego problemu może być stosowanie absolutnych ścieżek do modułów, liczonych względem głównego katalogu ze źródłami. Przy takim podejściu dopisywanie nowych importów jest proste (widząc strukturę aplikacji), a czytając jesteśmy w stanie szybko się zorientować jakie zależności właściwie wciągamy.Użycie absolutnych ścieżek zależnościNa szczęście zalecane podejście do pisania aplikacji w Angular 2 zakłada stosowanie kompilatora TypeScript, a ten od dłuższego czasu wspiera stosowanie absolutnych ścieżek przy importowaniu zależności. Dodatkowo, również Angular CLI prawidłowo obsluguje bundlowanie zasobów zbudowanych z wykorzystaniem absolutnych ścieżek.W tym celu należy zmodyfikować plik konfiguracyjny kompilatora TypeScript. W pliku tsconfig.json dopisujemy atrybut baseUrl: “.”{  &quot;compilerOptions&quot;: {    &quot;baseUrl&quot;: &quot;.&quot;  }}Od teraz możemy we wszystkich miejscach importować zależności podając ich absolutne ścieżki:import {AuthenticationService} from &#39;authentication/authentication.service&#39;;export class UserDetailsService {\tprivate service: AuthenticationService;}PodsumowanieWprowdzenie modularyzacji do standardu ES6 to ogromny krok w stronę poprawy jakości aplikacji. Jednak podawanie względnych ścieżek do zależności może szybko doprowadzić do pogorszenia czytelności kodu i przyprawić niejednego programistę o ból głowy. Na szczęścię kompilator TypeScript pozawala definiować zależności wskazując absolutne ścieżki - wystarczy za pomocą atrybutu baseUrl kompilatora zdefiniować katalog, względem którego chcemy wyszukiwać plików do importowania.  Wsparcie dla absolutnych importów w kompilatorze TypeScript  Wsparcie dla konfiguracji baseUrl w Angular CLI",
"url": "/2017/02/17/absolutne-importowanie-zaleznosci-w-angular-cli.html",
"author": "Grzegorz Lipecki",
"authorUrl": "/authors/glipecki.html",
"image": "glipecki.jpg",
"highlight": "/assets/img/posts/2017-02-17-absolutne-importowanie-zaleznosci-w-angular-cli/angular-cli.jpg",
"date": "17-02-2017",
"path": "pl/2017-02-17-absolutne-importowanie-zaleznosci-w-angular-cli"
}
,


"2017-02-03-dynamiczne-dodawanie-komponentow-w-angular-2-html": {
"title": "Dynamiczne dodawanie komponentów w Angular 2",
"lang": "pl",
"tags": "angular",
"content": "Od pewnego czasu pracuję nad świeżym projektem opartym o Angular 2. Częścią projektu jest prezentowanie użytkownikowi dynamicznie generowanych elementów interfejsu. Nie jesteśmy w stanie zaprojektować z wyprzedzeniem ekranów, nie znając ani ich struktury, ani konkretnych kontrolek.Standardowo aplikację budujemy korzystając z komponentów używających komponentów, które używają komponentów, i tak dalej… Komponenty określają selektory, którymi możemy je osadzać oraz szablony HTML opisujące sposób prezentacji. Korzystając z tego zestawu, w kolejnych szablonach osadzamy kolejne komponenty wykorzystując ich selektory, zupełnie jakby były to natywne elementy HTMLa.Co jednak, jeżeli nie jesteśmy w stanie ustalić konkretnego komponentu na etapie pisania aplikacji, a dopiero w trakcie jej wykonania? Musimy wymyślić coś kreatywnego :wink:Szybkie rozwiązaniePierwsze, co może nam przyjść do głowy, to wykorzystanie ngIf i opisanie widoku w formie frontendowego switch’a.Spójrzmy na przykład:@Component({    selector: &#39;app&#39;,    template: `        &amp;lt;div&amp;gt;            &amp;lt;book-details  *ngIf=&quot;model.type === &#39;book&#39;&quot;&amp;gt;&amp;lt;/book-details&amp;gt;            &amp;lt;movie-details *ngIf=&quot;model.type === &#39;movie&#39;&quot;&amp;gt;&amp;lt;/movie-details&amp;gt;            &amp;lt;comic-details *ngIf=&quot;model.type === &#39;comic&#39;&quot;&amp;gt;&amp;lt;/comic-details&amp;gt;        &amp;lt;/div&amp;gt;    `})class AppComponent {}Na pierwszy rzut oka wszystko wygląda nieźle - w zależności od typu prezentowanego obiektu potrafimy wyświetlić odpowiedni komponent prezentujący szczegóły. Nawet jesteśmy z siebie zadowoleni, w końcu mamy komponenty, a przecież mogliśmy zaszyć prezentację typów bezpośrednio w szablonie :wink:Spójrzmy jednak krytycznie na nasz twór, a zauważymy potencjalne problemy.Po pierwsze, dodanie nowego typu komponentu za każdym razem wiąże się z modyfikacją wszystkich szablonów komponentów zależnych. Dodając nową funkcjonalność do systemu, będziemy musieli zmienić wiele, teoretycznie niezależnych, fragmentów kodu. W praktyce, podążając tą drogą, szybko dotrzemy do wzorca Copy’ego i Paste’a. Stąd już blisko, żeby trafić na projektowy wall of shame za klasyczny Shotgun surgery.Kolejny, być może nawet bardziej narzucający się problem, to wypłynięcie warstwy logiki do warstwy prezentacji. Mieszanie logiki z prezentacją to nigdy nie jest dobry pomysł. W szczególności w świecie frontendu, gdzie szukanie referencji czy refaktoryzacja pomiędzy HTMLem a JS/TSem to zawsze loteria.Lepsze podejścieA co gdybyśmy mogli przenieść logikę wyboru komponentu do klasy? I dodatkowo wskazać w szablonie, gdzie komponent ma zostac wyrenderowany? Nadal nie tracąc niczego z komponentowego podejścia, w tym wstrzykiwania zależności? Dokładnie tak możemy to zrobić :wink:Przygotowanie szablonu widokuW pierwszym kroku przerobimy szablon komponentu:@Component({    selector: &#39;app&#39;,    template: `        &amp;lt;div&amp;gt;            &amp;lt;div #details&amp;gt;&amp;lt;/div&amp;gt;        &amp;lt;/div&amp;gt;    `})class AppComponent {}Dotychczasowe definicje konkretnych komponentów zastępujemy pojedynczym div’em pełniącym funkcję placeholdera. Dodatowo oznaczamy go jako zmienną lokalną o nazwie details. Dzięki temu w kolejnym kroku będziemy mogli odnieść się do niego z kontrolera komponentu.@Component(...)class AppComponent {    @ViewChild(&#39;details&#39;, {read: ViewContainerRef})    private placeholder: ViewContainerRef;}Stosując dekorator @ViewChild wstrzykujemy element DOMu do kontrolera komponentu. Pierwszy parametr może przyjąć klasę oczekiwanego elementu lub selektor. Dodatkowo przekazujemy, jakiej klasy element nas interesuje. W przypadku wstrzykiwania po selektorze standardowo jest to obiekt klasy ElementRef, nas jednak interesuje obiekt typu ViewContainerRef.Tworzenie komponentów w locieKlasa ViewContainerRef jest o tyle ciekawa, że dostarcza metodę pozwalającą tworzyć komponenty w locie na podstawie dostarczonej fabryki:createComponent(\tcomponentFactory: ComponentFactory&amp;lt;C&amp;gt;,    index?: number,    injector?: Injector,    projectableNodes?: any[][]) : ComponentRef&amp;lt;C&amp;gt;Zmienna oznaczona dekoratorem @ViewChild zostanie uzupełniona w trakcie tworzenia widoku - to znaczy, że możemy się nią posługiwać dopiero w fazie ngAfterViewInit. W efekcie możemy napisać kolejny kawałek naszego komponentu:@Component(...)class AppComponent implements AfterViewInit {    @ViewChild(&#39;details&#39;, {read: ViewContainerRef})    private placeholder: ViewContainerRef;    private componentRef: ComponentRef&amp;lt;any&amp;gt;;    ngAfterViewInit():void {        this.componentRef = this.placeholder.createComponent(this.componentFactory);    }}Pobieranie fabryki komponentówTeraz pozostaje nam już tylko uzyskanie fabryki komponentów. Gotową do działania fabrykę najlepiej uzyskać z obiektu ComponentFactoryResolver. Sam komponent możemy bez problemu wstrzyknąć z kontekstu DI, a następnie wywołać na nim metodę resolveComponentFactory, podając interesującą nas klasę komponentu.@Component(...)class AppComponent implements OnInit {    private componentFactory: ComponentFactory&amp;lt;any&amp;gt;;    constructor(private componentFactoryResolver: ComponentFactoryResolver) {    }    ngOnInit():void {        this.componentFactory = this.componentFactoryResolver.resolveComponentFactory(this.componentClass);    }}W ten sposób możliwe jest stworzenie dowolnego komponentu osiągalnego z kontekstu DI aplikacji.Sprzątanie po komponenciePrzy ręcznym tworzeniu komponentów warto też pamiętać o poprawnym zamknięciu utworzonych obiektów. W tym celu możemy wykorzystać fazę ngOnDestroy cyklu życia komponentu.@Component(...)class AppComponent implements OnDestroy {    private componentRef: ComponentRef&amp;lt;any&amp;gt;;    ngOnDestroy(): void {        this.componentRef.destroy();    }}Definiowanie dynamicznych komponentów w kontekście DIKolejna rzecz, o której musimy pamiętać, to odpowiednie oznaczenie komponentów tworzonych dynamicznie na poziomie definicji kontekstu DI. Standardowo Angular generuje kod jedynie dla tych komponentów, dla których zostały zdefiniowane referencje w kodzie. Takie referencje są tworzone automatycznie dla komponentów użytych w ramach metody bootstrap, w routingu, czy też po użyciu w szablonach widoków. Wszystkie pozostałe komponenty, nawet jeżeli zostały zdefiniowane w sekcji declarations, zostaną pominięte - dzięki temu mechanizm tree shaking będzie miał możliwość pominąć je przy budowaniu produkcyjnej wersji kodu. Komponenty dodawane dynamicznie musimy sami wskazać jawnie, na poziomie definicji modułu. W tym celu używamy pola entryComponents dekoratora @NgModule.@NgModule({    imports: [...],    declarations: [BookDetails, MovieDetails, ComicDetails],    exports: [...],    entryComponents: [BookDetails, MovieDetails, ComicDetails]})export class ItemDetailsModule {}Przekazywanie wartości do i z dynamicznego komponentuOstatnią rzeczą, na którą warto zwrócić uwagę, jest przekazywanie wartości do i z komponentu. W przypadku ręcznego dodawania komponentów do DOM nie możemy skorzystać ze standardowego przekazywania wartości przez dekoratory @Input() i @Output(). Komunikację z komponentem musimy oprogramować ręcznie. Jednak nie jest to trudne, bo obiekt ComponentRef zawiera referencję na faktyczną instancję stworzonego komponentu. Wykorzystując ją możemy zarówno ustawić wartości, jak i nasłuchiwać na zmiany.@Component(...)class AppComponent implements AfterViewInit {    private componentRef: ComponentRef&amp;lt;any&amp;gt;;    ngAfterViewInit():void {        this.componentRef = this.placeholder.createComponent(this.componentFactory);        this.componentRef.instance.inputVal = &#39;Hello world&#39;;        this.componentRef.instance.outputVal.subscribe((...) =&amp;gt; { ... });    }}Kompletny przykładNa koniec kompletny kod źródłowy omawianego przykładu.Komponent wrappera kontrolekimport {    Component, Input, OnInit, ViewContainerRef, ViewChild, ComponentFactoryResolver, Type, AfterViewInit,    OnDestroy, ComponentRef} from &#39;@angular/core&#39;;@Component({    selector: &#39;control-wrapper&#39;,    template: `        &amp;lt;div *ngIf=&quot;componentClass&quot;&amp;gt;            &amp;lt;div #componentHolder&amp;gt;&amp;lt;/div&amp;gt;        &amp;lt;/div&amp;gt;    `})export class ControlWrapper implements OnInit, AfterViewInit, OnDestroy {    private model: any;    @Input()    private controlFactory: IControlFactory;    @ViewChild(&#39;componentHolder&#39;, {read: ViewContainerRef})    private componentHolder: ViewContainerRef;    private componentClass: Type&amp;lt;IControl&amp;gt;;    private componentRef: ComponentRef&amp;lt;IControl&amp;gt;;    constructor(private componentFactoryResolver: ComponentFactoryResolver) {    }    ngOnInit(): void {        this.componentClass = this.controlFactory.getControlClass(this.model.type);    }    ngOnDestroy(): void {        if (this.componentRef) {            this.componentRef.destroy();        }    }    ngAfterViewInit(): void {        if (this.componentClass &amp;amp;&amp;amp; this.componentHolder) {            let componentFactory = this.componentFactoryResolver.resolveComponentFactory(this.componentClass);            this.componentRef = this.componentHolder.createComponent(componentFactory);            this.updateControlModel();        }    }    private updateControlModel() {        this.componentRef.instance.setModel(this.model);    }}Przykładowa fabryka klas komponentów na podstawie modeluexport class ItemDetailsControlFactory implements IControlFactory {    getControlClass(type: string): Type&amp;lt;IControlFactory&amp;gt; {        if (type === &#39;movie&#39;) {            return BookDetails;        } else if (type === &#39;book&#39;) {            return MovieDetails;        } else if (type === &#39;comic&#39;) {            return ComicDetails;        } else {            return null;        }    }}Definicja modułu dynamicznych komponentówimport {NgModule} from &#39;@angular/core&#39;;@NgModule({    imports: [...],    declarations: [BookDetails, MovieDetails, ComicDetails],    exports: [BookDetails, MovieDetails, ComicDetails],    entryComponents: [BookDetails, MovieDetails, ComicDetails]})export class ItemDetailsModule {}tl;drWykorzystując najnowszą wersję frameworka Angular 2 możemy bez problemu tworzyć elementy interfejsu w locie, w trakcie działania aplikacji. Serwis ComponentFactoryResolver i adnotacja @ViewChild pozwalają programowo tworzyć i dodawać komponenty do drzewa DOM, bez potrzeby zmieniania szablonów HTML komponentów.Źródła  Komunikacja pomiędzy komponentami, w tym oznaczanie jako zmienne lokalne i odwołania z kontrolerów.  Dokumentacja dekoratora @ViewChild  Dokumentacja klasy ViewContainerRef  Cykl życia komponentu  FAQ na temat konieczności używania entryComponents",
"url": "/2017/02/03/dynamiczne-dodawanie-komponentow-w-angular-2.html",
"author": "Grzegorz Lipecki",
"authorUrl": "/authors/glipecki.html",
"image": "glipecki.jpg",
"highlight": "/assets/img/posts/2017-02-03-dynamiczne-dodawanie-komponentow-w-angular-2/angular-2.jpg",
"date": "03-02-2017",
"path": "pl/2017-02-03-dynamiczne-dodawanie-komponentow-w-angular-2"
}
,


"2017-01-19-wireshark-czy-to-gryzie-html": {
"title": "Wireshark - czy to gryzie?",
"lang": "pl",
"tags": "wireshark networks",
"content": "Z programem Wireshark pierwszy raz zetknąłem się w czasie studiów na zajęciach z sieci komputerowych (dla niewtajemniczonych: Wireshark to aplikacja która umożliwia przechwytywanie i nagrywanie pakietów danych, a także ich dekodowanie i analizowanie). Wówczas odniosłem wrażenie że jest to narzędzie stricte dla administratorów sieci i nie ma większego zastosowania w obszarze wytwarzania i utrzymywania oprogramowania.Z upływem czasu zrozumiałem że tworzenie systemów to nie tylko pisanie pięknego i czystego kodu, ale także długie godziny spędzone na debugowaniu błędów i analizowaniu problemów. W wielu sytuacjach kluczowa jest możliwość podejrzenia komunikatów przesyłanych między elementami systemu. Do tego właśnie świetnie sprawdziło się narzędzie poznane na studiach.Poniżej pokażę przykładowe sytuacje w których wykorzystałem Wiresharka, żeby dotrzeć do sedna analizowanego problemu.“Dlaczego serwisy się nie dogadują?”Jakiś czas temu pracowałem przy utrzymaniu systemu zrealizowanego w architekturze rozproszonej, którego moduły komunikowały się za pomocą RESTa. Niestety logi aplikacyjne nie zawsze były wystarczająco szczegółowe. W pewnym przypadku dostawaliśmy w logach wpisy postaci:12:33:52.823 [main] DEBUG org.springframework.web.client.RestTemplate - GET request for &quot;http://172.19.57.4:8080/fin2/acc/balance?accNum=93%201090%200088%205180%205697%201019%203200&quot; resulted in 500 (null); invoking error handlerException in thread &quot;main&quot; org.springframework.web.client.HttpServerErrorException: 500 null    at org.springframework.web.client.DefaultResponseErrorHandler.handleError(DefaultResponseErrorHandler.java:94)    at org.springframework.web.client.RestTemplate.handleResponse(RestTemplate.java:667)    at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:620)    at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:580)    at org.springframework.web.client.RestTemplate.getForObject(RestTemplate.java:287)    at com.brdg.fin2.acc.AccClient.getBalance(AccClient.java:175)Same logi niewiele mówiły, poza tym że serwer nie radzi sobie z obsługą żądania. Serwer raportował konkretną przyczynę niepowodzenia w treści odpowiedzi, jednak ta nie była logowana na kliencie. W takiej sytuacji analiza Wiresharkiem pozwoliła szybko ustalić co faktycznie było nie tak:GET /fin2/acc/balance?accNum=93%201090%200088%205180%205697%201019%203200 HTTP/1.1Accept: text/plain, application/json, application/*+json, */*User-Agent: Java/1.8.0_111Host: 172.19.57.4:8080Connection: keep-aliveHTTP/1.1 500X-Application-Context: application:8080Content-Type: application/json;charset=UTF-8Transfer-Encoding: chunkedConnection: closeb5{&quot;status&quot;:500,&quot;error&quot;:&quot;Internal Server Error&quot;,&quot;exception&quot;:&quot;java.lang.NumberFormatException&quot;,&quot;message&quot;:&quot;For input string: \\&quot;93 10\\&quot;&quot;,&quot;path&quot;:&quot;/fin2/acc/balance&quot;}0W tym przypadku usunięcie spacji z numeru rachunku pozwoliło wyeliminować problem.“Przecież podaję ten parametr, o tu!”Zdarza się, że pisanie nowego kodu idzie gładko, aż napotykamy na sytuację, w której wydaje nam się że wszystko dobrze zakodowaliśmy, a jednak serwer twierdzi że wysyłane przez nas żądanie jest niepoprawne. Przykład kodu klienta RESTowej usługi, który wydawał mi się poprawny:WebTarget webTarget = createWebTarget();webTarget.queryParam(&quot;statementNo&quot;, statementNumber);Response response = webTarget.request().get();A jednak serwer uporczywie twierdził że:Required String parameter &#39;statementNo&#39; is not present.Wyglądało jakby serwer nie potrafił odczytać wysyłanego przeze mnie parametru. Zanim jednak zacząłem obwiniać stronę serwerową, postanowiłem upewnić się że żądanie HTTP w istocie jest poprawne. Szybka analiza Wiresharkiem pokazała:GET /crwr/statement HTTP/1.1User-Agent: Jersey/2.23.2 (HttpUrlConnection 1.8.0_79)Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2Connection: keep-aliveHTTP/1.1 400...Chwila, a gdzie mój parametr? Mając pewność, że problem leży jednak po stronie klienckiej, zacząłem wnikliwiej analizować kod linijka po linijce (tym razem czytając javadoki JAX-RS :) ) :/*** Create a new {@code WebTarget} instance by configuring a query parameter on the URI* of the current target instance.* ...* @return a new target instance.* ...*/public WebTarget queryParam(String name, Object... values);No cóż, przynajmniej mam teraz o czym pisać na blogu ;). Ten klient działał zdecydowanie lepiej:WebTarget webTarget = createWebTarget();Response response = webTarget.queryParam(&quot;statementNo&quot;, statementNumber).webTarget.request().get();Jak zacząć z Wiresharkiem?Podobnych przykładów mógłbym przywołać więcej, a każdy z nich utwierdza mnie w przekonaniu że warto nauczyć się korzystać z Wiresharka. Tym bardziej że jest to narzędzie niezależne od zastosowanego stosu technologicznego, więc jego znajomość może przydać się w różnych projektach.Instalacja na Ubuntu sprowadza się do wykonania standardowych poleceń:sudo apt-add-repository universesudo apt-get updatesudo apt-get install wiresharkDodatkowo Wireshark używa biblioteki dumpcap, która musi dostać uprawnienia do działania na użytkowniku root. Sprawdzamy jej lokalizację:sudo which dumpcapa następnie:sudo chmod 4711 [lokalizacja]Po uruchomieniu Wiresharka wybieramy opcję “Capture/Options”, a następnie interfejs sieciowy, na którym chcemy nasłuchiwać - w większości przypadków najwygodniej jest wybrać opcję “any”, czyli nasłuchiwać na wszystkich interfejsach sieciowych. Następnie wpisujemy filtr przechwytywania określający jaki ruch sieciowy nas interesuje. Często wystarczy ograniczyć przechwytywanie do określonego portu, na którym chcemy obejrzeć żądania:Pozostaje już tylko uruchomić przechwytywanie i wykonać akcję którą chcemy przeanalizować. W oknie Wiresharka powinniśmy zobaczyć zarejestrowany ruch sieciowy:Analizując komunikację HTTP najłatwiej jest wybrać opcję “Follow TCP Stream”, co pozwoli nam w przystępny sposób obejrzeć następujące po sobie żądania i odpowiedzi na poziomie HTTP:Końcowe przemyślenia  Powyżej przedstawiłem oczywiście tylko podstawowe możliwości użycia narzędzia (aczkolwiek pokrywające 80% moich deweloperskich potrzeb). Oferuje ono dodatkowo wiele zaawansowanych ficzerów, jak choćby zaawansowane filtrowanie na poziomie przechwytywania i wyświetlania, kolorowanie pakietów, czy statystyki sieci.  Wireshark nie zawsze może być wykorzystany bezpośrednio na danym hoście, bo np. ten nie posiada graficznego interfejsu, co najczęściej ma miejsce w środowisku serwerowym. Wówczas można posiłkować się narzędziem tcpdump, którym możemy zarejestrować ruch sieciowy do pliku na docelowym środowisku, a następnie plik ten wczytać do analizy w programie Wireshark.  Istnieje wiele innych narzędzi potrafiących rejestrować ruch HTTP, m.in. Fiddler czy Live HTTP Headers. Wydaje się jednak że Wireshark jest bardziej wszechstronny i jego znajomość można wykorzystać również w innych poza HTTP obszarach, np. dlaczego mój klient LDAP nie działa, a inny tak?  Używając Wireshark/tcpdump możemy analizować ruch nie tylko na hoście gdzie mamy uruchomione to narzędzie. Poprzez różne techniki, takie jak port mirroring czy ARP poisoning, możemy podsłuchiwać pakiety nie przechodzące bezpośrednio przez naszego hosta.",
"url": "/2017/01/19/wireshark-czy-to-gryzie.html",
"author": "Tomasz Lewandowski",
"authorUrl": "/authors/tlewandowski.html",
"image": "tlewandowski.jpg",
"highlight": "/assets/img/posts/2017-01-19-wireshark-czy-to-gryzie/wireshark.jpeg",
"date": "19-01-2017",
"path": "pl/2017-01-19-wireshark-czy-to-gryzie"
}
,


"2017-01-05-budowanie-aplikacji-angular-cli-spring-boot-html": {
"title": "Budowanie aplikacji Angular CLI + Spring Boot",
"lang": "pl",
"tags": "angular spring boot",
"content": "Każda nietrywialna aplikacja potrzebuje backendu. O ile obecnie to nie jest prawda, to na potrzeby tego artykułu przyjmijmy, że tak jest. A jak backend współpracujący z aplikacją webową to REST, a jak REST to Spring i Spring Boot. W kilku kolejnych akapitach stworzymy i z sukcesem przygotujemy gotowy do wdrożenia artefakt składający się z aplikacji webowej w Angular 2 i backendu usługowego wykorzystującego Spring Boot.Artykuł zakłada podstawową znajomość Angular CLI, Spring Boot i Maven.Wszystkie przedstawione kroki są commitami do testowego repozytorium, dzięki temu sam możesz prześledzić proces tworzenia aplikacji oraz rozwiać wszelkie wątpliwości. Link do repozytorium: demo@github.Stworzenie minimalnego projektuNowy projekt najłatwiej stworzymy wykorzystując Spring Initializer, możemy to zrobić wchodząc http://start.spring.io/ i wyklikując konfigurację projektu, lub możemy to zrobić w stylu prawdziwego geeka - curlem.$ cd demo$ curl https://start.spring.io/starter.tgz \\  -d groupId=net.lipecki.demo \\  -d packageName=net.lipecki.demo \\  -d artifactId=demo \\  -d dependencies=web \\  | tar -xzvf -  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed100 50190  100 50104  100    86  54925     94 --:--:-- --:--:-- --:--:-- 54878x mvnwx .mvn/x .mvn/wrapper/x src/x src/main/x src/main/java/x src/main/java/net/x src/main/java/net/lipecki/x src/main/java/net/lipecki/demo/x src/main/resources/x src/main/resources/static/x src/main/resources/templates/x src/test/x src/test/java/x src/test/java/net/x src/test/java/net/lipecki/x src/test/java/net/lipecki/demo/x .gitignorex .mvn/wrapper/maven-wrapper.jarx .mvn/wrapper/maven-wrapper.propertiesx mvnw.cmdx pom.xmlx src/main/java/net/lipecki/demo/DemoApplication.javax src/main/resources/application.propertiesx src/test/java/net/lipecki/demo/DemoApplicationTests.javaW efekcie dostajemy minimalną aplikację Spring Boot, którą możemy zbudować i uruchomić. Do testów dorzućmy prosty kontroler.$ curl -L https://goo.gl/MbWM8s -o src/main/java/net/lipecki/demo/GreetingRestController.java$ cat src/main/java/net/lipecki/demo/GreetingRestController.javapackage net.lipecki.demo;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class GreetingRestController {\t@RequestMapping(&quot;/greeting&quot;)\tpublic String greeting() {\t\treturn &quot;Welcome!&quot;;\t}}Całość możemy zbudować wykorzystując Maven. W zależności od preferencji możemy zbudować aplikację wykorzystując globalnie zainstalowaną w systemie instancję lub skorzystać z dostarczanego ze szkieletem Maven Wrappera. Stosowanie Wrappera pozwala pracować z aplikacją nie zmieniając zainstalowanych w systemie pakietów oraz zapewnia, że możemy różne projekty budować różnymi wersjami Mavena. Wrapper w razie potrzeby ściągnie odpowiednią wersję bibliotek przy pierwszym uruchomieniu../mvnw clean package . . .[INFO] --- spring-boot-maven-plugin:1.4.2.RELEASE:repackage (default) @ demo ---[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 4.929 s[INFO] Finished at: 2016-12-07T20:43:32+01:00[INFO] Final Memory: 28M/325M[INFO] ------------------------------------------------------------------------uruchomić$ java -jar target/demo-0.0.1-SNAPSHOT.jar . . .2016-12-07 20:44:13.392  INFO 70743 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)2016-12-07 20:44:13.397  INFO 70743 --- [           main] net.lipecki.demo.DemoApplication         : Started DemoApplication in 2.271 seconds (JVM running for 2.643)i przetestować$ curl http://localhost:8080/greeting &amp;amp;&amp;amp; echoWelcome!Docelowa struktura projektuW przypadku najprostszego podejścia wystarczające byłoby dorzucenie tylko kodów części webowej do aktualnego szkieletu i zadbania o jego poprawne budowanie. Jednak dla nas wystarczające to za mało, od razu przygotujemy strukturę która będzie miała szansę wytrzymać próbę czasu.Minimalny sensowny podział to przygotowanie dwóch modułów:  artefaktu wdrożeniowego z usługami REST,  aplikacji webowej.Taki podział aplikacja pozwala nam na dodatkową separację części backend i frontend (w pogoni za ideałem możemy przygotować jeszcze ciekawszą strukturę, szczegóły w jednym z ostatnich akapitów wpisu).W tym celu:  dodajemy do projektu dwa moduły,          demo-app,      demo-web,        przenosimy dotychczasową konfigurację budowania i kody do demo-app.Całość zmian możemy zweryfikować w commicie do testowego repozytorium GitHub: commit.Po tych zmianach, nadal możemy budować i uruchamiać aplikację, jednak tym razem z poziomu modułu demo-app.$ ./mvnw clean package . . .[INFO] Reactor Summary:[INFO][INFO] demo ............................................... SUCCESS [  0.141 s][INFO] demo-app ........................................... SUCCESS [  4.909 s][INFO] demo-web ........................................... SUCCESS [  0.002 s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 5.329 s[INFO] Finished at: 2016-12-07T21:15:51+01:00[INFO] Final Memory: 30M/309M[INFO] ------------------------------------------------------------------------$ java -jar demo-app/target/demo-app-0.0.1-SNAPSHOT.jar . . .2016-12-07 21:16:30.569  INFO 71306 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)2016-12-07 21:16:30.574  INFO 71306 --- [           main] net.lipecki.demo.DemoApplication         : Started DemoApplication in 2.732 seconds (JVM running for 3.117)Dodanie i budownie części webProjekt części webowej najłatwiej stworzyć z wykorzystaniem Angular CLI, w tym celu wykonujemy:$ pwd/tmp/demo/demo-web/$ rm -rf src$ ng init --style=scss . . .Installing packages for tooling via npm.Installed packages for tooling via npm.Standardowy Angular CLI będzie budował aplikację do folderu dist. Jednak konwencja budowania przez Maven zakłada, że wszystkie zasoby, czy to pośrednie, czy docelowe, wygenerowane w trakcie procesu budowania trafią do odpowiednich podfolderów katalogu target. Preferowanie konwencji ponad konfigurację to zawsze dobry pomysł. W tym celu zmieniamy standardową konfigurację folderu budowania z dist na target/webapp w angular-cli.json: commit.W tym momencie możemy już swobodnie pracować z aplikacją uruchamiając ją za pomocą ng serve. Kolejnym krokiem będzie zintegrowanie procesu budowania ng build z budowaniem modułu Maven. W tym celu wykorzystamy plugin frontend-maven-plugin.Plugin frontend-maven-plugin pozwala:  zainstalować niezależną od systemowej wersję node i npm,  uruchomić instalację zależności npm,  uruchomić budowanie aplikacji za pomocą ng.Całość konfiguracji wprowadzamy definiując dodatkowe elementy execution konfiguracji pluginu.Przed dodaniem konfiguracji poszczególnych kroków dodajemy do sekcji build.plugins definicję samego pluginu:&amp;lt;plugin&amp;gt;  &amp;lt;groupId&amp;gt;com.github.eirslett&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;frontend-maven-plugin&amp;lt;/artifactId&amp;gt;  &amp;lt;version&amp;gt;1.3&amp;lt;/version&amp;gt;  &amp;lt;configuration&amp;gt;    &amp;lt;installDirectory&amp;gt;target&amp;lt;/installDirectory&amp;gt;  &amp;lt;/configuration&amp;gt;  &amp;lt;executions&amp;gt;    &amp;lt;!-- tutaj będą konfiguracje kroków budowaia --&amp;gt;  &amp;lt;/executions&amp;gt;&amp;lt;/plugin&amp;gt;W pierwszym kroku instalujemy wskazaną wersję runtime node i menadżera pakietów npm. Całość jest instalowana lokalnie w folderze target. Dzięki lokalnej instalacji minimalizujemy listę wymagań wstępnych do pracy z naszym projektem, co jest szczególnie ważne przy wykorzystaniu systemów CI/CD.&amp;lt;execution&amp;gt;  &amp;lt;id&amp;gt;install node and npm&amp;lt;/id&amp;gt;  &amp;lt;goals&amp;gt;    &amp;lt;goal&amp;gt;install-node-and-npm&amp;lt;/goal&amp;gt;  &amp;lt;/goals&amp;gt;  &amp;lt;configuration&amp;gt;    &amp;lt;nodeVersion&amp;gt;v6.9.1&amp;lt;/nodeVersion&amp;gt;  &amp;lt;/configuration&amp;gt;&amp;lt;/execution&amp;gt;W drugim kroku plugin za pomocą npm instaluje wszystkie zdefiniowane w package.json zależności naszego projektu. Jest to odpowiednik wykonania npm install w katalogu głównym projektu.&amp;lt;execution&amp;gt;  &amp;lt;id&amp;gt;npm install&amp;lt;/id&amp;gt;  &amp;lt;goals&amp;gt;    &amp;lt;goal&amp;gt;npm&amp;lt;/goal&amp;gt;  &amp;lt;/goals&amp;gt;&amp;lt;/execution&amp;gt;Ostatni krok to wykonanie właściwego procesu budowania aplikacji z wykorzystaniem Angular CLI.W celu uproszczenia konfiguracji dodajemy nowy task o nazwie build w pliku package.json. Ręczne zdefiniowanie taska jest o tyle ważne, że w ten sposób będziemy się mogli uniezależnić od systemowej instancji Angular CLI i stosować lokalną wersję zainstalowaną na podstawie definicji w package.json.&quot;scripts&quot;: {  &quot;build&quot;: &quot;node node_modules/angular-cli/bin/ng build&quot;}Oraz dodajemy wykonanie nowo utworzonego tasku przez plugin.&amp;lt;execution&amp;gt;  &amp;lt;id&amp;gt;node build app&amp;lt;/id&amp;gt;  &amp;lt;phase&amp;gt;prepare-package&amp;lt;/phase&amp;gt;  &amp;lt;goals&amp;gt;    &amp;lt;goal&amp;gt;npm&amp;lt;/goal&amp;gt;  &amp;lt;/goals&amp;gt;  &amp;lt;configuration&amp;gt;    &amp;lt;arguments&amp;gt;run-script build&amp;lt;/arguments&amp;gt;  &amp;lt;/configuration&amp;gt;&amp;lt;/execution&amp;gt;  Nazwa tasku build jest czysto umowna, jedyne wymaganie to używanie tej samej w package.json i pom.xml. Jednak trzymanie się konkretnej konwencji, np. build, ułatwi pracę pomiędzy różnymi projektami.Tak przygotowana konfiguracja pozwala zintegrować budowanie aplikacji web z fazami cyklu życia Maven. Dodatkowo dostajemy uspójniony sposób uruchomienia za pomocą polecenia npm build. Dzięki wykorzystaniu frontend-maven-plugin uniezależniamy proces budowania od środowiska, wszystkie wymagane biblioteki (node, npm, angular-cli) są instalowane i wykonywane lokalnie w folderze projektu.Całość zmian z tego kroku możemy obejrzeć w commicie GitHub: commit.Składanie artefaktu z częścią webKolejnym krokiem jest umożliwienie spakowania modułu odpowiedzialnego za część web do pojedynczego artefaktu, gotowego do wykorzystania jako zależność lub przesłania do repozytorium artefaktów.Standardowo Maven obsługuje najpopularniejsze typy artefaktów, np. jar, war, ear. Dla tych typów znany jest sposób ich budowania, struktura archiwów jest odgórnie ustalona i niezmienna pomiędzy projektami. Jednak my chcemy przygotować archiwum w postaci pliku zip, więc wykorzystując maven-assembly-plugin będziemy mogli sami określić jakie pliki i w jaki sposób zbierać budując wynikowy artefakt.Do pom.xml modułu demo-web dodajemy definicję maven-assembly-plugin zawierającą docelową nazwę artefaktu oraz plik assembly opisujący sposób jego składania.W sekcji build.plugins dopisujemy:&amp;lt;plugin&amp;gt;  &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;maven-assembly-plugin&amp;lt;/artifactId&amp;gt;  &amp;lt;configuration&amp;gt;    &amp;lt;descriptor&amp;gt;assembly.xml&amp;lt;/descriptor&amp;gt;    &amp;lt;finalName&amp;gt;demo-web-${project.version}&amp;lt;/finalName&amp;gt;    &amp;lt;appendAssemblyId&amp;gt;false&amp;lt;/appendAssemblyId&amp;gt;  &amp;lt;/configuration&amp;gt;  &amp;lt;executions&amp;gt;    &amp;lt;execution&amp;gt;      &amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt;      &amp;lt;goals&amp;gt;        &amp;lt;goal&amp;gt;single&amp;lt;/goal&amp;gt;      &amp;lt;/goals&amp;gt;    &amp;lt;/execution&amp;gt;  &amp;lt;/executions&amp;gt;&amp;lt;/plugin&amp;gt;Następnie dodajemy plik assembly.xml (obok pliku pom.xml), w którym określamy docelowy format (zip) oraz które pliki, z którego katalogu spakować do artefaktu (wszystkie z folder target/webapp).&amp;lt;assembly&amp;gt;  &amp;lt;id&amp;gt;zip&amp;lt;/id&amp;gt;  &amp;lt;formats&amp;gt;    &amp;lt;format&amp;gt;zip&amp;lt;/format&amp;gt;  &amp;lt;/formats&amp;gt;  &amp;lt;includeBaseDirectory&amp;gt;false&amp;lt;/includeBaseDirectory&amp;gt;  &amp;lt;fileSets&amp;gt;    &amp;lt;fileSet&amp;gt;      &amp;lt;directory&amp;gt;target/webapp&amp;lt;/directory&amp;gt;      &amp;lt;outputDirectory&amp;gt;&amp;lt;/outputDirectory&amp;gt;      &amp;lt;includes&amp;gt;        &amp;lt;include&amp;gt;**&amp;lt;/include&amp;gt;      &amp;lt;/includes&amp;gt;    &amp;lt;/fileSet&amp;gt;  &amp;lt;/fileSets&amp;gt;&amp;lt;/assembly&amp;gt;Całość możemy przetestować wykonując:$ pwd/tmp/demo$ ./mvnw clean package . . .$ ls demo-web/target/demo-web-0.0.1-SNAPSHOT.zipdemo-web/target/demo-web-0.0.1-SNAPSHOT.zipCommit zawierający zmiany: commit.Składanie artefaktu wdrożeniowegoOstatnie co musimy zrobić żeby nasza aplikacja składała się w pojedynczy wykonywalny artefakt to skonfigurować moduł demo-web jako zależność w projekcie demo-app oraz skonfigurowanie pluginu maven-dependency-plugin, który będzie odpowiadał za odpowiednie rozpakowanie zasobów.Definiujemy zależność na moduł demo-web w pom.xml w sekcji dependencies:&amp;lt;dependency&amp;gt;  &amp;lt;groupId&amp;gt;net.lipecki.demo&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;demo-web&amp;lt;/artifactId&amp;gt;  &amp;lt;version&amp;gt;${project.version}&amp;lt;/version&amp;gt;  &amp;lt;type&amp;gt;zip&amp;lt;/type&amp;gt;&amp;lt;/dependency&amp;gt;  Standardowo Maven szuka zależności typu jar, jednak nasz moduł web jest typu zip, co możemy jawnie wskazać definiując zależność.Aplikacja Spring Boot poza serwowaniem zdefiniowany servletów i usług REST hostuje również wszystkie zasoby, które znajdują się na zdefiniowanych ścieżkach zasobów statycznych. W standardowej konfiguracji, jedną z takich ścieżek są zasoby wewnątrz samego jara aplikacji. Korzystając z tej wiedzy skonfigurujemy plugin maven-dependency-plugin w taki sposób, żeby rozpakowywał archiwum modułu web do odpowiedniego katalogu budowania.W sekcji build.plugins dodajemy:&amp;lt;plugin&amp;gt;  &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;maven-dependency-plugin&amp;lt;/artifactId&amp;gt;  &amp;lt;executions&amp;gt;    &amp;lt;execution&amp;gt;      &amp;lt;id&amp;gt;unpack&amp;lt;/id&amp;gt;      &amp;lt;phase&amp;gt;generate-resources&amp;lt;/phase&amp;gt;      &amp;lt;goals&amp;gt;        &amp;lt;goal&amp;gt;unpack&amp;lt;/goal&amp;gt;      &amp;lt;/goals&amp;gt;      &amp;lt;configuration&amp;gt;        &amp;lt;artifactItems&amp;gt;          &amp;lt;artifactItem&amp;gt;            &amp;lt;groupId&amp;gt;net.lipecki.demo&amp;lt;/groupId&amp;gt;            &amp;lt;artifactId&amp;gt;demo-web&amp;lt;/artifactId&amp;gt;            &amp;lt;version&amp;gt;${project.version}&amp;lt;/version&amp;gt;            &amp;lt;type&amp;gt;zip&amp;lt;/type&amp;gt;          &amp;lt;/artifactItem&amp;gt;        &amp;lt;/artifactItems&amp;gt;        &amp;lt;outputDirectory&amp;gt;${project.build.directory}/classes/resources&amp;lt;/outputDirectory&amp;gt;      &amp;lt;/configuration&amp;gt;    &amp;lt;/execution&amp;gt;  &amp;lt;/executions&amp;gt;&amp;lt;/plugin&amp;gt;W tym momencie mamy już kompletny proces budowania aplikacji. Po jego wykonaniu i uruchomieniu aplikacji możemy zarówno wywołać testową usługę REST, jak i obejrzeć szkielet Angular 2.$ ./mvnw clean package . . .[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO][INFO] demo ............................................... SUCCESS [  0.120 s][INFO] demo-web ........................................... SUCCESS [ 18.459 s][INFO] demo-app ........................................... SUCCESS [  5.797 s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 24.644 s[INFO] Finished at: 2016-12-07T22:06:14+01:00[INFO] Final Memory: 37M/346M[INFO] ------------------------------------------------------------------------$ java -jar demo-app/target/demo-app-0.0.1-SNAPSHOT.jar . . .2016-12-07 22:08:02.937  INFO 72316 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)2016-12-07 22:08:02.942  INFO 72316 --- [           main] net.lipecki.demo.DemoApplication         : Started DemoApplication in 2.681 seconds (JVM running for 3.077)$ curl http://localhost:8080/greeting &amp;amp;&amp;amp; echoWelcome!$ curl http://localhost:8080/&amp;lt;!doctype html&amp;gt;&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;  &amp;lt;meta charset=&quot;utf-8&quot;&amp;gt;  &amp;lt;title&amp;gt;DemoWeb&amp;lt;/title&amp;gt;  &amp;lt;base href=&quot;/&quot;&amp;gt;  &amp;lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&amp;gt;  &amp;lt;link rel=&quot;icon&quot; type=&quot;image/x-icon&quot; href=&quot;favicon.ico&quot;&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;  &amp;lt;app-root&amp;gt;Loading...&amp;lt;/app-root&amp;gt;&amp;lt;script type=&quot;text/javascript&quot; src=&quot;inline.bundle.js&quot;&amp;gt;&amp;lt;/script&amp;gt;&amp;lt;script type=&quot;text/javascript&quot; src=&quot;styles.bundle.js&quot;&amp;gt;&amp;lt;/script&amp;gt;&amp;lt;script type=&quot;text/javascript&quot; src=&quot;vendor.bundle.js&quot;&amp;gt;&amp;lt;/script&amp;gt;&amp;lt;script type=&quot;text/javascript&quot; src=&quot;main.bundle.js&quot;&amp;gt;&amp;lt;/script&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;Komplet dotychczasowych zmian możemy podsumować w repozytorium GitHub: repozytorium.Obsługa routingu z wykorzystaniem history.pushState (html5 url style)Poza samym budowaniem i uruchamianiem aplikacji, warto jeszcze zadbać o wsparcie dla nowych sposobów nawigacji. Całość można łatwo zrealizować wykorzystując mechanizmy generowania stron błędów w Springu. Zanim przejdziemy do kodu, kilka słów wprowadzenia teoretycznego.Dotychczas aplikacje web można było łatwo rozpoznać po routingu opartym o #… w url. Taki sposób nawigacji nie narzuca żadnych ograniczeń na stronę serwerową aplikacji, jednak tworzy kilka niemożliwych do rozwiązania problemów, np. renderowanie po stronie serwerowej, czy wsparcie dla SEO.Obecnie, większość nowoczesnych przeglądarek dostarcza nowe API history.pushState pozwalające zrealizować nawigację z pominięciem znaku #. Po szczegóły odsyłam do oficjalnej dokumentacji Angular, natomiast w kolejnych akapitach zajmiemy się konfiguracją Spring Boot wspierającą tę strategię.Całość jest o tyle ważna, że nawigacja bez # jest zalecaną przez zespół Angular 2 konfiguracją. Przez to jest stosowana zarówna w dokumentacji, oficjalnym guide oraz wszystkich szablonach projektów, w tym również Angular CLI. To jednak oznacza, że do serwera usług będą generowane żądania oparte o ścieżki, które fizycznie nie są nigdzie zdefiniowane, co zakończy się błędami 404. W tej sytuacji, bez dostosowania naszego projektu, nie będziemy w stanie w ogóle uruchomić aplikacji oferującej nawigację opartą o routing.W założeniu przedstawiony problem możemy uprościć do zwracania treści index.html zawsze wtedy, kiedy standardowo zwrócilibyśmy błąd 404. Rozwiązanie powinno uwzględniać zarówno istnienie zdefiniowanych w aplikacji mapowań, jak i pobieranie zasobów dostępnych w lokalizacjach zasobów statycznych.Najprostszym rozwiązaniem jest zdefiniowanie własnego ErrorViewResolver, który dla błędów 404 wykona przekierowanie na zasób /index.html.W tym celu dodajemy do kontekstu beana customErrorViewResolver, który wszystkie żądania standardowo zwracając HttpStatus.NOT_FOUND przekieruje na index.html.@Beanpublic ErrorViewResolver customErrorViewResolver() {  final ModelAndView redirectToIndexHtml = new ModelAndView(&quot;forward:/index.html&quot;, Collections.emptyMap(), HttpStatus.OK);    return (request, status, model) -&amp;gt; status == HttpStatus.NOT_FOUND ? redirectToIndexHtml : null;}Przy takim podejściu warto zadbać o to, żeby zawsze jakiś index.html mógł się rozwiązać!Sposób wprowadzenia zmiany można prześledzić w commicie GitHub: 57149a4.Wykorzystanie własnego ErrorViewResolver dodatkowo zapewnia nam wsparcie dla rozróżniania żądań na podstawie nagłówka HTTP produces. To znaczy, że żądania z przeglądarek (zawierające produces = “text/html”) zostaną obsłużone zawartością zasobu /index.html, natomiast pozostałe (np. curl) odpowiedzą standardowym błędem 404.Możliwe rozszerzeniaWartym rozważenia rozszerzeniem projektu może być wydzielenie trzeciego modułu i dodatkowe podzielenie aplikacji:demo\\-- demo-rest\\-- demo-app\\-- demo-webGdzie moduły:  demo-web - zawiera zasoby aplikacji web,  demo-rest - zawiera samodzielnie uruchamialną aplikację dostarczającą komplet usług REST,  demo-app - jest złączeniem modułów web i rest w jeden wykonywalny artefakt.Przy takim podziale uzyskujemy dużą separację pomiędzy modułami. Część backend odpowiedzialna ze udostępnienie usług REST i jest całkowicie niezależna od modułu demo-web. Moduł demo-web także nie ma żadnej zależności. To oznacza, że możemy je rozwijać, wersjonować oraz osadzać rozdzielnie. Dodatkowo wprowadzenie modułu app pozwala pisać usługi REST w oderwaniu od produkcyjnego osadzania, np. możliwe jest lokalne uruchamianie modułu demo-rest jako fat jar z Jetty, podczas gdy produkcyjnie moduł demo-app będzie osadzany jako war na Tomcat.Codzienna praca z aplikacjąPełnię możliwości duetu Spring Boot i Angular CLI poczujemy dopiero odpowiednio przygotowując środowisko codziennej pracy.Część kliencką uruchamiamy przez ng serve, dzięki temu dostajemy kompilację i budowanie aplikacji po każdej zmianie kodów źródłowych oraz dodatkowo powiadomienia live reload i odświeżanie aplikacji w przeglądarce. Część serwerową uruchamiamy w IDE wspierającym hot swap kodów.Przy takiej konfiguracji aplikacja webowa jest dostępna na porcie 4200, a backend REST na porcie 8080. Musimy jeszcze umożliwić dostęp do usług REST w sposób identyczny z docelowym, w tym celu na porcie 4200 skonfigurujemy proxy do usług.Dla wygody konfiguracji przenosimy wystawione usługi pod prefix /api i tworzymy plik mapowań proxy w demo-web/proxy.conf.json:{  &quot;/api&quot;: {    &quot;target&quot;: &quot;http://localhost:8080&quot;,    &quot;secure&quot;: false  }}We wszystkich zdefiniowanych adnotacjach @RequestMapping dopisałem prefix /api w mapowanym url.Część serwerową uruchamiamy w IDE (lub dowolny inny sposobów), natomiast część web uruchamiamy przez Angular CLI:$ ng serve --proxy-config proxy.conf.json . . .webpack: bundle is now VALID.$ curl http://localhost:8080/api/greeting &amp;amp;&amp;amp; echoWelcome!$ curl http://localhost:4200/api/greeting &amp;amp;&amp;amp; echoWelcome!Dodakowo konfigurację uruchomienia wspierającą proxy usług warto zdefiniować w pliku package.json, w sekcji scripts modyfikujemy polecenie skrypt start:&quot;scripts&quot;: {  &quot;start&quot;: &quot;ng serve --proxy-config proxy.conf.json&quot;,}Dzięki temu nie musimy pamiętać przełączików i parametrów, a całość możemy uruchamiać jednym poleceniem:$ npm start . . .webpack: bundle is now VALID.W ten sposób pracujemy z aplikacją wystawioną pod adresem http://localhost:4200/, a wszystkie zmiany w części serwerowej i webowej możemy mieć odświeżane na bieżąco, zaraz po ich wprowadzeniu.Commit opisujący wprowadzone zmiany.PodsumowanieJeżeli na co dzień pracujesz z projektami opartymi o Spring Boot i Maven ich integracja z aplikacjami pisanymi w Angular CLI nie będzie stanowić dużego wyzwania. W podstawowej realizacji pomoże Ci plugin maven-frontend-plugin, natomiast wykorzystując dodatkowo maven-assembly-plugin i maven-dependency-plugin możliwe jest przygotowanie dużo bardziej złożonych procesów budowania aplikacji.Ostateczną wersję aplikacji możemy obejrzeć na GitHub: demo@github.Na sam koniec chciałbym podziękować Krysi, Jackowi i Marcinowi. Bez Was nie byłoby tego wpisu, dzięki!Materiały  https://github.com/glipecki/spring-with-angular-cli-demo  http://www.consdata.pl/blog/7-szybki-start-z-angular-cli  https://angular.io/docs/ts/latest/guide/router.html#!#browser-url-styles  http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#common-application-properties",
"url": "/2017/01/05/budowanie-aplikacji-angular-cli-spring-boot.html",
"author": "Grzegorz Lipecki",
"authorUrl": "/authors/glipecki.html",
"image": "glipecki.jpg",
"highlight": "/assets/img/posts/2017-01-05-budowanie-aplikacji-angular-cli-spring-boot/angular-spring-boot.jpeg",
"date": "05-01-2017",
"path": "pl/2017-01-05-budowanie-aplikacji-angular-cli-spring-boot"
}
,


"2016-12-22-hystrix-praktyczne-uzycie-circuit-breakera-html": {
"title": "Hystrix - praktyczne użycie circuit breaker'a",
"lang": "pl",
"tags": "hystrix circuit breaker",
"content": "Awaria:Do katastrofy prowadzi często splot różnych czynników, które w pojedynkę nie stanowią większego zagrożenia. Wymieńmy więc:  system, z którym się komunikujemy miewa od czasu do czasu długi czas odpowiedzi - no, cóż zdarza się, cztery dziewiątki to wciąż 0,01% możliwych faili, przy 0.5 miliona requestów dziennie, wychodzi jakieś 5000 - rozwiązanie: dłuższe timeouty;  zdarzają się dłuższe przerwy, np. system nie działa parę minut - brak bezprzerwowych wdrożeń, problemy ze stabilnością środowiska - póki nie wpływa to bezpośrednio na user experience jest do ogrania, np. za pomocą kolejek;  ograniczona liczba wątków w kontenerze aplikacji - sprzęt kosztuje czy to w chmurze czy we własnej serwerowni.Całkiem prawdopodobny scenariusz awarii: System zewnętrzny przestaje odpowiadać. Z racji tego, że timeouty mamy dosyć wysokie do obsługi rosnącej liczby requestów w naszej aplikacji przydzielane są kolejne wątki kontenera. Dochodzimy do momentu, w którym wszystkie wątki są w użyciu (np. w Tomcacie domyślnie jest 100). Jeżeli w tym samym kontenerze działają inne usługi to obsługa requesta w każdej z nich czeka na wolny wątek. Co za tym idzie wywołania usług, które do tej pory odpowiadały bardzo szybko i nie potrzebują do swojego działania systemu zewnętrznego są de facto od niego zależne. Awaria występuje dosyć szybko do wykorzystania 100 wątków wystarczy ruch 50 requestów/s i timeout 2000 ms.Poniżej filmik z przykładowego scenariusza takiej awarii. W lewym oknie widzimy czasy odpowiedzi aplikacji niezależnej od systemu zewnętrznego, w prawej aplikacja korzystające z tego systemu z timeoutem 800 ms. W celach przykładu liczba wątków serwera webowego została ograniczona do pięciu. W 25 sekundzie zewnętrzny system zostaje wyłączony. Aplikacja korzystająca z niego, co zrozumiałe zwiększa czasy odpowiedzi do 800 ms. Niestety z powodu zajętości wątków serwera aplikacja niezależąca od zewnętrznego systemu (lewe okno) zwiększa czasy odpowiedzi z 12 ms do prawie 50 ms. Czyli czas odpowiedzi wydłuża się 4 krotnie.Czy można jakoś takiej sytuacji zaradzić? Co chcielibyśmy osiągnąć?Próba ograniczenia skutków:Po pierwsze: Spróbujmy ograniczyć propagację awarii na pozostałe komponenty systemu. Skoro system zewnętrzny nie odpowiada w przewidzianym przez nas czasie nie ma sensu bombardowania go kolejnymi requestami. Może jeżeli damy mu trochę czasu dojdzie do ładu. Załóżmy, że system nie działa i nie czekajmy 800 ms na odpowiedź. Co jakiś czas sprawdźmy czy czasem nie wstał.Taki model działania realizuje circuit breaker opisany przez Martina Fowlera. Implementację możemy znaleźć np. w hystrixie, bibliotece wchodzącej w skład stacka Netflix’a.W testowanym przykładzie posługujemy się prostą spring boot’ową aplikacją składającą się z kontrolera:package pl.consdata.hystrix.example.hystrixservice;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class RandomGeneratorController {    private final RandomGeneratorServiceClient randomGeneratorServiceClient;    public RandomGeneratorController(RandomGeneratorServiceClient randomGeneratorServiceClient) {        this.randomGeneratorServiceClient = randomGeneratorServiceClient;    }    @RequestMapping(&quot;/random&quot;)    String getRandom() throws InterruptedException {        return randomGeneratorServiceClient.getRandom();    }}klienta serwisu zewnętrznego:package pl.consdata.hystrix.example.hystrixservice;import org.springframework.cloud.netflix.feign.FeignClient;import org.springframework.web.bind.annotation.RequestMapping;@FeignClient(name = &quot;random-generator-service&quot;, url = &quot;http://localhost:8080&quot;)public interface RandomGeneratorServiceClient {    @RequestMapping    String getRandom();}Sama klasa aplikacji spring boot wygląda tak:package pl.consdata.hystrix.example.hystrixservice;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.feign.EnableFeignClients;@EnableFeignClients@SpringBootApplicationpublic class HystrixApplication {    public static void main(String[] args) throws Exception {        SpringApplication.run(HystrixApplication.class, args);    }}Używamy klienta Feign pochodzącego również z biblioteki Netflixa. Feign zawiera w sobie wiele predefiniowanych konfiguracji co znacznie upraszcza powstający kod, aczkolwiek czasami utrudnia nieco zrozumienie co się dzieje w programie ;-) - szczegóły poniżej.W przykładowej aplikacji potrzebny jest jeszcze plik application.properties zawierający następujące ustawienia:server.port=8090                #zmiana portu serwera webowegofeign.hystrix.enabled=false     #wyłączenie domyślnej konfiguracji hystrixaserver.tomcat.max-threads=5     #ograniczenie liczby wątków serwera dla celów naukowych (NIE UŻYWAJ NA PRODUKCJI!!!)Dodajemy circuit breaker’a:Aby dodać circuit breakera do naszej aplikacji należy:  Dodać klasę opakowującą klienta serwisu zewnętrznego komendą hystrixową (adnotacje dostarczane są przez bibliotekę javanica):      package pl.consdata.hystrix.example.hystrixservice;  import org.springframework.beans.factory.annotation.Autowired;  import org.springframework.stereotype.Service;  import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;  @Service  public class RandomGeneratorServiceClientHystrixAware {      final private RandomGeneratorServiceClient randomGeneratorServiceClient;      @Autowired      public RandomGeneratorServiceClientHystrixAware(RandomGeneratorServiceClient randomGeneratorServiceClient) {          this.randomGeneratorServiceClient = randomGeneratorServiceClient;      }      @HystrixCommand(commandKey = &quot;randomCommand&quot;)      public String getRandom() {          return randomGeneratorServiceClient.getRandom();      }  }        W kontrolerze zastąpić wywołania RandomGeneratorServiceClient’a wywołaniami RandomGeneratorServiceClientHystrixAware:      package pl.consdata.hystrix.example.hystrixservice;  import org.springframework.web.bind.annotation.RequestMapping;  import org.springframework.web.bind.annotation.RestController;  @RestController  public class RandomGeneratorController {      private final RandomGeneratorServiceClientHystrixAware randomGeneratorServiceClient;      public RandomGeneratorController(RandomGeneratorServiceClientHystrixAware randomGeneratorServiceClient) {          this.randomGeneratorServiceClient = randomGeneratorServiceClient;      }      @RequestMapping(&quot;/random&quot;)      String getRandom() throws InterruptedException {          return randomGeneratorServiceClient.getRandom();      }  }        Dodać adnotację @EnableCircuitBreaker do konfiguracji aplikacji:      package pl.consdata.hystrix.example.hystrixservice;  import org.springframework.boot.SpringApplication;  import org.springframework.boot.autoconfigure.SpringBootApplication;  import org.springframework.cloud.client.circuitbreaker.EnableCircuitBreaker;  import org.springframework.cloud.netflix.feign.EnableFeignClients;  @EnableCircuitBreaker  @EnableFeignClients  @SpringBootApplication  public class HystrixApplication {      public static void main(String[] args) throws Exception {          SpringApplication.run(HystrixApplication.class, args);      }  }        Dodać konfigurację hystrixa w pliku application.properties:      server.port=8090  feign.hystrix.enabled=false  server.tomcat.max-threads=5  hystrix.command.randomCommand.execution.isolation.thread.timeoutInMilliseconds=800  #timeout komendy hystrixowej &quot;randomCommand&quot; zdefinowanej w klasie RandomGeneratorServiceClientHystrixAware adnotacją @HystrixCommand(commandKey = &quot;randomCommand&quot;)  hystrix.command.randomCommand.circuitBreaker.requestVolumeThreshold=10              #liczba requestów, dla których musi wystąpić timeout w 10 sekundowym oknie, aby circuit breaker otworzył obwód  hystrix.command.randomCommand.metrics.rollingStats.timeInMilliseconds=10000         #czas okna, w którym zliczane są błędne requesty        Dodać zależność na biblioteki hystrixowe:      &amp;lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;  xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&amp;gt;      &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;      &amp;lt;groupId&amp;gt;pl.consdata.hystrix.example&amp;lt;/groupId&amp;gt;      &amp;lt;artifactId&amp;gt;hystrixservice&amp;lt;/artifactId&amp;gt;      &amp;lt;version&amp;gt;1.0-SNAPSHOT&amp;lt;/version&amp;gt;      &amp;lt;dependencies&amp;gt;          &amp;lt;dependency&amp;gt;              &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;              &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt;          &amp;lt;/dependency&amp;gt;          &amp;lt;dependency&amp;gt;              &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt;              &amp;lt;artifactId&amp;gt;spring-cloud-starter-feign&amp;lt;/artifactId&amp;gt;              &amp;lt;version&amp;gt;1.2.2.RELEASE&amp;lt;/version&amp;gt;          &amp;lt;/dependency&amp;gt;          &amp;lt;dependency&amp;gt;              &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt;              &amp;lt;artifactId&amp;gt;spring-cloud-starter-hystrix&amp;lt;/artifactId&amp;gt;              &amp;lt;version&amp;gt;1.2.3.RELEASE&amp;lt;/version&amp;gt;          &amp;lt;/dependency&amp;gt;      &amp;lt;/dependencies&amp;gt;      &amp;lt;dependencyManagement&amp;gt;          &amp;lt;dependencies&amp;gt;              &amp;lt;dependency&amp;gt;                  &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;                  &amp;lt;artifactId&amp;gt;spring-boot-dependencies&amp;lt;/artifactId&amp;gt;                  &amp;lt;version&amp;gt;1.4.2.RELEASE&amp;lt;/version&amp;gt;                  &amp;lt;type&amp;gt;pom&amp;lt;/type&amp;gt;                  &amp;lt;scope&amp;gt;import&amp;lt;/scope&amp;gt;              &amp;lt;/dependency&amp;gt;          &amp;lt;/dependencies&amp;gt;      &amp;lt;/dependencyManagement&amp;gt;  &amp;lt;/project&amp;gt;      Poniższy film pokazuje jak zachowuje się zmodyfikowana aplikacja. W lewym oknie czas odpowiedzi aplikacji niekorzystającej z systemu zewnętrznego. W prawej tak jak poprzednio czasy aplikacji korzystającej z tego systemu.W 20 sekundzie filmu wyłączony zostaje system zewnętrzny. Skutkuje to wzrostem czasów odpowiedzi aplikacji z niego korzystającej do 800 ms. Czasy odpowiedzi aplikacji niezależnej również wzrastają - do tej pory jeszcze nic się nie zmieniło względem pierwotnego zachowania. Po około 10 sekundach circuit breaker otwiera obwód i klient od razu odpowiada, że system jest niedostępny - czasy wywołania spadają, czasy wywołania niezależnej aplikacji wracają do normy. Co jakiś czas widać w prawym oknie nieco dłuższe czasy wywołania - to hystrix sprawdza czy system zewnętrzny jest już dostępny. Około 50 sekundy system zewnętrzny zostaje ponownie włączony.Próba zachowania funkcjonalności:Hystrix umożliwia nam podjęcie akcji naprawczej w momencie wystąpienia błędu. Aby skonfigurować taką akcję należy uzupełnić pole fallbackMethod w adnotacji HystrixCommand:package pl.consdata.hystrix.example.hystrixservice;import java.util.Random;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;@Servicepublic class RandomGeneratorServiceClientHystrixAware {    final private RandomGeneratorServiceClient randomGeneratorServiceClient;    @Autowired    public RandomGeneratorServiceClientHystrixAware(RandomGeneratorServiceClient randomGeneratorServiceClient) {        this.randomGeneratorServiceClient = randomGeneratorServiceClient;    }    @HystrixCommand(commandKey = &quot;randomCommand&quot;, fallbackMethod = &quot;getRandomFallback&quot;)    public String getRandom() {        return randomGeneratorServiceClient.getRandom();    }    private String getRandomFallback() {        return String.valueOf(new Random().nextInt());    }}W tym przykładzie jako fallbackMethod podaliśmy metodę prywatną getRandomFallback, która bierze na siebie odpowiedzialność generowania liczby losowej. Taka implementacja powoduje, że każdy request do naszej aplikacji zwróci poprawną odpowiedź. Nawet kiedy system zewnętrzny będzie niedostępny. Od chwili wystąpienia awarii do momentu otworzenia obwodu przez circuit breakera czasy odpowiedzi będą zbliżone do timeoutu skonfigurowanego dla systemu zewnętrznego. Po otworzeniu obwodu metoda fallbackowa będzie wywoływana od razu - co oznacza, że czasy wywołania powrócą do standardowych lub niższych wartości.Ciekawostki:Konfiguracja klienta Feign:FeignClient dostarcza wielu standardowych konfiguracji. W zasadzie wystarczy gdy w adnotacji wypełnimy pole url. Jeżeli chcemy jednak zmienić jakiś parametr konfiguracji możemy uzupełnić pole configuration podając nazwę klasy zawierająca springową konfigurację beanów. Jeżeli chcemy np. zmodyfikować standardową konfigurację timeoutów dodajemy beana:@BeanRequest.Options options() {    return new Request.Options(750, 200);}Pierwszy parametr konstruktora to connection timeout, drugi read timeout. Dostarczając taką konfigurację spodziewalibyśmy się, że po 750 ms dostaniemy timeout połączenia i tu niespodzianka, …dostaniemy go po 3750 ms. A to dlatego, że FeignClient zawiera w sobie retryer’a, który domyślnie 5 razy powtarza wywołanie. Aby to zmienić należy dostarczyć w klasie konfiguracji beana:@BeanRetryer.Default retryer() {    return new Retryer.Default(10000L, 3000, 1);}Ostatni parametr to liczba powtórzeń.Leniwa inicjalizacja komend hystrixa:Zainstancjonowanie komendy hystrixowej odbywa się podczas pierwszego jej użycia. Może nieść to ze sobą pewne przykre konsekwencje, gdyż inicjalizacja takiej komendy co ciekawe potrafi trwać dość długo. Hystrix próbuje na różne sposoby ustalić wszystkie parametry komendy (jest ich dużo). W produkcyjnej konfiguracji zdarzało się czas ten wynosił nawet 1000 - 2000 ms. Oznacza to, że po restarcie serwera czas pierwszego wywołania komendy może się znacznie różnić od kolejnych i może to doprowadzić do przekroczenia jakiegoś ustalonego czasu odpowiedzi. Implementując w ten sposób usługi synchroniczne warto moim zdaniem rozważyć wprowadzenie specjalnego ‘pustego’ wywołania, które spowoduje zainicjalizowanie krytycznych komend. Takie rozwiązanie wydaje się być nieco infantylne, ale rzeczywiście nie ma na to rady.",
"url": "/2016/12/22/hystrix-praktyczne-uzycie-circuit-breakera.html",
"author": "Jakub Wilczewski",
"authorUrl": "/authors/jwilczewski.html",
"image": "jwilczewski.jpg",
"highlight": "/assets/img/posts/2016-12-22-hystrix-praktyczne-uzycie-circuit-breakera/hystrix.jpg",
"date": "22-12-2016",
"path": "pl/2016-12-22-hystrix-praktyczne-uzycie-circuit-breakera"
}
,


"2016-12-08-sonarqube-wprowadzenie-do-statycznej-analizy-kodu-html": {
"title": "Sonarqube - wprowadzenie do statycznej analizy kodu",
"lang": "pl",
"tags": "sonarqube jakość kodu",
"content": "“It is not enough for code to work.” Robert C. Martin, Clean CodePamiętam jak kilka lat temu z uporem maniaka integrowałem biblioteki statycznej analizy kodu do każdego pom.xml, który wpadł w moje ręce. Do dzisiaj wiele osób za to mnie szczerze nienawidzi. Modyfikacja pomów, konfiguracja w repozytorium każdego projektu i długie instrukcje na wiki związane z integracją IDE. Patrząc z perspektywy czasu przyznaję, że było to dość pracochłonne przedsięwzięcie. Na szczęście pojawiło się narzędzie, które nie tylko uprościło proces statycznej analizy kodu, ale także znacznie tę analizę upowszechniło.KonfiguracjaSonarQube, bo o nim mowa, powstał jako system do integracji raportów z różnych bibliotek i wizualizacji wyników. Twórcy byli rozczarowani tempem zmian w popularnych bibliotekach statycznej analizy kodu, dlatego zaczęli na własną rękę przygotowywać zestaw reguł, które ich zdaniem powinien spełniać dobry kod. W wyniku tego procesu dostaliśmy kompleksowe narzędzie do zbierania analiz i raportowania jakościowych zmian projektu w czasie.W erze przeddokerowej, takie wprowadzenie zajęłoby pewnie kilka ekranów poleceń i czynności konfiguracyjnych. Ja pokażę, że cały proces konfiguracji i pierwszego użycia można zamknąć w 4 poleceniach, z których jedno to zmiana bieżącego katalogu.Ściągnięcie i uruchomienie serwera w domyślnej konfiguracji osiągniemy wydając polecenie:docker run -d --name sonarqube -p 9000:9000 -p 9092:9092 sonarqubeNastępnie potrzebujemy projekt, który poddamy analizie. Dla celów pokazowych weźmy szkielet aplikacji, który każdy z nas niejednokrotnie użył:mvn archetype:generate    -DgroupId=com.mycompany.app    -DartifactId=my-app    -DarchetypeArtifactId=maven-archetype-quickstart    -DinteractiveMode=falseNa koniec pozostaje nam zmienić katalog i uruchomić analizę:cd my-appmvn sonar:sonar(Ostatnie polecenie zadziała w takiej formie wyłącznie na systemach operacyjnych, w których Docker uruchamiany jest natywnie, czyli na chwilę powstawania tego wpisu Linux i Windows 10. W pozostałych przypadkach konieczne jest przekazanie w parametrach namiarów na Dockera ukrytego pod warstwą wirtualizacji. Więcej szczegółów znajdziecie w linkach załączonych na końcu wpisu.)Po zakończeniu przechodzimy na stronę localhost:9000 i powinniśmy zobaczyć gotowy raport. Tym, co nas na początku najbardziej interesuje są smrodki znalezione w kodzie czyli Code Smells:Code Smells w interfejsie SonarQubeJak możemy zobaczyć, w tak niewielkim fragmencie kodu, SonarQube namierzył aż trzy naruszenia! Nawigując po interfejsie możemy przeanalizować okoliczności, w jakich zostało wprowadzone każde naruszenie oraz zapoznać się z bardzo szczegółową dokumentacją. Każdy zdiagnozowany problem ma nie tylko opisaną przyczynę zakwalifikowania go jako Code Smell, ale także przykłady i instrukcje, jak takie naruszenie można z kodu wyeliminować. I to właśnie tutaj ukryte są niezgłębione pokłady wiedzy na temat filozofii czystego kodu.Należy w tym momencie zaznaczyć, że przedstawiona powyżej metoda jest odpowiednia do lokalnej metody analizy kodu. Jeśli zdecydujemy się wdrożyć w projekcie SonarQube jako narzędzie używane przez wielu programistów, warto poświecić trochę czasu i dokonfigurować takie elementy jak uwierzytelnianie, autoryzacja oraz dedykowana baza danych.Środowisko developerskiePrzeglądanie raportów i statystyk projektu pozostawmy jednak w gestii managerów. To, czego potrzebują programiści, to integracji na poziomie środowiska IDE możliwie jak najbliżej kodu. I tutaj z pomocą przychodzi nam plugin SonarLint, dostępny na wszystkie liczące się środowiska programistyczne. Wystarczy, że wskażemy mu serwer:Powiążemy bieżący projekt z projektem założonym na serwerze:Jeśli integracja przebiegnie poprawnie, wszystkie naruszenia na serwerze powinny zostać naniesione na plik otwarty w edytorze:I to w zasadzie wszystko co jest nam potrzebne, aby zacząć przygodę ze statyczną analizą kodu źródłowego naszych aplikacji.Uncle Bob nieprzypadkowo pojawił się na początku tego wpisu. Opisywane tutaj narzędzia są naturalnym rozszerzeniem idei czystego kodu. Grzegorz Huber, kolega z pracy, podzielił się ze mną bardzo trafną uwagą - SonarQube w IDE to prawie tak, jak Uncle Bob siedzący na krzesełku obok.PodsumowanieZachęcam każdego, aby uruchomił lokalnie SonarQube i przeskanował swoje projekty (o ile jeszcze tego nie robi). Wiedza, jaka płynie z takiej operacji jest nie do przecenienia. Jeżeli zainteresuje was ten sposób analizy kodu, warto zgłębić inne funkcjonalności jakie posiada SonarQube. Znajdziemy tam między innymi:  analizę pokrycia kodu testami  wykrywanie copy’n’paste  złożoność cyklomatycznąBibliografia  Robert C. Martin, Clean Code.  https://hub.docker.com/_/sonarqube/  https://about.sonarqube.com/get-started/",
"url": "/2016/12/08/sonarqube-wprowadzenie-do-statycznej-analizy-kodu.html",
"author": "Jacek Grobelny",
"authorUrl": "/authors/jgrobelny.html",
"image": "jgrobelny.jpg",
"highlight": "/assets/img/posts/2016-12-08-sonarqube-wprowadzenie-do-statycznej-analizy-kodu/sonarqube.png",
"date": "08-12-2016",
"path": "pl/2016-12-08-sonarqube-wprowadzenie-do-statycznej-analizy-kodu"
}
,


"2016-08-31-szybki-start-z-angular-cli-html": {
"title": "Szybki start z Angular CLI",
"lang": "pl",
"tags": "angular",
"content": "Wydanie stabilnej wersji Angular 2 to idealny moment, żeby zacząć swoją przygodę z tym frameworkiem. Nie ma lepszego sposobu na poznanie nowej technologii niż skok na głęboką wodę i rozpoczęcie zabawy z kodem 😉Częstym problemem przy pierwszym zetknięciu z nową technologią jest wysoki próg wejścia związany z przygotowaniem środowiska, projektu, czy też samym uruchomieniem i testowaniem. Mimo że samodzielne stworzenie projektu, przygotowanie całej konfiguracji i uruchomienie pierwszego buildu to świetna przygoda, tym razem skorzystamy z narzędzia Angular CLI i uruchomimy całość w kilka minut.W ramach artykułu dowiesz się:  jak stworzyć od zera projekt aplikacji webowej opartej o Angular 2,  jak budować i pracować z projektem,  jak szybko tworzyć składowe elementy projektu nie powtarzając tworzenia boilerplate.Przed przeczytaniem artykułu warto mieć zgrubne pojęcie o konceptach frameworka: https://angular.io/docs/ts/latest/guide/architecture.htmlUtworzenie projektuDo działania angular-cli potrzebujemy zainstalowanego node’a (&amp;gt;= 4.x.x) z npm’em (&amp;gt;= 3.x.x). Samo cli najłatwiej zainstalować globalnie:npm install -g angular-cliTworzymy nowy projekt, dodatkowo wskazując preprocesor CSSów – scss:ng new sample-cli --style=scssWszystkie kolejne operacje cli wykonujemy będąc w katalogu projektu.Uruchamiamy projekt w trybie developerskim:ng serveAplikacja standardowo wystawia się na localhost na porcie 4200:W kilku krokach dostajemy gotowy szkielet aplikacji, gotowy do rozwoju i dystrybucji.Dzięki Angular CLI dostajemy za darmo:  kompletny szkielet aplikacji  działający projekt ze skonfigurowaną najnowszą wersją Angulara  kompletne skrypty budujące z możliwością  uruchomienia buildu produkcyjnego  uruchomienia aplikacji w trybie developerskim  uruchomienia testów jednostkowych i testów e2e  wsparcie dla kompilowania budowania CSS’ów na bazie scss/sass/less/stylusW tym zaawansowane techniki budowania aplikacji:  bundlowanie kodu, w tym obsługa podziału na części wspólne i ładowane asynchronicznie  tree-shaking  uglifyingStruktura projektuWygenerowany szablon pozwala pracować nad kompletną aplikacją webową. W poszczególnych folderach znajdują się źródła aplikacji (kody TypeScript, html’e i stylizacje), pliki testów (w tym e2e) oraz zasoby statyczne serwowane as-is w docelowej aplikacji.Punktem wejścia aplikacji są src/index.html oraz src/main.ts, których celem jest załadowanie strony i osadzenie głównego komponentu naszej aplikacji zdefiniowanego w src/app/app.component.ts. Od tego ostatniego zaczynamy swoją przygodę z frameworkiem.Generowanie kodu i praca z CLIAngular CLI, poza wygenerowaniem i budowaniem projektu, pomaga też w codziennej pracy, zdejmując z programisty typowe zadania związane z boilerplate.Automatycznie wygenerujemy szablony dla wszystkich składowych frameworka:  komponenty (component),  dyrektywy (directive),  pipe’y (pipe),  seriwisy (service),  klasy i interfejsy (class, interface),  enumy (enum),  moduły (module).Użycie generatora pozwala szybko stworzyć komplet zasobów.Przykładowo stworzenie nowego komponentu wygeneruje dla nas:  klasę komponentu (*.ts)  szablon html komponentu (*.html)  plik stylizacji komponentu (*.html)  plik z testami komponentu (*.spec.ts)  podpięcie komponentu pod moduł aplikacjiTak wygenerowany komponent jest gotowy do użycia bez dodatkowych modyfikacji:ng generate component TodoListRównie łatwo możemy stworzyć serwis:ng generate service TodoNowo stworzone serwisy nie są standardowo podpinane pod żaden moduł. Można to jednak łatwo zrobić, np. podpinając w głównym module aplikacji, w pliku src/app/app.module.ts:import { BrowserModule } from &#39;@angular/platform-browser&#39;;import { NgModule } from &#39;@angular/core&#39;;import { FormsModule } from &#39;@angular/forms&#39;;import { HttpModule } from &#39;@angular/http&#39;;import { AppComponent } from &#39;./app.component&#39;;import { TodoListComponent } from &#39;./todo-list/todo-list.component&#39;;import { TodoService } from &#39;./todo.service&#39;;@NgModule({  declarations: [    AppComponent,    TodoListComponent  ],  imports: [    BrowserModule,    FormsModule,    HttpModule  ],  providers: [TodoService],  bootstrap: [AppComponent]})export class AppModule { }W podobny sposób możemy tworzyć pozostałe elementy aplikacji, zrzucając na narzędzie tworzenie zasobów na podstawie szablonów.Budowanie projektuWygenerowany projekt można zbudować w finalną aplikację:ng buildWygenerowane zasoby trafiają do folderu dist.Budowanie można uruchomić podając dodatkowo profil. Standardowo budowana jest developerska wersja aplikacji. Jeżeli chcemy zbudować produkcyjną aplikację, wykorzystującą uglifying i tree-shaking, powinniśmy wskazać profil prod:ng build --prodDodatkowo możliwe jest podanie wartości środowiskowych dla buildu i np. uruchamianie warunkowo szerszego logowania czy też punktów debugowania. W tym celu można użyć przełącznika –env nazwa.Tak ustawiony profil można wykorzystać na etapie budowania oraz w uruchomionej aplikacji, przykładowo:if (environment.production) {  // ...}Uruchomienie testówZarówno testy jednostkowe, jak i end-to-end możemy spokojnie uruchamiać za pomocą CLI, wszystkie potrzebne konfiguracje są już dostarczone w ramach wygenerowanego szkieletu aplikacji.Standardowo testy są uruchamiane w trybie nasłuchiwania na zmiany, to znaczy że framework testowy pozostaje uruchomiony w tle i wykonuje zestaw testów za każdym razem gdy wprowadzimy zmianę w źródłach.Żeby testy uruchomić jednokrotnie należy dodać flagę –watch false.ng test --watch falseTesty funkcjonalne uruchamiamy poleceniem:ng e2ejednak przed ich uruchomieniem należy wcześniej uruchomić samą aplikację, np. przez pokazany już ng serve:ng serve &amp;amp; ng e2ePodsumowanieWykorzystanie narzędzia typu Angular CLI pozwala nam na:  korzystanie z najnowszych technologii,  ustandaryzowanie struktury i konfiguracji projektu,  zrzucenie utrzymania procesu budowania na community,  szybkie wsparcie dla nowych wersji frameworka przy minimalnym koszcie,  stosowanie wzorcowej/zalecanej konfiguracji,  szybkie uruchamianie nowych projektów,  stosowanie podejścia convention over configuration.",
"url": "/2016/08/31/szybki-start-z-angular-cli.html",
"author": "Grzegorz Lipecki",
"authorUrl": "/authors/glipecki.html",
"image": "glipecki.jpg",
"highlight": "/assets/img/posts/2016-08-31-szybki-start-z-angular-cli/start_z_angularem.jpg",
"date": "31-08-2016",
"path": "pl/2016-08-31-szybki-start-z-angular-cli"
}


}
